---
title: "Supervision 6"
format: html
execute:
  echo: true
  warning: false
  message: false
---

## Lab 19 ‚Äî Qualitative Dependent Variables: Logit vs Probit on the Hedonic Model

> This supervision builds directly on **Supervision 5**. We will keep the *same* hedonic right-hand-side (RHS) variables, but transform the **dependent variable** into **High vs Low Price** using the sample median. You will fit **logit** and **probit** models, interpret **marginal effects**, and assess **predictive performance**.

---

### üéØ Learning Outcomes

By the end of this lab you will be able to:

1. Transform a continuous target into a **binary** target using the **median**.
2. Fit **Logit** and **Probit** models in R with the same hedonic covariates used previously.
3. Interpret **Average Marginal Effects (AMEs)** in probability terms.
4. Compare model performance via **ROC curves** and **AUC**.
5. Build **confusion matrices** at a chosen threshold and compute basic metrics.

---

‚úîÔ∏è **Step-by-step workflow**

- The sections follow a logical sequence:

- Data setup

- Binary transformation

- Model estimation

- Interpretation

- Forecasting ‚Üí performance evaluation

---


### üì¶ Setup & Data

We reuse the dataset prepared in Supervision 5 (we called the dataset `df`, see Sup 5 - "Data Preparation"). 

```{r}
#| include: false

library(AmesHousing)
library(dplyr)
library(readr)
library(janitor)
library(stringr)

ames <- AmesHousing::make_ames()

df <- ames %>%
  janitor::clean_names() %>%
  dplyr::transmute(
    sale_price,
    gr_liv_area, total_bsmt_sf, lot_area,
    bedroom_abv_gr, full_bath, half_bath, kitchen_abv_gr,
    overall_qual, overall_cond, year_built, year_remod_add,
    exter_qual, kitchen_qual, central_air,
    garage_cars, garage_area, fireplace_qu,
    bldg_type, house_style, neighborhood,
    ms_zoning, foundation
  ) %>%
  dplyr::mutate(
    central_air = factor(central_air, levels = c("N","Y")),
    exter_qual = factor(exter_qual, ordered = TRUE),
    kitchen_qual = factor(kitchen_qual, ordered = TRUE),
    fireplace_qu = factor(fireplace_qu, ordered = TRUE),
    bldg_type = factor(bldg_type),
    house_style = factor(house_style),
    neighborhood = factor(neighborhood),
    ms_zoning = factor(ms_zoning),
    foundation = factor(foundation),
    baths_total = full_bath + 0.5 * half_bath,
    age = pmax(0, 2010 - year_built),
    remod_age = pmax(0, 2010 - year_remod_add)
  ) %>%
  dplyr::filter(
    sale_price > 20000, sale_price < 600000,
    gr_liv_area > 300, lot_area < 100000
  ) %>%
  tidyr::drop_na(sale_price, gr_liv_area, bedroom_abv_gr, baths_total, overall_qual)

set.seed(42)
df <- df %>% mutate(listing_id = row_number()) %>% relocate(listing_id)

write_csv(df, "hedonic_ames.csv")
glimpse(df)

# 1. Load data & transform
df <- readr::read_csv("hedonic_ames.csv")

# ---------------------------------------------------
# 1) Load & transform
# ---------------------------------------------------
df <- read_csv("hedonic_ames.csv") %>%
  mutate(
    ln_price = log(sale_price),
    ln_area  = log(gr_liv_area),
    overall_qual = factor(overall_qual, ordered = TRUE),
    overall_cond = factor(overall_cond, ordered = TRUE),
    exter_qual   = factor(exter_qual, ordered = TRUE),
    kitchen_qual = factor(kitchen_qual, ordered = TRUE),
    fireplace_qu = factor(fireplace_qu, ordered = TRUE),
    bldg_type    = factor(bldg_type),
    house_style  = factor(house_style),
    neighborhood = factor(neighborhood),
    baths_total  = full_bath + 0.5 * half_bath
  )

```


```{r}
# If a package is missing on your machine, run install.packages("package name")
# Example: install.packages("readr")

# Load core packages used in this lab
library(readr)     # reading data
library(dplyr)     # data manipulation
library(ggplot2)   # plotting

# Optional/advanced packages for inference and diagnostics
# If missing, install as needed:
# install.packages(c("margins", "sandwich", "lmtest", "pROC"))
library(margins)   # marginal effects
library(sandwich)  # robust (HC) variance
library(lmtest)    # coeftest with robust SE
#git addinstall.packages("pROC", repos = "https://cloud.r-project.org")
library(pROC)      # ROC and AUC

# ---- Load the data using Supervision 5 - Section: data preparation
# Adjust the path if your file is elsewhere.

# Quick preview
dplyr::glimpse(df)
```



---

### üß† From Continuous to Binary: Define High vs Low Price

We create a binary target equal to **1** if `sale_price` is **at least** the sample median and **0** otherwise. The median splits the sample into two balanced groups.

```{r}
# Compute the sample median of the sale price
price_med <- median(df$sale_price, na.rm = TRUE)

# Create the binary dependent variable
df <- df %>% mutate(high_price = as.integer(sale_price >= price_med))

# Check class balance
table(df$high_price)
prop.table(table(df$high_price))
```

::: callout-note
### ‚ú® Why logit & probit exist (the *aha!* moment)

OLS fails when the dependent variable is 0/1 because predicted values can be <0 or >1.

Instead of predicting price category directly, logit and probit assume:

> There is an **unobserved (latent) index** of "housing quality/value"
> and we only observe whether it crosses a threshold.

$y_i^* = X_i \beta + \varepsilon_i
\quad\rightarrow\quad
y_i = \begin{cases}
1 & \text{if } y_i^* > 0 \\
0 & \text{otherwise}
\end{cases}$

The difference is the distribution of the error term $\varepsilon$:

| Model | Assumption for \( \varepsilon \) | Effect |
|--------|----------------------------------|--------|
| **Logit** | Logistic distribution | Coefficients scale with *odds*. |
| **Probit** | Normal distribution | Coefficients on *z-score* scale. |

Because logistic and normal are very similar:

> **Logit coefficient ‚âà 1.6 √ó probit coefficient**

But both produce nearly **identical predicted probabilities and classification**.
:::



---

### üß© Model Specification (same RHS as Supervision 5)

We keep the hedonic RHS unchanged. Only the outcome changed (continuous -> binary).

```{r}
# Hedonic formula reused for binary models
f_bin <- high_price ~ ln_area + bedroom_abv_gr + baths_total +
  total_bsmt_sf + garage_cars + central_air + overall_qual +
  overall_cond + exter_qual + kitchen_qual + fireplace_qu +
  bldg_type + house_style + neighborhood
```

---


### ‚öôÔ∏è Estimation (fit only) + ü™ì Backward Selection (BIC)

We now estimate the latent model using `glm()`:

$P(y = 1 \mid X) = \text{link}^{-1}(X\beta)$

- `logit` ‚Üí inverse-logit (`plogis()`): returns a probability via *odds*
- `probit` ‚Üí normal CDF (`pnorm()`): returns a probability via *z-score*

Logit uses the logistic CDF **(cumulative distribution function)**; probit uses the (scaled) normal CDF. They almost overlap, which is why results are so similar and logit ‚âà 1.6 √ó probit in scale. Let's compare the CDFs:

```{r}
# Logistic vs Probit (variance-matched) ‚Äî super short CDF plot
sd_match <- pi / sqrt(3)              # match Normal variance to Logistic
curve(plogis(x), from=-6, to=6, lwd=2, xlab="Latent index (x)", ylab="CDF",
      main="CDF: Logistic vs Normal (variance-matched)")
curve(pnorm(x, sd=sd_match), from=-6, to=6, lwd=2, lty=2, add=TRUE)
legend("topleft", c("Logistic (logit)", "Normal scaled (probit)"),
       lwd=2, lty=c(1,2), bty="n")
```

In the next chunk we **suppress printing** baseline models to avoid overwhelming output (too many variables). However, we run **backward stepwise (BIC)** and present a compact, readable table of that selected model. Coefficient signs and significance are typically very similar; logit coefficients are usually ~1.6√ó probit coefficients (scale difference).

```{r}
# --- Packages used in this chunk -------------------------------------------
library(dplyr)      # data wrangling
library(broom)      # tidy model outputs (term, estimate, std.error, statistic, p.value)
library(stringr)    # clean up long variable names
library(gt)         # neat tables
library(lmtest)     # coeftest() with robust SEs
library(sandwich)   # vcovHC() for robust (HC) variance

# --- Convert ordered factors to plain factors --------------------------------
# Why? Ordered factors produce polynomial contrasts (.L, .Q, ...) that are hard to interpret.
# Using plain factors yields standard dummies and lets step() drop/keep whole factors cleanly.
df <- df |>
  mutate(
    overall_qual = as.factor(overall_qual),
    overall_cond = as.factor(overall_cond),
    exter_qual   = as.factor(exter_qual),
    kitchen_qual = as.factor(kitchen_qual),
    fireplace_qu = as.factor(fireplace_qu),
    central_air  = as.factor(central_air)  # ensure "Y"/"N" is categorical
  )

# --- Fit full LOGIT & PROBIT (we suppress long summaries for readability) ----
logit_full  <- glm(f_bin, data = df, family = binomial("logit"))
probit_full <- glm(f_bin, data = df, family = binomial("probit"))

# --- Backward stepwise with BIC (k = log(n)) on both models ------------------
# BIC penalizes complexity more than AIC -> typically a leaner, more generalizable model.
k_bic <- log(nrow(df))
logit_bic  <- step(logit_full,  direction = "backward", k = k_bic, trace = 0)
probit_bic <- step(probit_full, direction = "backward", k = k_bic, trace = 0)

# --- Quick fit comparison (lower is better) ----------------------------------
bic_tab <- tibble(
  model = c("logit_bic", "probit_bic"),
  AIC   = c(AIC(logit_bic),  AIC(probit_bic)),
  BIC   = c(BIC(logit_bic),  BIC(probit_bic))
)
bic_tab  # glance: which BIC-selected model scores better?

# --- Keep BOTH models for downstream sections --------------------------------
main_logit <- logit_bic   # LOGIT (BIC-selected)
main_probit <- probit_bic  # PROBIT (BIC-selected)

# --- Also clarify which one is "best" by BIC ---------------------------------
best_model <- if (BIC(logit_bic) < BIC(probit_bic)) {
  message("‚úÖ BIC selects the LOGIT model as best (more parsimonious).")
  logit_bic
} else {
  message("‚úÖ BIC selects the PROBIT model as best (more parsimonious).")
  probit_bic
}

# --- Helper: compact robust-SE table for any glm -----------------------------
make_compact_table <- function(mod, title_text) {
  # Robust HC1 variance-covariance and tidy coefficients
  rob_vcov <- sandwich::vcovHC(mod, type = "HC1")
  out <- broom::tidy(lmtest::coeftest(mod, vcov. = rob_vcov)) |>
    filter(term != "(Intercept)") |>
    arrange(p.value) |>
    # Friendlier labels for students (extend as needed)
    mutate(
      term = term |>
        str_replace("^ln_area$", "ln(living area)") |>
        str_replace("^baths_total$", "Total baths") |>
        str_replace("^garage_cars$", "Garage capacity") |>
        str_replace("^central_airY$", "Central air: Yes") |>
        str_replace("^bldg_type", "Bldg type: ") |>
        str_replace("^house_style", "Style: ") |>
        str_replace("^neighborhood", "Nbh: ")
    ) |>
    slice_head(n = 20) |>
    mutate(
      estimate  = round(estimate, 4),
      std.error = round(std.error, 4),
      statistic = round(statistic, 4),
      p.value   = round(p.value, 4)
    )

  # Render a gt table (no exotic functions to avoid errors)
  out |>
    gt() |>
    tab_header(title = title_text) |>
    cols_label(
      term = "Term", estimate = "Coef", std.error = "Robust SE",
      statistic = "z", p.value = "p"
    ) |>
    tab_options(table.width = pct(100), data_row.padding = px(3))
}

# --- Print compact tables for BOTH models ------------------------------------
make_compact_table(main_logit, "Logit (BIC-selected): Key Terms ‚Äî Robust SEs")
make_compact_table(main_probit, "Probit (BIC-selected): Key Terms ‚Äî Robust SEs  üìå We will also use this downstream")

# --- (Optional) Kept vs Dropped list for transparency ------------------------
# Uncomment to show what BIC dropped vs. kept for each model.
# kept1   <- attr(terms(main_logit), "term.labels"); full1 <- attr(terms(logit_full),  "term.labels")
# kept2   <- attr(terms(main_probit), "term.labels"); full2 <- attr(terms(probit_full), "term.labels")
# dropped1 <- setdiff(full1, kept1); dropped2 <- setdiff(full2, kept2)
# tibble(model = "LOGIT", status = "Kept", term = kept1) |>
#   bind_rows(tibble(model = "LOGIT", status = "Dropped", term = dropped1)) |>
#   bind_rows(tibble(model = "PROBIT", status = "Kept", term = kept2)) |>
#   bind_rows(tibble(model = "PROBIT", status = "Dropped", term = dropped2)) |>
#   arrange(model, status, term) |>
#   gt() |>
#   tab_header(title = "Backward BIC: Kept vs Dropped Terms (Logit & Probit)") |>
#   tab_options(table.width = pct(100))
```

**Robust Standard Errors (recommended for cross-sections)**

```{r}
# HC1 robust SEs using sandwich + lmtest
coeftest(main_logit,  vcov. = vcovHC(main_logit,  type = "HC1"))
coeftest(main_probit, vcov. = vcovHC(main_probit, type = "HC1"))
```

---

::: callout-tip
### Why marginal effects?  
Logit/probit coefficients are **not** changes in probability ‚Äî they change the *latent index*.

Marginal effects compute:

> ‚ÄúHolding everything else constant, how does  $P(y=1)$ change when $x$ increases by 1?‚Äù

That is why AMEs are interpretable in **percentage points**.
:::



### üìà Interpreting Effects with Marginal Effects (AMEs)

Raw coefficients are on latent/odds scales. **Average Marginal Effects (AMEs)** translate them to **percentage-point changes in probability** for a one-unit change in each predictor.

Logit/probit coefficients are not probabilities‚Äîthey shift the latent score, not $P(Y=1)$ directly. Marginal effects translate results into probability terms: ‚Äúby how many percentage points does $P(Y=1)$ change when a variable increases by one unit, holding others constant?‚Äù Average Marginal Effects (AMEs) average this change over all observations, so the numbers can be read directly in percentage-point terms. Use AMEs for clear, substantive interpretation.

```{r}
# Average Marginal Effects for both models
me_logit  <- margins(main_logit)
me_probit <- margins(main_probit)

summary(me_logit)
summary(me_probit)

# Tip for reporting:
# "A 0.01 increase in ln_area (~1% more area) changes the probability of being high-priced by X percentage points (AME)."
```

---

### üîÆ Predicted Probabilities & Agreement Between Models

```{r}
# Predicted probabilities from each model
df <- df %>%
  mutate(
    p_logit  = predict(main_logit,  type = "response"),
    p_probit = predict(main_probit, type = "response")
  )

# Compare probability predictions
cor(df$p_logit, df$p_probit, use = "complete.obs")
summary(df$p_logit)
summary(df$p_probit)
```

---

### üß™ Classification Diagnostics: ROC & AUC

An ROC (Receiver Operating Characteristic) curve is a graph that shows a classifier's performance by plotting the true positive rate against the false positive rate at various threshold settings. The AUC (Area Under the Curve) is a single summary metric that measures how well a model can distinguish between classes, with a higher value indicating better performance. A perfect classifier has an AUC of \(1.0\), while a random classifier has an AUC of \(0.5\).¬†

We evaluate ranking performance via **ROC curves** and **AUC**.

::: callout-note
### ROC intuition

ROC doesn‚Äôt evaluate the predicted **probability**, but the **ranking quality**.

> ‚ÄúAmong all pairs of houses, how often does the model rank the expensive one higher?‚Äù

- AUC = 0.5 ‚Üí random odds
- AUC = 1.0 ‚Üí perfect ranking
:::

A ROC curve evaluates how well the model ranks positives above negatives across all possible thresholds. It does not measure overall ‚Äúpercent correct,‚Äù but the trade-off between true positive rate (sensitivity) and false positive rate as the cutoff moves. The AUC summarizes ranking ability from 0.5 (random) to 1.0 (perfect). Two models can have similar accuracy at one cutoff yet different AUCs, which is why ROC/AUC is informative.

```{r}
# ROC objects (treating 1 as the positive class)
roc_logit  <- roc(df$high_price, df$p_logit)
roc_probit <- roc(df$high_price, df$p_probit)

# AUC values
auc(roc_logit)
auc(roc_probit)

# Simple ROC plots with ggplot2
autoplot_roc <- function(roc_obj, title){
  # Build a data frame with TPR (sensitivity) and FPR (1 - specificity)
  plot_df <- data.frame(
    tpr = roc_obj$sensitivities,
    fpr = 1 - roc_obj$specificities
  )
  ggplot(plot_df, aes(x = fpr, y = tpr)) +
    geom_line() +
    geom_abline(slope = 1, intercept = 0, linetype = 2) +
    labs(x = "False Positive Rate", y = "True Positive Rate", title = title)
}

autoplot_roc(roc_logit,  paste0("ROC ‚Äì Logit (AUC = ", round(auc(roc_logit), 3), ")"))
autoplot_roc(roc_probit, paste0("ROC ‚Äì Probit (AUC = ", round(auc(roc_probit), 3), ")"))
```

---

### ‚úÇÔ∏è Choosing a Threshold & Confusion Matrices

Classification requires a threshold on predicted probabilities. The **Youden J** criterion often gives a better cut-off than the na√Øve 0.5.

Choosing a single cutoff is a separate decision from estimating probabilities. The default 0.5 is arbitrary and may be sub-optimal when the costs of false positives and false negatives differ. The Youden J statistic selects the threshold that maximizes sensitivity + specificity ‚àí 1, giving a balanced operating point. We will compare results at 0.5 and at the Youden-optimal threshold

```{r}
# Optimal thresholds by Youden's J
thr_logit  <- coords(roc_logit,  x = "best", best.method = "youden", transpose = TRUE)["threshold"]
thr_probit <- coords(roc_probit, x = "best", best.method = "youden", transpose = TRUE)["threshold"]

thr_logit; thr_probit
```

A confusion matrix maps predictions to actuals and classifies outcomes into four cells: True Positive (correct 1), False Positive (false alarm), False Negative (missed 1), and True Negative (correct 0). 
Sensitivity = TP/(TP+FN) is a rate ‚Äúhow many actual true positives we catch from all true positives?‚Äù 

Specificity = TN/(TN+FP) asks ‚Äúhow many actual 0s did we avoid flagging?‚Äù 

```{r}
# Helper to compute a confusion matrix and basic metrics without extra packages
confusion_metrics <- function(y_true, y_prob, thr = 0.5) {
  y_pred <- ifelse(y_prob >= thr, 1L, 0L)
  TP <- sum(y_pred == 1 & y_true == 1, na.rm = TRUE)
  TN <- sum(y_pred == 0 & y_true == 0, na.rm = TRUE)
  FP <- sum(y_pred == 1 & y_true == 0, na.rm = TRUE)
  FN <- sum(y_pred == 0 & y_true == 1, na.rm = TRUE)
  acc <- (TP + TN) / (TP + TN + FP + FN)
  tpr <- TP / (TP + FN)  # sensitivity / recall
  tnr <- TN / (TN + FP)  # specificity
  ppv <- TP / (TP + FP)  # precision
  npv <- TN / (TN + FN)  # negative predicted value
  list(
    threshold = thr,
    matrix = matrix(c(TN, FP, FN, TP), nrow = 2,
                    dimnames = list("Predicted" = c("0","1"), "Actual" = c("0","1"))),
    accuracy = acc, sensitivity = tpr, specificity = tnr, precision = ppv, npv = npv
  )
}

# Confusion matrices at 0.5 and at the optimal Youden threshold
cm_logit_05 <- confusion_metrics(df$high_price, df$p_logit,  thr = 0.5)
cm_probit_05 <- confusion_metrics(df$high_price, df$p_probit, thr = 0.5)
cm_logit_opt <- confusion_metrics(df$high_price, df$p_logit,  thr = as.numeric(thr_logit))
cm_probit_opt <- confusion_metrics(df$high_price, df$p_probit, thr = as.numeric(thr_probit))

cm_logit_05
cm_probit_05
cm_logit_opt
cm_probit_opt
```

::: callout-success
### Final takeaway
- Logit and probit tell the *same economic story*
- AMEs let us interpret effects in **percentage points**
- ROC/Youden threshold evaluates predictive/classification performance
:::

## Policy applications

Binary choice modelling is often used by governments or planners when the dependent variable can be framed as a yes/no outcome, for example:

Probability a property is affordable or not affordable

Probability a house is in poor condition

Probability a dwelling meets energy efficiency standards

Probability a house is at risk of flooding

Probability the area is gentrifying

---

### üìù What to Report (suggested structure)

1. **Model setup**: how the binary target was defined; why the median.
2. **Estimation**: logit & probit results; include **robust SE** tables.
3. **Interpretation**: **AMEs** for key variables (e.g., `ln_area`, `overall_qual`, `baths_total`).
4. **Performance**: AUC for both models; comment on similarity/differences.
5. **Classification**: confusion matrices at 0.5 and at the Youden-optimal threshold; discuss trade-offs.
6. **Reflection**: when would you prefer logit vs probit? (Hint: results are usually very similar; choice often driven by convention or interpretability.)

---

### ‚úÖ Checklist

- [ ] Code runs from top to bottom **without manual edits**.
- [ ] All variables in your RHS are present/constructed.
- [ ] You report **robust SEs** and **AMEs**.
- [ ] You include **AUC** and describe **threshold choice**.
- [ ] Your interpretations are in **probability** (percentage-point) terms.

---

üèÅ *End of Supervision 6*  
üõë Remember to save your script üíæ
