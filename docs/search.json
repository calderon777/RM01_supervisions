[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Methods for Land Economy",
    "section": "",
    "text": "Welcome\nWelcome to these interactive laboratory sessions for Research Methods in Land Economy. This digital workbook provides hands-on experience with R programming, quantitative analysis, and data visualisation techniques essential for contemporary research in land economy, finance, planning, and environmental housing studies.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Welcome and Supervision Overview</span>"
    ]
  },
  {
    "objectID": "index.html#supervisions-overview",
    "href": "index.html#supervisions-overview",
    "title": "Research Methods for Land Economy",
    "section": "Supervisions Overview",
    "text": "Supervisions Overview\nThese supervised laboratory sessions are designed for MPhil students beginning their quantitative research journey. Over several progressive supervision, you will develop from complete programming novice to confident data analysis skills, working with appropriate real-world land economy datasets from institutional sources and applying cutting-edge analytical techniques to substantive research questions in your field.\n\nAcademic Integration\nEach lab connects technical skill development with substantive research applications, ensuring that programming competency serves your broader academic and professional objectives. Youâ€™ll work with datasets relevant to housing policy, regional development, and planning analysis while developing reproducible research workflows used in leading academic institutions and policy organisations.\n\n\n\nHow to Use This Interactive Workbook\nThis workbook is designed as a practical, hands-on learning experience that requires active engagement with both the content and your RStudio environment. Success depends on following the structured approach outlined below.\n\n\nEssential Setup\n\nTechnical Requirements\nBefore beginning, ensure you have: - R (version â‰¥ 4.3) installed from r-project.org - RStudio Desktop (latest version) from posit.co - A stable internet connection for package installation and data downloads - Sufficient disk space (â‰¥ 2GB) for datasets and outputs\n\n\nğŸ’¾ Install R and RStudio on your laptops/gadgets\nRStudio is a wrapper for R; we are using RStudio because it makes looks R more organised and straightforward. Follow the following instructions:\n\ngo to https://www.r-project.org\nclick on â€˜CRAN mirrorâ€™\nchoose one of the two UK mirrors - https://www.stats.bris.ac.uk/R/ or https://cran.ma.imperial.ac.uk/\nClick on â€˜Download R for Windowsâ€™ or â€˜Download R for macOSâ€™\nClick on â€™install R for the first time\nClick on â€˜Download R-â€¦ for Windowsâ€™, the current version is R-4.5.1. Or alternatively, download the executable file for macOS.\ndouble-click the downloaded file, then click on â€˜Runâ€™, â€˜language:Englishâ€™, â€˜OKâ€™, and read and follow the instructions of installation until you click on â€˜Nextâ€™ for installing and â€˜Finishâ€™\n\nThen proceed to Installing RStudio.\n\nGo to rstudio.com\nclick on â€˜downloadâ€™\ndownload the Free version, download RStudio for Windows or macOS\nClick on â€˜Nextâ€™ if you agree and on â€˜Installâ€™ and â€˜Finishâ€™ to complete the process.\n\n\n\nProject Organisation\nCreate a dedicated folder structure for the course:\nRM01_Labs/\nâ”œâ”€â”€ data/           # Raw datasets and imports\nâ”œâ”€â”€ scripts/        # Your R code files  \nâ”œâ”€â”€ outputs/        # Generated plots, tables, reports\nâ”œâ”€â”€ certificates/   # Professional development documentation\nâ””â”€â”€ reflections/    # Written responses to critical questions\n\n\n\nActive Learning Protocol\n\n1. Read â†’ Code â†’ Reflect\nEach lab section follows this essential pattern: - Read the contextual material and instructions carefully - Code by replicating all examples in your RStudio session - Reflect by answering critical thinking questions in writing\n\n\n2. Hands-On Replication\nCritical: Do not simply read the code examples. You must: - Type (donâ€™t copy-paste) all code into RStudio scripts - Execute each code block and verify the outputs - Experiment with variations and modifications - Debug any errors before proceeding\n\n\n3. Documentation Standards\nMaintain professional documentation practices: - Save each lab as a separate R script (e.g., lab01_certification.R) - Comment your code extensively using # explanatory text - Include session information - Date and version your files systematically\n\n\n\nDeliverables\nEach lab includes specific deliverables: - Technical: Working R scripts with documented code - Visual: Publication-quality plots and analytical outputs\n- Written: Responses to all critical reflection questions - Professional: Certificates, portfolios, and documentation\n\n\nLearning Support\n\nGetting Help\nWhen you encounter difficulties: 1. Review the relevant section carefully for missed details or typos 2. Check R documentation using ?function_name or help(function_name) 3. Examine error messages systematicallyâ€”they often provide clear guidance and AI can often provide answers 4. Consult peers through appropriate academic channels 5. Attend supervision sessions prepared with specific questions\n\n\nCommon Pitfalls to Avoid\n\nPassive learning: You must complete the materials before attending the face-to-face supervisions.\nPassive reading: You must actively code to learn programming\nCopy-paste coding: Type examples yourself to build muscle memory\nIgnoring errors: Always resolve issues before proceeding\nSuperficial reflections: Engage deeply with methodological questions\nPoor organisation: Maintain systematic file and folder structures\n\n\n\nIndustry Applications\nR programming competency is increasingly valued in: - Policy Analysis: Government departments and think tanks - Consultancy: Urban planning, housing, and development firms - Research Organisations: Academic institutions and policy institutes - Data Science: Emerging opportunities across multiple sectors\n\n\n\n\nTechnical Notes\nLabs will introduce packages progressively. Install required packages as indicated in each session, and maintain a record of your technical environment using:\n# Document your session for reproducibility\nsessionInfo()\n\n\n\nGetting Started\nNavigate to Supervision 1 using the link to begin your journey into R programming for land economy research. Remember: this is an interactive learning experience that requires preparation before attending supervisions, your active participation, critical thinking, and professional engagement.\nSuccess in these labs depends not just on following instructions, but on developing the analytical thinking skills that will serve your research skills.\n\nFor technical support or academic guidance, consult your teaching team or attend designated office hours with specific questions prepared.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Welcome and Supervision Overview</span>"
    ]
  },
  {
    "objectID": "supervision_1.html",
    "href": "supervision_1.html",
    "title": "Supervision 1",
    "section": "",
    "text": "ğŸ§ª Lab 1 â€” R Quickstart",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Supervision 1</span>"
    ]
  },
  {
    "objectID": "supervision_1.html#lab-1-r-quickstart",
    "href": "supervision_1.html#lab-1-r-quickstart",
    "title": "Supervision 1",
    "section": "",
    "text": "ğŸ¯ Learning outcomes (you will be able to)\n\nOpen RStudio and recognise the Console, Script, Environment/History, Files/Plots/Help panes.\nCreate and run a short R script (comments, simple operations).\nSet your working directory to a course folder.\nCreate and inspect basic objects (a scalar and a small matrix).\nSave your script for the next supervision.\n\n\n\nğŸ§° Prerequisites\n\nRead â€œWelcome & Supervision Overviewâ€ section.\nR and RStudio installed.\nA course folder and subfolders on your machine where you will save scripts and other files:\n\nRM01_labs/\nâ”œâ”€â”€ data/           # For raw datasets and imports\nâ”œâ”€â”€ scripts/        # Your R code files  \nâ”œâ”€â”€ outputs/        # Generated plots, tables, reports\nâ”œâ”€â”€ certificates/   # Professional development documentation\nâ””â”€â”€ reflections/    # Written responses to critical questions\n\n\nğŸ“˜ Introduction to R\nAfter downloading and installing open RStudio, we will cover now some tutorials.\nğŸ‘¨â€ğŸ’» Long Tutorial\nWe would cover a brief introduction on this labs, students can use a longer tutorial for detailed learning at https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf\n\n\nA. Get oriented in RStudio\nYou need to have installed R and RStudio to continue with this lab.\n\nOpen Rstudio in your computer.\n\n\n\n\nOn this picture, there are three panels â€˜the Consoleâ€™, â€˜The Environmentâ€™, and â€˜The Plots, Help and Filesâ€™ panel.\n\n\n\nLetâ€™s choose a better appeareance for our RStudio. Click on Tools&gt;Global Options.\n\n\n\n\nIn RStudio, click on â€˜Toolsâ€™ and â€˜Global Optionsâ€™\n\n\n\nAfterwards, in the window â€œOptionsâ€, click on â€œAppearanceâ€, RStudio theme: â€œModernâ€, Editor font: â€œCourierâ€, Editor theme: Cobalt. Finally, click on â€œApplyâ€\n\n\n\n\nYou are here by clicking on Tools&gt;Global Options..&gt;Appereance&gt;Modern&gt;Courier&gt;Cobalt&gt;Apply\n\n\n\n\n\nB. Your first script\n\nOpen a file to save the code for this session. Click on File â†’ New File â†’ R Script; or simply press Ctrl+Shift+N (command+shift+enter in Mac); this will open a new panel called â€˜Scriptâ€™ where we are going to write the commands for R. Change the name of the script to â€œlab_1â€, saving the script in the folder RM01_Labs/scripts/ File â†’ Save As â†’ â€œlab_1â€\n\n\n\n\nClick on File&gt;New File&gt;R Script\n\n\n\nAdd a comment (starts with #.) On the script panel, type the caracter # and then write 'LAB ONE AND TODAY'S DATE'. R will ignore any line of command starting with #. You can use # to add comments, titles, reminders within your R code/ R Script.\n\nYour R Script now shall look similar to this:\n\n\n\nComments are crucial to learn R\n\n\nThe Rstudio screen has four windows or panels: 1. Console. 2. Environment and history. 3. Files, plots, packages and help. 4. The R script(s) and data view. The R script is where you keep a record of your work or commands - in a line by line basis.\n\nOn your script, type getwd() in line number â€˜2â€™ and hit the keys â€˜ctrl+enterâ€™ in your keyboard (The + indicates â€˜do it at the same timeâ€™), you dont need to press the key â€˜+â€™. After â€˜runningâ€™ line 2, you will see in the console panel the working directory (or folder) that R is using to download and upload files.\n\n\n\n\nThis image shows how your R Script should look\n\n\n\n\n\nThe outcome in the console -bottom panel after getwd()\n\n\n\n\n\nC. Set your working directory\nChoose your course file folder ğŸ“. Recall we called the working folder folder â€˜RM01_labsâ€™. In RStudioâ€™ go to â€˜Sessionâ€™, â€˜Set Working Directoryâ€™, â€˜Choose Directoryâ€™, and select this folder.\n\n\n\nCreate your folder and then click on Session&gt;Set Working Directory&gt;Choose Directory\n\n\nThe â€˜Consoleâ€™ panel renders setwd(and a path/directory to your chosen folder in your computer). Copy and paste this output in line 3 from your Console to your R Script. Setting the directory with code and not â€˜clicksâ€™ will save you time.\n\n\n\nThis is the code you shall copy and paste in your R Script, ignore â€˜&gt;â€™\n\n\n\n\n\nsetwd() in your R Script\n\n\nSo far, we have created the following script:\n# LAB ONE 25/09/2025\ngetwd()\nsetwd(\"C:/Users/YOUR DIRECTORY/RM01_labs\")\nand this is the Console output:\n&gt; # LAB ONE 25/09/2025\n&gt; getwd()\n[1] \"C:/Users/YOUR DIRECTORY/Documents\"\n&gt; setwd(\"C:/Users/YOUT DIRECTORY/RM01_labs\")\n\n\nD. First calculations & a matrix (3â€“4 min)\n\nIn your script, run: 5 * 5 (result should be 25 in the Console).\n\n\n&gt; 5*5\n[1] 25\n\n\nCreate a small matrix and run it. Use the following code, explanatory comments are written with #:\n# A is an object, \n# &lt;- is similar to equal\n# 1:8 are the numbers from 1 to 8\n# nrow is the matrix number of rows = 4\n# ncol is the matrix number of columns = 2 \nA &lt;- matrix(1:8, nrow = 4, ncol = 2)\n# The object A now has a matrix. Run the object A\nA\nThe outcome in the console shall be as follows:\n\n\n&gt; # A is an object, \n&gt;    # &lt;- is similar to equal\n&gt;    # 1:8 are the numbers from 1 to 8\n&gt;    # nrow is the matrix number of rows = 4\n&gt;    # ncol is the matrix number of columns = 2 \n&gt;    A &lt;- matrix(1:8, nrow = 4, ncol = 2)\n&gt;    # The object A now has a matrix. Run the object A\n&gt;    A\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\n\nIn brief, you created a matrix called â€˜Aâ€™, with numbers from 1 to 8, with four rows and two columns.\n\n\n\n\nE. Inspect objects & view data\n\nCheck the Environment tab to see objects youâ€™ve created.\nThe tab History shows a list of commands used do far.\n\n\n\n\nEnvironment Panel\n\n\n\nTo see the matrix A in a new window, type the command View(A) in your script and hit the keys â€˜ctrl+enterâ€™ to run that line , this will send you to a different tab. Afterwards, click on your R script to come back.\n\n# LAB ONE 25/09/2025\ngetwd()\nsetwd(\"C:/Users/Cam/OneDrive - University of Cambridge/Cambridge 2025-26/RM01_labs\")\n5*5\n# A is an object, \n# &lt;- is similar to equal\n# 1:8 are the numbers from 1 to 8\n# nrow is the matrix number of rows = 4\n# ncol is the matrix number of columns = 2 \nA &lt;- matrix(1:8, nrow = 4, ncol = 2)\n# The object A now has a matrix. Run the object A\nA\nView(A)\n\n\nF. (Optional) Install and load a package\nThe tab â€˜Packagesâ€™ shows the list of add-ons included in the installation of RStudio. Click on the tab â€˜Packagesâ€™, if you check a box next to a package, that package is loaded into R, if not, any command related to that package wonâ€™t work, you will need select it. You can also install other add-ons by clicking on the â€˜Installâ€™ icon.\nAnother way to install add-ons is to type the function install.packages(\"name of the package\"), and then you will be able to open the library of commands of that package.\n\nIn line 8, to install the package forecast, type install.packages(â€œforecastâ€) and hit ctrl+enter. If a window opens, you can hit yes to restart RStudio. See the picture below, make sure you do not get errors in the console. The sign â€˜&gt;â€™ will show when R finishes running a line.\n\n# the function to install packages in R\n# we use \"forecast\" as example\ninstall.packages(\"forecast\") # to install the package\nlibrary(forecast) # to activate the package 'forecast'\nRemember to activate the package by typing â€˜library(the package)â€™, for example, library(forecast) and ctrl+enter. This will activate all functions within the â€˜forecastâ€™ package.\nMake sure there are no error messages in the installation, and if so, make sure to solve the issue before continuing.\nThe package forecast is used to analyse and predict time series (e.g.Â yearly house prices during the last 20 years).\nScript/Data View Window:\nâ€¢ Begin scripts with a comment for title and description using the hash character (#). Anything after the hash on the same line is considered a comment and is ignored by R. â€¢ Code can continue to the next line without a special character, but only if the previous line ends in a way that suggests continuation (e.g., a comma or unclosed brackets). â€¢ To run a line of code, position the cursor on the line and press Ctrl+Enter. For multiple lines, select them and press Ctrl+Enter.\nConsole/Output Window:\nâ€¢ Itâ€™s recommended to work in script mode for reusability. â€¢ Commands outside the script context can be typed at the bottom of the console, indicated by the â€œ&gt;â€ sign. â€¢ Press Enter to execute a command. â€¢ Use the up and down arrows to revisit and edit older lines of code previously typed into the console.\n\n\n\nğŸ’¾ Save your script\nFile â†’ Save (e.g., lab_1) into your â€˜scriptâ€™ folder. Remember to save your R Scripts after each supervision, this will save you a lot time!\n\n\nâœ… Checkâ€‘off\n\nI can run code from a script (Ctrl+Enter).\nMy working directory is set via setwd() at the top of the script.\nI created A &lt;- matrix(1:8, 4, 2) and can see it in Environment.\nScript saved.\n\n\nğŸ End of Lab 1 ğŸ›‘ Remember to save your script ğŸ’¾",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Supervision 1</span>"
    ]
  },
  {
    "objectID": "supervision_1.html#lab-2-professional-development-r-certification-via-linkedin-learning",
    "href": "supervision_1.html#lab-2-professional-development-r-certification-via-linkedin-learning",
    "title": "Supervision 1",
    "section": "ğŸ§ª Lab 2 â€” Professional Development: R Certification via LinkedIn Learning",
    "text": "ğŸ§ª Lab 2 â€” Professional Development: R Certification via LinkedIn Learning\nProfessional certifications complement academic learning by providing industry-recognised credentials. For land economy graduates leading industries and entering consultancy, government, or research roles, demonstrating quantitative competency through multiple channels strengthens your professional profile and evidences commitment to continuous learning.\n\n\nLearning outcomes\nBy the end of this lab you will be able to:\n\nAccess institutional resources / LinkedIn Learning for continuous professional development\nComplete a certified R programming course that demonstrates foundational competency\nDocument your learning through professional certification pathways\nPrepare your technical environment for advanced coursework\nReflect critically on skill acquisition in academic and professional contexts\n\n\n\n\nPrerequisites\n\nUniversity credentials for LinkedIn Learning access.\n\nR and RStudio installed (youâ€™ll run a small code check).\n\n\n\n\nğŸ§ª Part A â€” Firstâ€‘time access to LinkedIn Learning (20 minutes)\nEstablish access to University-provided LinkedIn Learning resources.\n\nFollow the UIS guidance: help.uis.cam.ac.uk/service/support/training/linkedin-learning-info (opens the official instructions).\n\n\n\nChoose Login with your University account (SSO). If prompted, connect to an existing LinkedIn profile (optional, but helpful for saving certificates).\n\n\n\n\nğŸ§ª Part B â€” Course Selection and Academic Alignment\n\nIn LinkedIn Learning, search for â€œR Programmingâ€ or â€œR for Data Analysisâ€.\n\nPick a Beginner course or a Learning Path that issues a certificate upon completion.\n\nSkim the syllabus; confirm it covers: R basics (e.g.Â objects, vectors), data frames, importing data, and simple plots.\n\n\nShort beginner certificates often take only a few hours. Pick one you can finish this week.\n\nğŸ’­ Reflect: a) Provide a certificate/enrolment URL or a screenshot proving enrolment. What challenges do you anticipate in acquiring programming skills alongside your substantive coursework? b) List three new R commands and three packages you learned and one common error you corrected.\n\n\n\nğŸ§ª Part C â€” Add to your LinkedIn profile (optional but recommended)\n\nOpen your LinkedIn profile â†’ Add profile section â†’ Recommended â†’ Licenses & Certifications.\n\nName: Course title. Issuing organization: LinkedIn Learning. Credential URL: paste the certificate share link (if available).\n\nSave.\n\n\n\n\nğŸ§ª Part D â€” RStudio techâ€‘check for upcoming labs\nRun the following lines in RStudio to ensure youâ€™re ready for the following Labs.\n\nRecall you can run lines of code in R by hitting the keys ctrl+enter (commad+enter for Mac)\n\n\nR.version.string\n\n[1] \"R version 4.4.2 (2024-10-31 ucrt)\"\n\nsessionInfo()$running\n\n[1] \"Windows 10 x64 (build 19045)\"\n\n\nInstall key packages (run once):\n\n# install all packages at once\ninstall.packages(c(\"learnr\", \"r4ds.tutorials\", \"readr\", \"readxl\", \"ggplot2\"))\n\nVerify they load and basic plotting works:\n\n# Create a small data frame with x = 1,...,10 and y = sqrt(x).\n# We'll use this to do a quick \"tech check\" with a base R plot.\ndf &lt;- data.frame(x = 1:10, y = (1:10)^0.5)\n\n# Base R line plot:\n# - type = \"l\" draws a line (instead of points).\n# - main/xlab/ylab set the title and axis labels.\n# This confirms base graphics are working and shows a concave curve of sqrt(x).\nplot(df$x, df$y, type = \"l\",\n     main = \"Tech check: base plot\",\n     xlab = \"x\",\n     ylab = \"sqrt(x)\")\n\n\n\n\n\n\n\n\n\n# -----------------------------------------------\n# Test basic plotting capability with ggplot2\n# -----------------------------------------------\n\n# Load ggplot2 (grammar of graphics) for layered, customizable plots.\nlibrary(ggplot2)\n\n# Create a toy dataset: yearly \"average house prices\" (hypothetical numbers).\n# This lets us test lines, points, labels, and themes in ggplot2.\ntest_data &lt;- data.frame(\n  year = 2010:2020,\n  house_prices = c(250000, 260000, 275000, 285000, 290000, \n                   310000, 325000, 340000, 355000, 370000, 385000)\n)\n\n# Start a ggplot:\n# - aes(x = year, y = house_prices) maps variables to axes.\n# - geom_line() draws the trend line; linewidth controls thickness.\n# - geom_point() adds markers at each year.\n# - scale_y_continuous(labels = scales::comma_format()) formats y-axis with commas (e.g., 310,000).\n# - labs() sets a clear title, subtitle, axis labels, and a caption.\n# - theme_minimal() gives a clean, publication-style look.\nggplot(test_data, aes(x = year, y = house_prices)) +\n  geom_line(colour = \"steelblue\", linewidth = 1.2) +   # trend line\n  geom_point(colour = \"darkred\", size = 2) +           # points on each year\n  scale_y_continuous(labels = scales::comma_format()) +# pretty y-axis labels\n  labs(\n    title = \"Hypothetical House Price Trends\",         # main title\n    subtitle = \"System functionality test\",            # context/subtitle\n    x = \"Year\",                                        # x-axis label\n    y = \"Average House Price (Â£)\",                     # y-axis label\n    caption = \"Test data for R/RStudio verification\"   # caption under the plot\n  ) +\n  theme_minimal()                                      # clean theme\n\n\n\n\n\n\n\n\nğŸ’­ Reflect: Did everything install and run without warnings? Capture any errors so we can fix them in class.\n\n\n\nBest practices\n\nPractice while you watchâ€”replicate examples in RStudio, donâ€™t just read the labs and watch videos.\n\nKeep a snippets file (your personal cheatâ€‘sheet) with commands youâ€™ll reuse.\n\nStore all artifacts (notes, PDFs) in your course project folder for quick reference.\n\n\nğŸ End of Lab 2 ğŸ›‘ Remember to save your script ğŸ’¾",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Supervision 1</span>"
    ]
  },
  {
    "objectID": "supervision_1.html#lab-3-interactive-r-learning-building-programming-competency",
    "href": "supervision_1.html#lab-3-interactive-r-learning-building-programming-competency",
    "title": "Supervision 1",
    "section": "ğŸ§ª Lab 3 â€” Interactive R Learning: Building Programming Competency",
    "text": "ğŸ§ª Lab 3 â€” Interactive R Learning: Building Programming Competency\n\n\nLearning outcomes\nThis lab exposes you to different pedagogical approaches in quantitative methods education while building the practical skills necessary for data-driven research in land economy.\nBy the end of this lab you will be able to:\n\nNavigate multiple R learning platforms with different pedagogical approaches\nComplete structured programming exercises with immediate feedback\nDevelop troubleshooting skills through guided problem-solving\nAcquire multiple ways of self-development for your own study plan.\n\n\n\n\nPrerequisites\n\nR (â‰¥ 4.1) and RStudio.\nPackages (install once):\n\n\ninstall.packages(c(\"learnr\", \"r4ds.tutorials\", \"swirl\"))\n\n\nIf tutorials donâ€™t appear in RStudioâ€™s Tutorial tab after installation, restart R.\n\n\n\n\nPart A â€” r4ds.tutorials: 01-data-visualization\nThis runs as an interactive tutorial in RStudioâ€™s Tutorial pane.\n\n# Option 1: via code\nlearnr::run_tutorial(\"01-data-visualization\", package = \"r4ds.tutorials\")\n\n# Option 2: discover tutorials installed on your system\nlearnr::available_tutorials(\"r4ds.tutorials\")\n\nDo this: 1. Start and complete the tutorial on data visualization. Use the Show in New Window icon if the pane is small. 2. When you reach the Submit page, follow its instructions and save the html (we wonâ€™t collect submissions in this lab, just finish to the end).\nğŸ’­ Reflect: - What topic would benefit from additional practice or explanation? e.g.Â aesthetics (e.g., aes(x, y, color)) and geoms (e.g., geom_point(), geom_line())\n\n\n\nPart B â€” learnr: ex-data-basics\nThis is a short core tutorial bundled with learnr.\n\n# open interactive tutorial in the package 'learnr'\nlearnr::run_tutorial(\"ex-data-basics\", package = \"learnr\")\n\n# Explore other tutorials\nlearnr::run_tutorial(, package = \"learnr\")\n\nDo this: Complete all exercises for the tutorial â€˜ex-data-basicsâ€™..\nğŸ’­ Reflect: - What is the difference between a tibble and a data frame? - Why might tibbles be preferred in modern R workflows?\n\n\n\nPart C â€” swirl (consoleâ€‘based)\nswirl runs entirely in the Console and saves your progress.\n\n# Install/run swirl\ninstall.packages(\"swirl\")\nlibrary(swirl)\n\n# Start swirl\nswirl()\n# (enter your name when prompted)\n\n# Install and start a course (first time only)\ninstall_from_swirl(\"R Programming\")   # or \"R_Programming\"\n\n# At any time, leave swirl with\n# bye()\n\n# Check progress later\nswirl::progress()\n\nDo this: - Install R Programming and complete the first lesson. - Exit with bye() and verify progress with swirl::progress().\nğŸ’­ Reflect: - How does working in the Console (swirl) feel compared with the GUI tutorials?\n\n\n\nPart D â€” Mini practice\nCreate a tiny vector and compute a statistic, then plot a quick line:\n\n# create variable x\nx &lt;- 1:10\n#calculate the mean and asign it to mean_x\nmean_x &lt;- mean(x)\n# review mean_x\nmean_x\n\n[1] 5.5\n\n# quick plotting function\nplot(x, type = \"l\", main = \"Quick line\", ylab = \"x\")\n# abline adds horizontal (h) or vertical (v) lines.\nabline(h = mean_x, lty = 2)\n\n\n\n\n\n\n\n\nğŸ’­ Reflect: - Which approach (r4ds.tutorials, learnr, swirl) best prepared you to learn R coding independently?\n\n\n\nBest practices\n\nUse learnr/r4ds.tutorials for guided practice with hints, autoâ€‘checks, and saved state.\nUse swirl when you prefer keyboardâ€‘only Console practice or have limited UI.\nRestart RStudio if tutorials donâ€™t show; list whatâ€™s available with learnr::available_tutorials().\nKeep your scripts open alongside tutorials to copy refined solutions into your own notes.\n\n\nğŸ End of Lab 3 ğŸ›‘ Remember to save your script ğŸ’¾",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Supervision 1</span>"
    ]
  },
  {
    "objectID": "supervision_1.html#lab-4-uploading-excel-csv-files-to-r-pipr",
    "href": "supervision_1.html#lab-4-uploading-excel-csv-files-to-r-pipr",
    "title": "Supervision 1",
    "section": "ğŸ§ª Lab 4 â€” Uploading Excel & CSV Files to R (PIPR)",
    "text": "ğŸ§ª Lab 4 â€” Uploading Excel & CSV Files to R (PIPR)\n\n\nLearning outcomes\nBy the end of this lab you will be able to: 1. Download the Private rents annual inflation to August 2024 monthly workbook and save it in a tidy project structure. 2. Import Excel (.xlsx/.xls) and CSV files using RStudioâ€™s Import Dataset UI and reproducible R code. 3. Deal with metadata/header rows via skip, set col_names, and verify column types. 4. Plot time series (index and annual % change) with base R and add reference lines. 5. Use help pages (e.g., ?plot) and annotate charts for a policy audience.\n\n\n\nPrerequisites\n\nR (â‰¥ 4.0) and RStudio.\nProject folder (e.g., RM01_labs/) with subfolder data/.\nPackages (install once):\n\n\n# Make sure to install the packages.\n# install.packages(c(\"readxl\", \"readr\"))\n\n\nğŸ’¡ Tip: Prefer file names without spaces and lower-cases, e.g.Â pipr_monthly.xls rather than pipr monthly.xls.\n\n\n\n\nPart A â€” Get PIPR monthly from ONS (manual steps)\n\nGo to ONS.gov.uk and search Private rents annual inflation, UK countries, January 2016 to August 2024. This lab is using ONS data on Figure 4: Rent annual inflation slowed across the UK, Private rents annual inflation, across the UK, January 2016 to August 2025.\n\n\n\n\nThis is the website / data source.\n\n\n\n\n\nThis is the the data we used in the tutorial.\n\n\n\nOn the dataset page, choose Data â†’ Download monthly workbook. Save the file as Excel and CSV Comma delimited.\nSave as data/pipr_monthly.xls inside your project folder (e.g., RM01_labs/) with subfolder data/.\n(Optional) Open the file and identify the sheet that contains the UK and country/regions.\n\nğŸ’­ Reflect: Why does PIPR (rents) provide a more direct lens on housing affordability and regional pressures than GDP for planning and regeneration?\n\n\n\nPart B â€” Import Excel via code (reproducible)\nUse readxl::read_excel() and inspect column names to choose the UK series.\n\nlibrary(readxl)\n\nWarning: package 'readxl' was built under R version 4.4.3\n\n# Adjust 'sheet' and 'skip' depending on the workbook structure you download.\n# we use skip = 7 because there are headings in the first 7 rows of the dataset.\npipr_raw &lt;- read_excel(\"data/pipr_monthly.xls\", sheet = 1, skip = 7)\n\n# Inspect names to locate the date/month, UK Index, and UK annual % change columns\nnames(pipr_raw)\n\n[1] \"Date\"             \"UK\"               \"England\"          \"Wales\"           \n[5] \"Scotland\"         \"Northern Ireland\"\n\n# A good practice is to keep a copy of the raw data and create a new version of the data for manipulation.\n\npipr&lt;-pipr_raw\n\n# rename column names for easiness. \nnames(pipr) &lt;- c(\"month\", \"uk\", \"england\", \"wales\", \"scotland\", \"n_ireland\")\n\nstr(pipr)\n\ntibble [116 Ã— 6] (S3: tbl_df/tbl/data.frame)\n $ month    : POSIXct[1:116], format: \"2016-01-01\" \"2016-02-01\" ...\n $ uk       : num [1:116] 3.3 3.3 3.4 3.3 3.2 3.3 3.4 3.3 3.2 3.3 ...\n $ england  : num [1:116] 3.5 3.5 3.6 3.5 3.3 3.5 3.7 3.6 3.5 3.6 ...\n $ wales    : num [1:116] 1.2 0.8 0.8 0.6 0.4 0.3 0.3 0.9 1.4 1.3 ...\n $ scotland : num [1:116] 2.1 1.8 1.6 1.5 1.4 1.1 1.2 0.9 1.1 1.1 ...\n $ n_ireland: num [1:116] 1.1 1.3 1.4 1.6 1.6 1.6 1.5 1.5 1 0.9 ...\n\nhead(pipr, 6)\n\n# A tibble: 6 Ã— 6\n  month                  uk england wales scotland n_ireland\n  &lt;dttm&gt;              &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 2016-01-01 00:00:00   3.3     3.5   1.2      2.1       1.1\n2 2016-02-01 00:00:00   3.3     3.5   0.8      1.8       1.3\n3 2016-03-01 00:00:00   3.4     3.6   0.8      1.6       1.4\n4 2016-04-01 00:00:00   3.3     3.5   0.6      1.5       1.6\n5 2016-05-01 00:00:00   3.2     3.3   0.4      1.4       1.6\n6 2016-06-01 00:00:00   3.3     3.5   0.3      1.1       1.6\n\n\n\n\n\nPart C â€” Import via RStudio UI (reference)\n\nGo to RStudio â†’ Import Dataset â†’ From Excel â†’ select data/pipr_monthly.xls.\n\n\n\n\nimporting from Rstudio with â€˜import datasetâ€™\n\n\n\nChoose the relevant sheet and, if needed, set Skip for metadata rows (recall we used skip = 7 in Part B).\n\n\n\n\nImport Ecvel data window\n\n\n\nSet Name to pipr_monthly and verify column names (e.g., month, uk_index, uk_yoy).\nClick Copy to grab the generated R code, then Import.\n\nPaste the generated code in your R script so your workflow is reproducible.\n\n\n\nPart D â€” Basic timeâ€‘series line plot (annual % change)\nThe following code plots PIPR annual % change for the UK as a line. Label the xâ€‘axis sparsely to keep it readable.\n\n# Create a line plot of UK private rent annual % changes\nplot(pipr$uk, type = \"l\",  # type = \"l\" means line plot\n     xlab = \"\",  # empty x-axis label (we'll customize it below)\n     ylab = \"Private rents â€” annual % change\",  # y-axis label\n     main = \"UK private rents (PIPR): annual % change\",  # chart title\n     xaxt = \"n\")  # suppress default x-axis (we'll add custom labels)\n\n# Add sparse x-axis labels (every 6th data point to avoid overcrowding)\nlabs_every &lt;- 6  # show a label every 6 months\nat_idx &lt;- seq(1, nrow(pipr), by = labs_every)  # create sequence of index positions\naxis(1,  # 1 means x-axis (bottom)\n     at = at_idx,  # positions where labels appear\n     labels = pipr$month[at_idx],  # the actual month labels\n     las = 2,  # rotate labels perpendicular to axis\n     cex.axis = 0.7)  # make label font smaller (70% of default)\n\n# Add subtitle showing data source directly under the title\ntitle(sub = \"Source: ONS - Index of Private Housing Rental Prices (PIPR)\", \n      cex.sub = 0.8,  # 80% of normal text size\n      font.sub = 3)  # 3 means italic font\n\n# Add horizontal reference line at 2% (Bank of England inflation target)\nabline(h = 2,  # horizontal line at y = 2\n       lty = 2,  # line type 2 = dashed\n       col = \"blue\")  # blue color\n\n# Add text label for the reference line\ntext(x = 40,  # position at start of time series\n     y = 2,  # at the 2% level\n     labels = \"Inflation target (2%)\",  # the text to display\n     pos = 3,  # position 3 = above the point\n     col = \"blue\",  # match the line color\n     cex = 0.8)  # slightly smaller text (80% of default)\n\n\n\n\n\n\n\n\nğŸ’­ Reflect: Identify periods of fastest rent inflation and periods of slowdown. What macro factors might line up with these shifts?\n\n\n\nPart E â€” Import PIPR as CSV (alternative)\nRecall we downloaded the CSV version from the PIPR page. Import the file with readr::read_csv().\n\n# Open the library to read csv comma delimited files.\nlibrary(readr)\n# Create the object pipr_csv to store the data\npipr_csv &lt;- read_csv(\"data/pipr_monthly.csv\", skip =7)  # adjust file name\n\nRows: 116 Columns: 6\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\nchr (1): Date\ndbl (5): UK, England, Wales, Scotland, Northern Ireland\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# colums/variable names\nnames(pipr_csv)\n\n[1] \"Date\"             \"UK\"               \"England\"          \"Wales\"           \n[5] \"Scotland\"         \"Northern Ireland\"\n\n# quick overview of the data (tibble)\nhead(pipr_csv)\n\n# A tibble: 6 Ã— 6\n  Date      UK England Wales Scotland `Northern Ireland`\n  &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;              &lt;dbl&gt;\n1 Jan-16   3.3     3.5   1.2      2.1                1.1\n2 Feb-16   3.3     3.5   0.8      1.8                1.3\n3 Mar-16   3.4     3.6   0.8      1.6                1.4\n4 Apr-16   3.3     3.5   0.6      1.5                1.6\n5 May-16   3.2     3.3   0.4      1.4                1.6\n6 Jun-16   3.3     3.5   0.3      1.1                1.6\n\n\n\n\n\nPart G â€” Compare UK with a region\nThe workbook includes regions (England, Wales, Scotland, Northern Ireland), select one annual % change column and plot together with the UK.\n\nplot(\n    pipr$uk, type = \"l\", lwd = 2,\n    xlab = \"Month\", ylab = \"Annual % change\",\n    main = \"PIPR annual % change: UK vs UK countries to Aug 2025\",\n    ylim = c(0,12)\n  )\n# Use the function 'lines' to add the time series to the initial plot.\nlines(pipr$england, lwd= 1, col=\"red\")\nlines(pipr$scotland, lwd= 1, col=\"blue\")\nlines(pipr$wales, lwd= 1, col=\"green\")\nlines(pipr$n_ireland, lwd= 1, col=\"grey\")\n\nlegend(\n  \"topleft\",\n  legend = c(\"UK\", \"England\", \"Scotland\", \"Wales\", \"N. Ireland\"),\n  col    = c(\"black\", \"red\", \"blue\", \"green\", \"grey\"),\n  lwd    = c(2, 1, 1, 1, 1),\n  lty    = 1,\n  bty    = \"n\"       # no box around legend (optional)\n  # inset = 0.02     # nudge inward if needed\n)\n\n\n\n\n\n\n\n\nğŸ’­ Reflect: Did the countries move broadly with the UK or diverge materially? What local factors could explain divergence?\n\n\n\nPart H â€” Help pages and plotting extras\nLetâ€™s explore ?plot in the console. This will launch a help page in the help tab.\nWe also explore curve() and abline().\n\n# ?plot   # open during interactive session\ncurve(sin(x), from = 0, to = 6.28, xlab = \"x\", ylab = \"y = sin(x)\")\nabline(h = 0, v = 5, lty = 2)\n\n\n\n\n\n\n\n\nğŸ’­ Reflect: Which plot() arguments improved readability most? Explain the use of las, xaxt, lwd, and xlim.\n\n\n\nPart I â€” Access the data of â€˜Principles of Econometricsâ€™\nYou can also explore textbook datasets with PoEdata for practice with scatter plots and abline(lm()).\n\n#install.packages(c(\"remotes\", \"pkgbuild\"))   # helper packages\n#pkgbuild::has_build_tools(debug = TRUE)       # should say TRUE on Windows\n#remotes::install_github(\"ccolonescu/PoEdata\")\nlibrary(PoEdata)\ndata()\ndata(\"andy\")\nhead(andy)\n\n  sales price advert\n1  73.2  5.69    1.3\n2  71.8  6.49    2.9\n3  62.4  5.63    0.8\n4  67.4  6.22    0.7\n5  89.3  5.02    1.5\n6  70.3  6.41    1.3\n\n\n\n\n\nBest practices\n\nReproducibility: Prefer code over manual spreadsheet edits; paste RStudioâ€™s generated import code into your script.\nPaths & naming: Use a data/ subfolder; avoid spaces; use forward slashes.\nTypes & missing values: Check with str(), summary(), anyNA(); coerce explicitly when needed.\nAxis labelling: Sparse, rotated labels help on monthly series.\nVersioning: Save dated copies of raw downloads (e.g., pipr_monthly_2025-09-19.xlsx).\n\n\nğŸ End of Lab 4\nğŸ›‘ Remember to save your script ğŸ’¾",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Supervision 1</span>"
    ]
  },
  {
    "objectID": "supervision_1.html#lab-5-data-visualisation",
    "href": "supervision_1.html#lab-5-data-visualisation",
    "title": "Supervision 1",
    "section": "ğŸ§ª Lab 5 â€” Data Visualisation",
    "text": "ğŸ§ª Lab 5 â€” Data Visualisation\nAdapted from: A Guide to Data Visualisation in R for Beginners\n\n\nLearning outcomes\nBy the end of this lab you will be able to:\n\nExplore the state.x77 dataset in R.\nGenerate basic descriptive statistics.\nCreate simple plots using base R graphics.\nAdd labels, titles, and colours to plots.\nCompare different chart types (scatter, bar, histogram, boxplot).\nUse multi-panel displays to visualise multiple plots at once.\n\n\n\n\nPrerequisites\n\nR (â‰¥ 4.0) and RStudio.\nNo additional packages required beyond base R (optional: ggplot2 for comparison).\n\n\n\n\nPart A â€” Load dataset\n\nlibrary(datasets)\nstate_data &lt;- as.data.frame(datasets::state.x77)\nView(state_data)\n\n\n\n\nPart B â€” Data exploration\n\nstr(state_data)\n\n'data.frame':   50 obs. of  8 variables:\n $ Population: num  3615 365 2212 2110 21198 ...\n $ Income    : num  3624 6315 4530 3378 5114 ...\n $ Illiteracy: num  2.1 1.5 1.8 1.9 1.1 0.7 1.1 0.9 1.3 2 ...\n $ Life Exp  : num  69 69.3 70.5 70.7 71.7 ...\n $ Murder    : num  15.1 11.3 7.8 10.1 10.3 6.8 3.1 6.2 10.7 13.9 ...\n $ HS Grad   : num  41.3 66.7 58.1 39.9 62.6 63.9 56 54.6 52.6 40.6 ...\n $ Frost     : num  20 152 15 65 20 166 139 103 11 60 ...\n $ Area      : num  50708 566432 113417 51945 156361 ...\n\n\nğŸ’­ Reflect: Explore possible research aims from the state.x77 dataset. What variables are present in state.x77? How many states (rows) and variables (columns) are there? Which are continuous, and which categorical (if any)?\n\n\n\nPart C â€” Beginning and end of the dataset\n\nhead(state_data, 7)\n\n            Population Income Illiteracy Life Exp Murder HS Grad Frost   Area\nAlabama           3615   3624        2.1    69.05   15.1    41.3    20  50708\nAlaska             365   6315        1.5    69.31   11.3    66.7   152 566432\nArizona           2212   4530        1.8    70.55    7.8    58.1    15 113417\nArkansas          2110   3378        1.9    70.66   10.1    39.9    65  51945\nCalifornia       21198   5114        1.1    71.71   10.3    62.6    20 156361\nColorado          2541   4884        0.7    72.06    6.8    63.9   166 103766\nConnecticut       3100   5348        1.1    72.48    3.1    56.0   139   4862\n\ntail(state_data, 5)\n\n              Population Income Illiteracy Life Exp Murder HS Grad Frost  Area\nVirginia            4981   4701        1.4    70.08    9.5    47.8    85 39780\nWashington          3559   4864        0.6    71.72    4.3    63.5    32 66570\nWest Virginia       1799   3617        1.4    69.48    6.7    41.6   100 24070\nWisconsin           4589   4468        0.7    72.48    3.0    54.5   149 54464\nWyoming              376   4566        0.6    70.29    6.9    62.9   173 97203\n\n\n\n\n\nPart D â€” Descriptive statistics\nThere are several functions to obtain descriptive statistics, summary, psych, skimr, etc.\nHere is an example with summary.\n\nsummary(state_data)\n\n   Population        Income       Illiteracy       Life Exp    \n Min.   :  365   Min.   :3098   Min.   :0.500   Min.   :67.96  \n 1st Qu.: 1080   1st Qu.:3993   1st Qu.:0.625   1st Qu.:70.12  \n Median : 2838   Median :4519   Median :0.950   Median :70.67  \n Mean   : 4246   Mean   :4436   Mean   :1.170   Mean   :70.88  \n 3rd Qu.: 4968   3rd Qu.:4814   3rd Qu.:1.575   3rd Qu.:71.89  \n Max.   :21198   Max.   :6315   Max.   :2.800   Max.   :73.60  \n     Murder          HS Grad          Frost             Area       \n Min.   : 1.400   Min.   :37.80   Min.   :  0.00   Min.   :  1049  \n 1st Qu.: 4.350   1st Qu.:48.05   1st Qu.: 66.25   1st Qu.: 36985  \n Median : 6.850   Median :53.25   Median :114.50   Median : 54277  \n Mean   : 7.378   Mean   :53.11   Mean   :104.46   Mean   : 70736  \n 3rd Qu.:10.675   3rd Qu.:59.15   3rd Qu.:139.75   3rd Qu.: 81163  \n Max.   :15.100   Max.   :67.30   Max.   :188.00   Max.   :566432  \n\n\nAnd, another example with the package psych.\n\n#install.packages(\"psych\")   # if not already installed\nlibrary(psych)\n\ndescribe(state_data)\n\n           vars  n     mean       sd   median  trimmed      mad     min\nPopulation    1 50  4246.42  4464.49  2838.50  3384.28  2890.33  365.00\nIncome        2 50  4435.80   614.47  4519.00  4430.08   581.18 3098.00\nIlliteracy    3 50     1.17     0.61     0.95     1.10     0.52    0.50\nLife Exp      4 50    70.88     1.34    70.67    70.92     1.54   67.96\nMurder        5 50     7.38     3.69     6.85     7.30     5.19    1.40\nHS Grad       6 50    53.11     8.08    53.25    53.34     8.60   37.80\nFrost         7 50   104.46    51.98   114.50   106.80    53.37    0.00\nArea          8 50 70735.88 85327.30 54277.00 56575.72 35144.29 1049.00\n                max     range  skew kurtosis       se\nPopulation  21198.0  20833.00  1.92     3.75   631.37\nIncome       6315.0   3217.00  0.20     0.24    86.90\nIlliteracy      2.8      2.30  0.82    -0.47     0.09\nLife Exp       73.6      5.64 -0.15    -0.67     0.19\nMurder         15.1     13.70  0.13    -1.21     0.52\nHS Grad        67.3     29.50 -0.32    -0.88     1.14\nFrost         188.0    188.00 -0.37    -0.94     7.35\nArea       566432.0 565383.00  4.10    20.39 12067.10\n\n\nHowever, some users writing research might want more elaborated outputs.\n\nlibrary(dplyr)\nlibrary(gtsummary)\nlibrary(flextable)\n\n# Recreate data inside this chunk (Quarto runs in a clean session)\nstate_data &lt;- as.data.frame(datasets::state.x77)\nstate_data$region &lt;- factor(datasets::state.region)  # add stratifier\nnames(state_data) &lt;- make.names(names(state_data))   # Life.Exp, HS.Grad, etc.\n\ntbl &lt;- state_data |&gt;\n  gtsummary::tbl_summary(\n    by = region,\n    type = gtsummary::all_continuous() ~ \"continuous2\",\n    statistic = gtsummary::all_continuous() ~ c(\"{mean} ({sd})\", \"{median} [{p25}, {p75}]\"),\n    digits = gtsummary::all_continuous() ~ 1,\n    missing = \"ifany\",\n    label = list(\n      Income ~ \"Per-capita Income\",\n      Illiteracy ~ \"Illiteracy (%)\",\n      Life.Exp ~ \"Life Expectancy\",\n      Murder ~ \"Murder Rate\",\n      HS.Grad ~ \"High-School Grad (%)\",\n      Frost ~ \"Frost Days\",\n      Area ~ \"Area (sq mi)\"\n    )\n  ) |&gt;\n  gtsummary::add_overall(last = TRUE) |&gt;\n  gtsummary::add_p() |&gt;\n  gtsummary::bold_labels() |&gt;\n  gtsummary::modify_caption(\"**Table: Descriptive statistics by region**\")\n\ntbl\n\n\n\nTableÂ 2.1: Table: Descriptive statistics by region\n\n\n\n\n\n\n\nTable: Descriptive statistics by region\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nNortheast\nN = 9\nSouth\nN = 16\nNorth Central\nN = 12\nWest\nN = 13\nOverall\nN = 50\np-value1\n\n\n\n\nPopulation\n\n\n\n\n\n\n\n\n\n\n0.054\n\n\nÂ Â Â Â Mean (SD)\n5,495.1 (6,079.6)\n4,208.1 (2,779.5)\n4,803.0 (3,702.8)\n2,915.3 (5,578.6)\n4,246.4 (4,464.5)\n\n\n\n\nÂ Â Â Â Median [Q1, Q3]\n3,100.0 [931.0, 7,333.0]\n3,710.5 [2,528.0, 4,956.0]\n4,255.0 [1,912.0, 7,212.0]\n1,144.0 [746.0, 2,284.0]\n2,838.5 [1,058.0, 4,981.0]\n\n\n\n\nPer-capita Income\n\n\n\n\n\n\n\n\n\n\n0.019\n\n\nÂ Â Â Â Mean (SD)\n4,570.2 (559.1)\n4,011.9 (605.5)\n4,611.1 (283.1)\n4,702.6 (663.9)\n4,435.8 (614.5)\n\n\n\n\nÂ Â Â Â Median [Q1, Q3]\n4,558.0 [4,281.0, 4,903.0]\n3,848.0 [3,620.5, 4,444.5]\n4,594.5 [4,463.0, 4,713.0]\n4,660.0 [4,347.0, 4,963.0]\n4,519.0 [3,983.0, 4,815.0]\n\n\n\n\nIlliteracy (%)\n\n\n\n\n\n\n\n\n\n\n&lt;0.001\n\n\nÂ Â Â Â Mean (SD)\n1.0 (0.3)\n1.7 (0.6)\n0.7 (0.1)\n1.0 (0.6)\n1.2 (0.6)\n\n\n\n\nÂ Â Â Â Median [Q1, Q3]\n1.1 [0.7, 1.1]\n1.8 [1.4, 2.2]\n0.7 [0.6, 0.8]\n0.6 [0.6, 1.5]\n1.0 [0.6, 1.6]\n\n\n\n\nLife Expectancy\n\n\n\n\n\n\n\n\n\n\n&lt;0.001\n\n\nÂ Â Â Â Mean (SD)\n71.3 (0.7)\n69.7 (1.0)\n71.8 (1.0)\n71.2 (1.4)\n70.9 (1.3)\n\n\n\n\nÂ Â Â Â Median [Q1, Q3]\n71.2 [70.6, 71.8]\n70.1 [68.9, 70.4]\n72.3 [70.8, 72.6]\n71.7 [70.3, 72.1]\n70.7 [70.1, 71.9]\n\n\n\n\nMurder Rate\n\n\n\n\n\n\n\n\n\n\n&lt;0.001\n\n\nÂ Â Â Â Mean (SD)\n4.7 (2.7)\n10.6 (2.6)\n5.3 (3.6)\n7.2 (2.7)\n7.4 (3.7)\n\n\n\n\nÂ Â Â Â Median [Q1, Q3]\n3.3 [3.1, 5.5]\n10.9 [9.0, 12.4]\n3.8 [2.3, 8.4]\n6.8 [5.0, 9.7]\n6.9 [4.3, 10.7]\n\n\n\n\nHigh-School Grad (%)\n\n\n\n\n\n\n\n\n\n\n&lt;0.001\n\n\nÂ Â Â Â Mean (SD)\n54.0 (3.9)\n44.3 (5.7)\n54.5 (3.6)\n62.0 (3.5)\n53.1 (8.1)\n\n\n\n\nÂ Â Â Â Median [Q1, Q3]\n54.7 [52.5, 57.1]\n41.7 [40.3, 49.7]\n53.3 [52.7, 58.3]\n62.6 [59.5, 63.9]\n53.3 [47.8, 59.2]\n\n\n\n\nFrost Days\n\n\n\n\n\n\n\n\n\n\n&lt;0.001\n\n\nÂ Â Â Â Mean (SD)\n132.8 (30.9)\n64.6 (31.3)\n138.8 (23.9)\n102.2 (68.9)\n104.5 (52.0)\n\n\n\n\nÂ Â Â Â Median [Q1, Q3]\n127.0 [115.0, 161.0]\n67.5 [42.5, 90.0]\n133.0 [123.0, 154.5]\n126.0 [32.0, 155.0]\n114.5 [65.0, 140.0]\n\n\n\n\nArea (sq mi)\n\n\n\n\n\n\n\n\n\n\n&lt;0.001\n\n\nÂ Â Â Â Mean (SD)\n18,141.0 (18,075.7)\n54,605.1 (57,965.3)\n62,652.0 (14,967.0)\n134,463.0 (134,981.7)\n70,735.9 (85,327.3)\n\n\n\n\nÂ Â Â Â Median [Q1, Q3]\n9,027.0 [7,521.0, 30,920.0]\n46,113.0 [34,937.5, 53,017.5]\n62,906.0 [55,106.0, 76,219.0]\n103,766.0 [82,677.0, 121,412.0]\n54,277.0 [36,097.0, 81,787.0]\n\n\n\n\n\n1 Kruskal-Wallis rank sum test\n\n\n\n\n\n\n\n\n\n\n\nğŸ’­ Reflect: Which socio-economic indicators show the widest spread?\n\n\n\nPart E â€” Graphics package\nReview other type of graphics or plotting functions that can be useful for land economy research. The folloging code opens a new tab with more details.\n\nlibrary(help = \"graphics\")\n\n\n\n\nPart F â€” Scatterplots\n\nplot(state_data$Income, state_data$Illiteracy)\n\n\n\n\n\n\n\n\nğŸ’­ Reflect: Do states with higher income tend to have lower illiteracy rates?\n\n\n\nPart G â€” Entire dataset plot\n\nplot(state_data)\n\n\n\n\n\n\n\n\nğŸ’­ Reflect: Which pairs of socio-economic indicators appear strongly related?\n\n\n\nPart H â€” Points and lines\n\nplot(state_data$Life.Exp, type = \"b\")\n\n\n\n\n\n\n\nplot(state_data$Life.Exp, type = \"h\")\n\n\n\n\n\n\n\n\nğŸ’­ Reflect: Which representation communicates variation in life expectancy more clearly?\n\n\n\nPart J â€” Help\n\n?plot\n\nstarting httpd help server ... done\n\n\nğŸ’­ Reflect: Identify one new argument from the help page that could improve your plot.\n\n\n\nPart K â€” Labels and titles\n\nplot(state_data$Income,\n     xlab = \"State Index\",\n     ylab = \"Per Capita Income\",\n     main = \"Income levels across US states\",\n     col = \"blue\")\n\n\n\n\n\n\n\n\nğŸ’­ Reflect: Do the labels improve clarity? Suggest improvements if needed.\n\n\n\nPart L â€” Horizontal bar plot\n\n# Take the Murder column as a named vector (names = state names)\nm &lt;- state_data$Murder\nnames(m) &lt;- rownames(state_data)\n\n# Pick the top 10 states by murder rate (highest first)â€¦\nm_top &lt;- sort(m, decreasing = TRUE)[1:10]\n# â€¦then re-sort ascending so the biggest bar appears at the TOP in a horizontal plot\nm_top &lt;- sort(m_top)\n\n# Make the plot: horizontal bars, tidy labels\nbp &lt;- barplot(\n  m_top,                                # bar heights (uses names(m_top) as labels)\n  horiz = TRUE,                         # horizontal bars\n  las = 1,                              # horizontal axis labels\n  cex.names = 0.8,                      # slightly smaller labels\n  space = 0.02,                         # small gaps between bars\n  col = rainbow(length(m_top)),         # one colour per bar (top 10 only)\n  main = \"Top 10 States by Murder Rate\",\n  xlab = \"Murder arrests per 100,000\",\n  xlim = c(0, max(m_top) * 1.1)         # a bit of right padding\n)\n\n\n\n\n\n\n\n\nğŸ’­ Reflect: - Which states stand out with particularly high murder rates? - Can you adapt the script to barplot the 10 states with lower murder rates?\n\n\n\nPart M â€” Vertical bar plot\nWe could use the previous script to barplot vertically by changing the option from horiz = TRUE to horiz = FALSEas in the following routine/code.\n\nbarplot(state_data$Illiteracy,\n        main = \"Illiteracy Rate by State\",\n        xlab = \"% Illiteracy\",\n        col = \"orange\",\n        horiz = FALSE)\n\n\n\n\n\n\n\n\nğŸ’­ Reflect: How might this plot inform discussions on educational policy?\n\n\n\nPart N â€” Histograms\n\nhist(state_data$Income,\n     main = \"Distribution of State Incomes\",\n     xlab = \"Per Capita Income\",\n     col = \"green\")\n\n\n\n\n\n\n\n\nğŸ’­ Reflect: Is the income distribution symmetric, skewed, or multi-modal?\n\n\n\nPart O â€” Boxplots\n\nboxplot(state_data$Life.Exp)\n\n\n\n\n\n\n\n\nğŸ’­ Reflect: What does the boxplot reveal about state-level life expectancy?\n\n\n\nPart P â€” Multiple boxplots\n\nboxplot(state_data[, c(\"Life.Exp\", \"Murder\", \"HS.Grad\", \"Frost\")],\n        main = \"Socio-economic Indicators across States\")\n\n\n\n\n\n\n\n\nğŸ’­ Reflect: Which variable shows the most variability? The least?\n\n\n\nPart Q â€” Grid of charts\nWe can create a grid of charts with the function par; (3,3) means a grid made of 3 rows x 3 columns.\n\npar(mfrow = c(3,3), mar = c(2,5,2,1), las = 1, bty = \"n\")\n\nplot(state_data$Population)\nplot(state_data$Income, state_data$Illiteracy)\nplot(state_data$Life.Exp, type = \"c\")\nplot(state_data$Life.Exp, type = \"s\")\nplot(state_data$Life.Exp, type = \"h\")\n\nbarplot(state_data$Income, main = \"Income levels\", col = \"blue\", horiz = TRUE)\nhist(state_data$Murder)\nboxplot(state_data$HS.Grad)\nboxplot(state_data[, c(\"Population\", \"Income\", \"Illiteracy\", \n\"Life.Exp\")], main = \"Multiple Box plots\")\n\n\n\n\n\n\n\n\n\n\n\nBest practices\n\nAlways check the structure of socio-economic data before plotting.\nMatch plot type to variable type.\nAdd descriptive titles and labels.\nAvoid misleading graphics (bar plots for continuous data should be used cautiously).\nVisualise relationships before modelling.\n\n\n\n\nFurther visualisation packages\n\nlattice: kernel density plots\nggplot2: flexible grammar of graphics\nplotly: interactive plots\nmaps: plot country maps\n\nMore resources: towardsdatascience.com article\n\nğŸ End of Lab 5 ğŸ›‘ Remember to save your script ğŸ’¾",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Supervision 1</span>"
    ]
  },
  {
    "objectID": "supervision_1.html#lab-6-using-github-copilot-in-rstudio",
    "href": "supervision_1.html#lab-6-using-github-copilot-in-rstudio",
    "title": "Supervision 1",
    "section": "ğŸ§ª Lab 6 â€” Using GitHub Copilot in RStudio",
    "text": "ğŸ§ª Lab 6 â€” Using GitHub Copilot in RStudio\n\nGoal\nLearn to use GitHub Copilot inside RStudio to speed up routine coding, while staying in control of quality, privacy, and reproducibility.\n\n\n\nLearning outcomes\nBy the end you will be able to:\n\nEnable and sign into GitHub Copilot in RStudio.\nAccept, reject, or modify Copilot autocomplete suggestions effectively.\nUse comment-first prompting to steer suggestions.\nDiagnose and fix typical Copilot mistakes (wrong column names, redundant args, etc.).\nApply safe-use practices (privacy, determinism, avoiding â€œvibe codingâ€).\n\n\n\n\nPrerequisites\n\nR (â‰¥ 4.3 recommended) and RStudio (a recent version with Copilot support).\nA GitHub account with Copilot access (student/educator plans are available).\nPackages: tidyverse, palmerpenguins, readxl, ggplot2.\n\ninstall.packages(c(\"tidyverse\", \"palmerpenguins\", \"readxl\", \"ggplot2\"))\n\nNote: Column names for the penguins dataset may differ depending on source. Treat this as a feature to practice Copilot-aware debugging.\n\n\n\n\nPart A â€” Set up Copilot in RStudio\n\nOpen RStudio â†’ Tools â†’ Global Options â†’ Copilot.\nEnable GitHub Copilot and sign in with GitHub.\nCreate a new R Script (File â†’ New File â†’ R Script).\nType some characters to see ghost text suggestions.\n\nTab to accept, Esc to dismiss, or keep typing to ignore.\n\n\n\n\nReflection A\nExplain in one sentence the difference between ghost text and code youâ€™ve actually inserted.\n\n\n\nPart B â€” First suggestions with ggplot2 (20 min)\nWeâ€™ll use penguins to practice.\n\nAdd a guiding comment:\n\n# Task: scatter plot of bill length (x) vs body mass (y), colour by species\n\nStart a plot:\n\nlibrary(ggplot2)\n# library(palmerpenguins)\n# penguins &lt;- palmerpenguins::penguins\n\np &lt;- ggplot(penguins, aes(x = bill_length_mm, y = body_mass_g, colour = species)) +\n  geom_point()\n\nRun and, if errors occur, inspect:\n\nglimpse(penguins)\nnames(penguins)\n\nFix names if needed (bill_length_mm â†’ bill_len).\n\n\n\nMini-exercises\n\nAdd title, caption, and axis labels:\n\n# Add a title, caption, and nicer axis labels\n\nAdd smoother:\n\n# Add a loess smoother without confidence band\n\n\nReflection B\nWhen is Copilot faster? When slower?\n\n\n\nPart C â€” Comment-first prompting for histograms\n\nAdd a guiding comment:\n\n# Histogram of body mass by species, overlapping with transparency and an accessible palette\n\nInspect Copilotâ€™s suggestion, remove redundant arguments, and adjust to:\n\ngeom_histogram(position = \"identity\", alpha = 0.5)\n\nCommit the cleaned version.\n\n\n\nReflection C\nReplace colour with fill, adjust legend, apply minimal theme.\n\n\n\nPart D â€” Loading local data safely\nCopilot does not know your file system.\n\nSave Scooby.xlsx in your data/ folder.\n\nDownload Scooby-doo.csv\n\nLoad explicitly:\n\nlibrary(readxl)\nscooby &lt;- read_xlsx(\"data/Scooby.xlsx\")\n\nAdd prompt:\n\n# Quick EDA: glimpse, summary, and count episodes by network\n\nEdit Copilotâ€™s code before running.\n\n\n\nReflection D\nWhy be explicit with data loading and library imports?\n\n\n\nPart E â€” Boxplots\nVisualise IMDb ratings by network.\n\nPrompt:\n\n# Boxplot of IMDb (y) by network (x); tilt x labels; use fill instead of colour\n\nEdit column names if needed.\nAdd improvements:\n\n# Make it horizontal, tidy legend, and add labs\n\n\nExtension E\nOrder networks by median rating.\n\n\n\nPart F â€” Best practices\n\nPrivacy: Never expose credentials/confidential data.\nNon-determinism: Review suggestions before accepting.\nNo vibe coding: Donâ€™t accept what you donâ€™t understand.\nContext helps: Use clean code and comments.\nReproducibility: Remove redundant args, consider renv.\n\ninstall.packages(\"renv\")\nrenv::init()\n\n\n\nPart G â€” Check-off & submission\n\nCopilot enabled and signed in.\nOne scatter plot (corrected column names).\nOne histogram (transparency + accessible palette).\nOne boxplot (labels/theme improved).\nReflection paragraph on Copilot.\n\n\n\n\nTroubleshooting\n\nSuggestions not appearing? Check Global Options â†’ Copilot.\nWrong dataset/columns? Run names()/glimpse().\nOverconfident code? Scale back, test small steps.\n\n\n\n\nFurther practice\n\nRewrite prompts as comments for grouped summaries, joins, faceted plots.\nCompare Copilot vs manual solutions.\n\n\nğŸ End of lab 6 ğŸ›‘ Remember to save your script ğŸ’¾",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Supervision 1</span>"
    ]
  },
  {
    "objectID": "supervision_2.html",
    "href": "supervision_2.html",
    "title": "Supervision 2",
    "section": "",
    "text": "ğŸ§ª Lab 7 â€” The Simple Linear Regression Model",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Supervision 2</span>"
    ]
  },
  {
    "objectID": "supervision_2.html#lab-7-the-simple-linear-regression-model",
    "href": "supervision_2.html#lab-7-the-simple-linear-regression-model",
    "title": "Supervision 2",
    "section": "",
    "text": "ğŸ¯ Learning outcomes\nBy the end of this lab you will be able to:\n\nUnderstand the mathematical formulation of the simple linear regression model\nEstimate regression coefficients using lm() in R\nInterpret the intercept and slope parameters in an applied context\nExtract and examine regression output using summary() and related functions\nVisualize the fitted regression line on a scatter plot\nAssess the quality of model fit using residuals and R-squared\n\n\n\n\nğŸ§° Prerequisites\n\nR (â‰¥ 4.0) and RStudio installed\nCompleted Labs 1-6\nPackages (install once):\n\ninstall.packages(c(\"remotes\", \"PoEdata\"))\nremotes::install_github(\"ccolonescu/PoEdata\")\n\nSummary. In simple linear regression, we use one independent variable to predict a dependent variable. The relationship is represented by a straight line with an intercept and a slope.\nIntroduction. We explore the fundamentals of the Simple Linear Regression Model, a method to understand the relationship between two variables. This model is foundational for more advanced analyses.\nKey terms. The dependent variable \\((y)\\) is what we aim to predict; the independent variable, \\(x\\), explains \\(y\\).\n\n\nThe General Model\nAssume a linear relationship between the conditional expectation of (y) and (x):\n\\[\ny_i = \\beta_1 + \\beta_2 x_i + e_i \\tag{1}\n\\]\n\n\\(\\beta_1\\): intercept\n\\(\\beta_2\\): slope\n\n\\(e_i\\): error term with variance \\(\\sigma^2\\)\n\n\\(i=1,\\dots,N\\): observation index\n\nThe predicted (estimated) value of \\(y\\) given \\(x\\) is:\n\\[\n\\hat{y} = b_1 + b_2 x \\tag{2}\n\\]\nAssumptions:\n\nnon-random (x);\nconstant error variance (homoskedasticity);\nerrors uncorrelated across observations;\n\\(E[e_i \\mid x_i]=0\\).\n\n\nPART A. Example: Food Expenditure vs Income\n# Recall to install the package PoEdata once if needed:\ninstall.packages(\"remotes\")\nremotes::install_github(\"ccolonescu/PoEdata\")\n\nlibrary(PoEdata)\ndata(\"cps_small\")\nplot(cps_small$educ, cps_small$wage, xlab=\"Education\", ylab=\"Wage\")\n\n\n\n\n\n\n\n# Food data\ndata(\"food\")\nhead(food)\n\n  food_exp income\n1   115.22   3.69\n2   135.98   4.39\n3   119.34   4.75\n4   114.96   6.03\n5   187.05  12.47\n6   243.92  12.98\n\nplot(food$income, food$food_exp,\n     ylim=c(0, max(food$food_exp)),\n     xlim=c(0, max(food$income)),\n     xlab=\"weekly income in $100\", ylab=\"weekly food expenditure in $\",\n     type=\"p\")\n\n\n\n\n\n\n\n\n\n\nPART B. Estimating a Linear Regression\nFor the food data the model is\n\\[\n\\text{food\\_exp}_i = \\beta_1 + \\beta_2\\,\\text{income}_i + e_i.\\tag{3}\n\\]\n\nlibrary(PoEdata)\nmod1 &lt;- lm(food_exp ~ income, data = food)\nb1 &lt;- coef(mod1)[[1]]\nb2 &lt;- coef(mod1)[[2]]\nsmod1 &lt;- summary(mod1)\nsmod1\n\n\nCall:\nlm(formula = food_exp ~ income, data = food)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-223.025  -50.816   -6.324   67.879  212.044 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   83.416     43.410   1.922   0.0622 .  \nincome        10.210      2.093   4.877 1.95e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 89.52 on 38 degrees of freedom\nMultiple R-squared:  0.385, Adjusted R-squared:  0.3688 \nF-statistic: 23.79 on 1 and 38 DF,  p-value: 1.946e-05\n\n\nAdd the regression line to the scatter plot:\n\nplot(food$income, food$food_exp,\n     ylim=c(0, max(food$food_exp)),\n     xlim=c(0, max(food$income)),\n     xlab=\"weekly income in $100\", ylab=\"weekly food expenditure in $\",\n     type = \"p\")\nabline(b1, b2)\n\n\n\n\n\n\n\n\nRetrieve common results:\n\nnames(mod1)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\nnames(smod1)\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\nmod1$coefficients\n\n(Intercept)      income \n   83.41600    10.20964 \n\nsmod1$coefficients\n\n            Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 83.41600  43.410163 1.921578 6.218242e-02\nincome      10.20964   2.093264 4.877381 1.945862e-05\n\n\n\n\nPART C. MCQs\n\n\n\n\n1) What relationship does simple linear regression assume between y and x?\n\n\n Quadratic Logarithmic Linear Exponential\n\n\n\n\nHint\n\nThink straight line: one slope + one intercept.\n\nÎ²\n\n\n2) In the model y = Î²â‚€ + Î²â‚ x + Îµ, what does Î²â‚ represent?\n\n\n The average of y The error variance Expected change in y for a one-unit increase in x The intercept\n\n\n\n\nHint\n\nÎ”y for +1 in x.\n\n\n\n3) In the model y = Î²â‚€ + Î²â‚ x + Îµ, what does Î²â‚€ represent?\n\n\n The slope of x The expected value of y when x = 0 The variance of Îµ Correlation between x and y\n\n\n\n\nHint\n\nValue of y at x = 0.\n\n\n\n4) A residual is defined as:\n\n\n Predicted y minus observed y Observed x minus predicted x Observed y minus predicted y The fitted value\n\n\n\n\nHint\n\nActual âˆ’ fitted.\n\n\n\n5) Ordinary Least Squares (OLS) chooses coefficients to:\n\n\n Maximize RÂ² Minimize the sum of absolute residuals Minimize the sum of squared residuals Maximize the likelihood under normal errors only\n\n\n\n\nHint\n\nSquares penalize big errors more.\n\n\n\n6) With an intercept in the model, OLS residuals:\n\n\n Sum to a positive number Sum to a negative number Sum to zero Sum to the sample mean of y\n\n\n\n\nHint\n\nâˆ‘eáµ¢ = 0 and Xáµ€e = 0 when an intercept is included.\n\n\n\n7) In simple OLS with an intercept, the fitted line passes throughâ€¦\n\n\n The origin (0,0) The point (xÌ„, È³) The median of x and y The maximum of y\n\n\n\n\nHint\n\nIt goes through the means.\n\n\n\n8) Which R function fits a simple linear regression model?\n\n\n plot() cor() lm() predict()\n\n\n\n\n9) In summary(mod), the p-value for the slope tests:\n\n\n Hâ‚€: Î²â‚ â‰  0 Hâ‚€: Î²â‚ = 0 Hâ‚€: Î²â‚€ = 0 Hâ‚€: errors are normal\n\n\n\n\n10) The p-value is best described as:\n\n\n The probability Hâ‚ is true The probability the estimate is correct The probability (under Hâ‚€) of a result at least as extreme as observed The Type II error rate\n\n\n\n\n11) A 95% confidence interval for Î²â‚ is generally:\n\n\n Estimate Â± 1.96 Ã— Ïƒ of y Estimate Â± critical t Ã— SE(Î²â‚) Estimate Â± 2 Ã— residual SD SE(Î²â‚) Â± estimate\n\n\n\n\n12) RÂ² measures:\n\n\n The correlation between x and y Proportion of variance in y explained by x The average residual size The probability the model is correct\n\n\n\n\n13) Adjusted RÂ² differs from RÂ² because it:\n\n\n Ignores sample size Is always larger than RÂ² Penalizes adding predictors relative to sample size Equals 1 âˆ’ RÂ²\n\n\n\n\n14) To make a prediction at x = xâ‚€, you use:\n\n\n Î²â‚ Ã— xâ‚€ Î²â‚€ Ã· xâ‚€ Å· = Î²â‚€ + Î²â‚ Ã— xâ‚€ SE(Î²â‚) Ã— xâ‚€\n\n\n\n\nHint\n\nPlug xâ‚€ into the fitted line.\n\n\n\n15) For OLS to be unbiased, a key assumption is:\n\n\n x is normally distributed Errors have zero variance E[Îµ | x] = 0 (errors have zero mean conditional on x) y is standardized\n\n\n\n\n16) Homoskedasticity means:\n\n\n Errors are perfectly correlated Variance of errors is constant across x Mean of errors is zero only at xÌ„ x has constant variance\n\n\n\n\n17) In summary(mod), the t value for Î²â‚ equals:\n\n\n Î²â‚ SE(Î²â‚) Estimate(Î²â‚) / SE(Î²â‚) 1 âˆ’ p-value\n\n\n\n\nHint\n\nSignal Ã· noise for the slope estimate.\n\n\n\n18) The units of Î²â‚ are best described as:\n\n\n Units of y Units of y per unit of x Unitless Units of x per unit of y\n\n\n\n\n19) Rescaling x from dollars to hundreds of dollars will:\n\n\n Leave Î²â‚ unchanged Rescale Î²â‚ numerically but leave RÂ² and the t-test for Î²â‚ unchanged Change the data but not the model Invalidate OLS\n\n\n\n\n20) After fitting mod &lt;- lm(y ~ x, d), the fitted value at xâ‚€ is:\n\n\n residuals(mod)[x==xâ‚€] predict(mod, newdata = data.frame(x = xâ‚€)) coef(mod)['x'] fitted(mod)[1] always\n\n\n\n\nHint\n\nUse predict() with a small data frame for xâ‚€.\n\n\n\n\nğŸ End of lab 7 ğŸ›‘ Remember to save your script ğŸ’¾",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Supervision 2</span>"
    ]
  },
  {
    "objectID": "supervision_2.html#lab-8-prediction-with-the-linear-regression-model",
    "href": "supervision_2.html#lab-8-prediction-with-the-linear-regression-model",
    "title": "Supervision 2",
    "section": "ğŸ§ª Lab 8 â€” Prediction with the Linear Regression Model",
    "text": "ğŸ§ª Lab 8 â€” Prediction with the Linear Regression Model\n\nğŸ¯ Learning outcomes\nBy the end of this lab you will be able to:\n\nGenerate point predictions and quantify their uncertainty using Râ€™s prediction framework\nDistinguish between confidence intervals for average outcomes and prediction intervals for individual cases, and select the appropriate interval for applied research contexts\nAssess the reliability of coefficient estimates through their variance-covariance structure\nRecognize when predictions involve extrapolation beyond observed data and understand the associated risks\nHandle non-linear relationships through quadratic and log-linear specifications\n\n\n\n\nğŸ§° Prerequisites\nKnowledge: - Understanding of simple linear regression (Lab 7) - Familiarity with residuals, fitted values, and RÂ² - Basic probability concepts (sampling distributions, standard errors)\nTechnical: - R (â‰¥ 4.0) and RStudio installed - Completed Labs 1-7 - Required packages (install if needed):\ninstall.packages(c(\"remotes\", \"PoEdata\")) \nremotes::install_github(\"ccolonescu/PoEdata\")\nlibrary(PoEdata)\nDatasets:\nfood - household food expenditure and income (40 observations) br - Baton Rouge house prices and characteristics (1,080 observations) â€”\n\n\n\n\n\n\nNote\n\n\n\nNotation reminder: We use Greek letters (Î²1,Î²2â€‹) for true population parameters and Latin letters (b1,b2â€‹) for their sample estimates.\nÅ·: predicted y value\n\\(\\hat{\\beta}\\) or \\(\\hat{b}\\) are sometimes used interchangeably with b for estimates\n\n\nIn Lab 7, we learned how to estimate relationships, But estimation is only half the story. In applied research, we often need to make predictions. In applied land economics research, prediction is crucialâ€”whether forecasting property values, estimating rental yields, or projecting land use changes. This lab equips you with the tools to make robust predictions and quantify their uncertainty. Letâ€™s begin with the basic prediction workflow.\nOnce we have estimated our regression coefficients \\(b_1\\) (intercept) and \\(b_2\\) (slope), we can use them to predict food expenditure for any given income level using the fitted regression equation (Eq. 2).\n\\[\n\\hat{y} = b_1 + b_2 x \\tag{2}\n\\]\nâš ï¸Unit conversion: In the following R script â€œincome = $2000â€ is â€œincome = 20â€ (the data is in hundreds of dollars).\n\n#### Step 1: Fit the model (recap from Lab 7)\n\nlibrary(PoEdata)\ndata(\"food\")\n\n# Estimate the food expenditure model\nmod1 &lt;- lm(food_exp ~ income, data = food)\n\n# Review estimates\nsummary(mod1)\n\n\nCall:\nlm(formula = food_exp ~ income, data = food)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-223.025  -50.816   -6.324   67.879  212.044 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   83.416     43.410   1.922   0.0622 .  \nincome        10.210      2.093   4.877 1.95e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 89.52 on 38 degrees of freedom\nMultiple R-squared:  0.385, Adjusted R-squared:  0.3688 \nF-statistic: 23.79 on 1 and 38 DF,  p-value: 1.946e-05\n\n#### Step 2: Create a data frame with target income values\n\n# Scenario: Predict food expenditure for three household types\n# Note: income is in $100s, so divide actual income by 100\nnewx &lt;- data.frame(income = c(20, 25, 27))\n\n#### Step 3: Generate predictions\n\n# The predict() function takes two arguments:\n#   1. A fitted model object (mod1)\n#   2. A data frame with new x-values (must have same column names as original data)\nyhat &lt;- predict(mod1, newx)\n\n# Give friendly names to each prediction so the output is   easy to read\nnames(yhat) &lt;- c(\"Low income = $2000\", \"Median income = $2500\", \"High income = $2700\")\n\n# Show the predictions\nyhat\n\n   Low income = $2000 Median income = $2500   High income = $2700 \n             287.6089              338.6571              359.0764 \n\n#### Step 4: Interpret the results\n\n# A household earning $2,000/week is predicted to spend $287.60 on food per week.\n# This represents about 14.4% of their income (287.6/2000).\n\n# For the median household ($2,500/week), predicted food expenditure is $338.70,\n# or about 13.5% of income.\n\n# Observation: The proportion of income spent on food decreases as income risesâ€”\n# this is consistent with Engel's Law from economics.\n\nWhatâ€™s Missing? Uncertainty!\nThese are point predictionsâ€”our single best guess at each income level. But how confident are we?\nConsider the household earning $2,000/week: - We predict theyâ€™ll spend $287.60 on food - But thereâ€™s sampling uncertainty (our \\(b_1\\) and \\(b_2\\) are estimates) - And individual variation (not all $2,000/week households spend exactly $287.60)\nTwo types of intervals address these concerns:\n\nConfidence intervals answer: â€œWhatâ€™s the average food expenditure for all households at this income level?â€\nPrediction intervals answer: â€œWhat might one specific household at this income level spend?â€\n\n\n\n\nUnderstanding Prediction Uncertainty Through Sampling Variability\nOur predictions depend entirely on our estimated coefficients (\\(b_1, b_2\\)). But these are sample estimatesâ€”if we collected a different dataset, weâ€™d get different values and therefore different predictions.\nBefore making predictions, itâ€™s important to understand that our coefficient estimates vary across samples. This matters for land economics applications: if weâ€™re advising on property valuations, we need to know how sensitive our predictions are to sampling variation.\n\n\n\n\n\n\nWhat is â€œsampling with replacementâ€?\n\n\n\nImagine our 40 observations are numbered balls in an urn. We draw a ball, record its data, put it back, then draw again. Some observations may appear multiple times in a bootstrap sample; others not at all. This mimics the randomness of drawing a new sample from the population.\n\n\nThe following bootstrap exercise (simulating sampling variability) demonstrates this variability by repeatedly resampling our data and re-estimating the model:\n\nN &lt;- nrow(food)   # observations\nC &lt;- 50           # repeats\nS &lt;- 38           # subsample size\n\nsumb2 &lt;- 0        # sum of slopes\nfor (i in 1:C){\n  set.seed(3*i)   # reproducible\n  # Draw a bootstrap sample (with replacement)\n  subsample &lt;- food[sample(1:N, S, replace = TRUE), ]  # bootstrap draw\n  # Fit the model on this bootstrap sample\n  mod2 &lt;- lm(food_exp ~ income, data = subsample)      \n  sumb2 &lt;- sumb2 + coef(mod2)[2]                       # store slope Î²2\n}\n\nprint(sumb2 / C, digits = 3)   # average slope\n\nincome \n  9.89 \n\n\nCompare this bootsrap average (repeated samples) witht the original OLS regression output (from mod1) for \\(b_2\\). They should be similar, confirming that OLS is unbiased.\n\n\nEstimated Variances and Covariance of Coefficients\nThe variance-covariance matrix tells us two crucial things: 1. Variances (diagonal): How much each coefficient estimate varies 2. Covariances (off-diagonal): How the estimates move together\nThis matters for prediction because uncertainty in \\(\\hat{y}\\) depends on uncertainty in both coefficients and their correlation. For land valuation models, ignoring this covariance can lead to overconfident predictions.\n\n\n\n\n\n\nWarning\n\n\n\nCommon Error in Applied Work Many analysts incorrectly calculate prediction variance as just \\(\\text{Var}(b_1) + x_0^2 \\cdot \\text{Var}(b_2)\\), omitting the covariance term. This can make predictions appear less precise than they actually are.\n\n\nThe following R script extracts estimated variances and covariances from the object mod 1.\n\n# coef varianceâ€“covariance matrix.\n# This is a 2Ã—2 symmetric matrix:\n# - Top-left (1884.44): Var(bâ‚), variance of intercept\n# - Bottom-right (4.38): Var(bâ‚‚), variance of slope  \n# - Off-diagonals (-87.78): Cov(bâ‚,bâ‚‚), covariance between coefficients\nvcov(mod1)                       \n\n            (Intercept)     income\n(Intercept)  1884.44226 -85.903157\nincome        -85.90316   4.381752\n\n#### Extract individual components\n(varb1   &lt;- vcov(mod1)[1, 1])        # Var(B1), intercept variance\n\n[1] 1884.442\n\n(varb2   &lt;- vcov(mod1)[2, 2])        # Var(B2), slope variance\n\n[1] 4.381752\n\n(covb1b2 &lt;- vcov(mod1)[1, 2])        # Cov(B1, B2), covariance\n\n[1] -85.90316\n\n\nHaving established how to assess coefficient stability, we now turn to cases where linear relationships are insufficient to capture real-world patterns in land markets.\n\n\nNon-Linear Relationships - When Straight Lines Donâ€™t Fit\nIn practice, many land economic relationships are non-linear: - Property prices donâ€™t increase linearly with size (diminishing returns to scale) - Land values may have threshold effects near transport nodes - Agricultural productivity often follows diminishing returns\nWeâ€™ll explore two common approaches Quadratic Models and Log-Linear Models.\n\nPART A. Quadratic model - for U-shaped or inverted-U relationships\n\\[\ny_i = \\beta_1 + \\beta_2 x_i^2 + e_i.\\tag{5}\n\\]\n\nlibrary(PoEdata)            # load package\ndata(br)                    # dataset\n\nmod3 &lt;- lm(price ~ I(sqft^2), data = br)  # fit model\nb1 &lt;- coef(mod3)[1]        # intercept\nb2 &lt;- coef(mod3)[2]        # coeff on sqft^2\n\nsqftx &lt;- c(2000, 4000, 6000)              # evaluation points\npricex &lt;- b1 + b2 * sqftx^2               # predicted price\nDpriceDsqft &lt;- 2 * b2 * sqftx             # marginal effect d(price)/d(sqft)\nelasticity &lt;- (DpriceDsqft * sqftx) / pricex  # elasticity\n\nb1; b2; DpriceDsqft; elasticity           # output\n\n(Intercept) \n   55776.57 \n\n\nI(sqft^2) \n0.0154213 \n\n\n[1]  61.68521 123.37041 185.05562\n\n\n[1] 1.050303 1.631251 1.817408\n\n\nPlotting two alternatives for the quadratic fit:\n\nmod31 &lt;- lm(price ~ I(sqft^2), data = br)                # fit\nplot(br$sqft, br$price, col = \"grey\",\n     xlab = \"Total square feet\", ylab = \"Sale price, $\") # scatter\n\nb &lt;- coef(mod31)                                         # [intercept], sqft^2\ncurve(b[1] + b[2]*x^2,\n      from = min(br$sqft), to = max(br$sqft),            # range\n      add = TRUE, lwd = 2)                               # fitted curve\n\n\n\n\n\n\n\n\n\nordat &lt;- br[order(br$sqft), ]                             # sort by sqft\nmod31 &lt;- lm(price ~ I(sqft^2), data = ordat)              # fit model on sorted data\n\nplot(br$sqft, br$price, col = \"grey\",                     # scatter\n     main = \"Dataset ordered by 'sqft'\",\n     xlab = \"Total square feet\", ylab = \"Sale price, $\")\n\nlines(fitted(mod31) ~ ordat$sqft)                         # add fitted curve\n\n\n\n\n\n\n\n\n\n\nPART B. Log-Linear Models - for proportional (percentage) relationships\n\\[\n\\log(y_i) = \\beta_1 + \\beta_2 x_i + e_i.\\tag{6}\n\\]\n\nhist(br$price)                # price distribution\n\n\n\n\n\n\n\nhist(log(br$price))           # log-price distribution\n\n\n\n\n\n\n\nmod4 &lt;- lm(log(price) ~ sqft, data = br)  # log-linear fit\nb1 &lt;- coef(mod4)[1]          # intercept\nb2 &lt;- coef(mod4)[2]          # slope\n\n# Back-transform fitted curve to price scale\nordat &lt;- br[order(br$sqft), ]                     # sort by sqft\nmod4  &lt;- lm(log(price) ~ sqft, data = ordat)      # refit (ordered)\nplot(br$sqft, br$price, col = \"grey\",\n     xlab = \"Total square feet\", ylab = \"Sale price, $\")  # scatter\nlines(exp(fitted(mod4)) ~ ordat$sqft, lwd = 2)    # exp of fitted log-price\n\n\n\n\n\n\n\n# âš ï¸ Important: Simply exponentiating log predictions (exp(fitted)) gives the median, not the mean. For mean predictions, apply a smearing correction\n\nElasticity and marginal effect at the median price:\n\npricex &lt;- median(br$price)                                   # target price (median)\nsqftx  &lt;- (log(pricex) - coef(mod4)[1]) / coef(mod4)[2]      # back out sqft from log model\n(DyDx &lt;- pricex * coef(mod4)[2])                              # marginal effect d(price)/d(sqft) = b2 * price\n\n    sqft \n53.46495 \n\n(elasticity &lt;- sqftx * coef(mod4)[2])                         # elasticity = (dP/dx)*(x/P) = b2 * sqft\n\n(Intercept) \n  0.9366934 \n\n\nMultiple points:\n\nb1 &lt;- coef(mod4)[1]                     # intercept of log(price) ~ sqft\nb2 &lt;- coef(mod4)[2]                     # slope wrt sqft\n\nsqftx  &lt;- c(2000, 3000, 4000)           # sqft points\npricex &lt;- c(100000, exp(b1 + b2*sqftx)) # prices: first fixed at 100k; others from model\n\nsqftx  &lt;- (log(pricex) - b1) / b2       # implied sqft from each price (now length = length(pricex))\n(elasticities &lt;- b2 * sqftx)            # elasticity = (dP/dx)*(x/P) = b2 * sqft\n\n[1] 0.6743291 0.8225377 1.2338066 1.6450754\n\n\nğŸ’¡ Land Economics Application In rental valuation, we rarely observe rental income for vacant plots. Prediction intervals help quantify the uncertainty in forecasted rents, which is crucial for development feasibility analysis.\nâš ï¸ Common Mistake Students often confuse confidence intervals (for the average) with prediction intervals (for individuals). In property valuation, this distinction matters: are you estimating the average price for houses of this type, or the likely sale price of one specific house?\n\n\nPART C. MCQs\n\n\n1) Using Bâ‚€ and Bâ‚ primarily helps to:\n\n\n Test normality of residuals Estimate ÏƒÂ² Predict E[y|x] Standardize x and y\n\n\n\n\nHint\n\nPlug x into the fitted line.\n\n\n\n2) The function lm() is used to:\n\n\n Simulate data Draw histograms Estimate a linear model Compute correlation only\n\n\n\n\nHint\n\nIt returns coefficients and a model object.\n\n\n\n3) The function predict() mainly:\n\n\n Fits a model Computes residuals only Estimates y-hat for new data Sorts a data frame\n\n\n\n\nHint\n\nYou pass newdata.\n\n\n\n4) Coefficients are random because they:\n\n\n Depend only on fixed formulas Are set by the user Depend on the sample Donâ€™t change across samples\n\n\n\n\n5) Random subsamples help to:\n\n\n Reduce file size Guarantee higher RÂ² Evaluate stability/variability Eliminate outliers always\n\n\n\n\n6) vcov(model) returns:\n\n\n Residuals Fitted values Variances and covariances of coefficients Confidence intervals for y-hat\n\n\n\n\n7) Using data.frame() with predict() is to:\n\n\n Provide new x values with correct column names Shuffle the rows Change variable types to character Compute RÂ²\n\n\n\n\n8) To request standard errors from predict.lm you set:\n\n\n stderr = TRUE se = TRUE se.fit = TRUE ci = TRUE\n\n\n\n\nHint\n\nIt returns fit and se.fit.\n\n\n\n9) For a confidence interval for the mean response at xâ‚€, use:\n\n\n interval = 'pi' bands = 'mean' interval = 'confidence' type = 'mean'\n\n\n\n\nHint\n\nCI for E[y|xâ‚€].\n\n\n\n10) For an interval predicting an individual future y at xâ‚€, use:\n\n\n interval = 'confidence' interval = 'prediction' level = 'future' type = 'response'\n\n\n\n\nHint\n\nIncludes error variance.\n\n\n\n11) Prediction intervals are typically:\n\n\n Narrower than confidence intervals Wider than confidence intervals Identical to confidence intervals Unrelated to residual variance\n\n\n\n\n12) confint(model) returns:\n\n\n Intervals for y-hat Confidence intervals for coefficients Prediction intervals for new y Variance of residuals\n\n\n\n\n13) For models with multiple predictors, newdata must:\n\n\n Contain columns matching model terms and types Be a vector in any order Include only the response Be sorted by the response\n\n\n\n\nHint\n\nNames must match the formulaâ€™s variables.\n\n\n\n14) In a log-linear model log(y) ~ x, to back-transform the mean prediction you should:\n\n\n Use only exp(eta) Multiply exp(eta) by a smearing factor Square the fitted values Add residual SD then exponentiate\n\n\n\n\nHint\n\nDuanâ€™s correction: mean(exp(ÎµÌ‚)).\n\n\n\n15) A bootstrap (or resampling with replacement) in R uses:\n\n\n sample(x, replace = FALSE) sample(x, replace = TRUE) sort(x) order(x)\n\n\n\n\n16) set.seed(123) is used to:\n\n\n Speed up lm() Normalize variables Make random subsamples reproducible Center predictors\n\n\n\n\n17) For out-of-sample performance, a good metric is:\n\n\n Training RÂ² Test RMSE Number of predictors Intercept size\n\n\n\n\nHint\n\nEvaluate on held-out data.\n\n\n\n18) Extrapolation risk means:\n\n\n Predictions are always unbiased xâ‚€ lies outside the observed x range so predictions can be unreliable RÂ² increases automatically CI and PI become identical\n\n\n\n\n19) Variance of the predicted mean at xâ‚€ depends on:\n\n\n Only ÏƒÂ² Only the intercept xâ‚€áµ€ Var(BÌ‚) xâ‚€ Only sample size\n\n\n\n\nHint\n\nUse the varianceâ€“covariance of coefficients.\n\n\n\n20) When predicting with factors, newdata must:\n\n\n Use levels seen in the training data Introduce new unseen levels freely Coerce factors to numeric Omit the factor columns\n\n\n\n\nHint\n\nUnseen levels cause errors in predict().\n\n\n\n\nğŸ End of lab 8 ğŸ›‘ Remember to save your script ğŸ’¾",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Supervision 2</span>"
    ]
  },
  {
    "objectID": "supervision_2.html#lab-9-hypothesis-test-pvalue-testing-linear-combinations",
    "href": "supervision_2.html#lab-9-hypothesis-test-pvalue-testing-linear-combinations",
    "title": "Supervision 2",
    "section": "ğŸ§ª Lab 9 â€” Hypothesis Test, pâ€‘Value & Testing Linear Combinations",
    "text": "ğŸ§ª Lab 9 â€” Hypothesis Test, pâ€‘Value & Testing Linear Combinations\n\nğŸ¯ Learning Outcomes\nBy the end of this lab you will be able to:\n\nFormulate null and alternative hypotheses for regression coefficients\nCompute test statistics and compare them to critical values\nCalculate and interpret p-values for one-tailed and two-tailed tests\nUnderstand the relationship between confidence intervals and hypothesis tests\nTest linear combinations of parameters (e.g., predictions at specific x values)\nApply hypothesis testing to real-world land economics questions\n\n\n\nHypothesis Tests\nğŸ§° Prerequisites Knowledge:\nUnderstanding of simple linear regression (Lab 7) Familiarity with coefficient estimates and standard errors (Lab 8) Basic understanding of statistical inference (confidence intervals, significance levels)\nTechnical:\nR (â‰¥ 4.0) and RStudio installed Completed Labs 1-8 Required packages:\ninstall.packages(c(\"remotes\", \"PoEdata\")) \nremotes::install_github(\"ccolonescu/PoEdata\")\nlibrary(PoEdata)\n\n\n\nIntroduction: Why Hypothesis Testing Matters\nIn Lab 7, we estimated relationships. In Lab 8, we made predictions. But how do we know if our findings are statistically meaningful? Consider our food expenditure model:\nWe estimated that food spending increases by $10.21 for every $100 increase in income But is this effect real, or could it be due to random chance? Could the true effect actually be zero (no relationship)?\nHypothesis testing provides a formal framework to answer these questions.\nğŸ’¡ Land Economics Application When valuing agricultural land, we might find that proximity to roads increases land value by Â£500 per meter. But before advising clients, we need to know: Is this effect statistically significant, or could it be sampling noise?\n\nPART A - Hypothesis Tests for Individual Coefficients\nWe start with two competing claims: \\[\nH_0: \\beta_k = c, \\qquad H_A: \\beta_k \\ne c.\n\\]\nTest statistic:\nUnder the null hypothesis, the test statistic follows a t-distribution: \\[\nt = \\frac{b_k - c}{\\operatorname{se}(b_k)},\\quad t \\sim t_{N-2}.\\tag{6}\n\\]\nwhere:\n\\(b_k\\)â€‹ is our estimated coefficient\n\\(c\\) is the hypothesized value (zero in our case)\n\\(\\operatorname{se}(b_k)\\) is the standard error of \\(b_k\\)\n\\(Nâˆ’K\\) is degrees of freedom (N observations, K parameters)\nIntuition: This ratio measures how many standard errors our estimate is away from the hypothesized value.\nExample 1: Two-Tailed Test (Is there any effect?) Question: Does income affect food expenditure? (i.e., is \\(Î²2â‰ 0\\)?)\n\n# Set significance level\nalpha &lt;- 0.05\n\n# Load data and estimate model\nlibrary(PoEdata); library(xtable); library(knitr)\n\nWarning: package 'xtable' was built under R version 4.4.3\n\n\nWarning: package 'knitr' was built under R version 4.4.3\n\ndata(\"food\")\nmod1 &lt;- lm(food_exp ~ income, data=food)\nsmod1 &lt;- summary(mod1)\n\n# Regression output\ntable &lt;- data.frame(xtable(mod1))\nkable(table, caption=\"Regression output showing the coefficients\")\n\n\nRegression output showing the coefficients\n\n\n\nEstimate\nStd..Error\nt.value\nPrâ€¦t..\n\n\n\n\n(Intercept)\n83.41600\n43.410163\n1.921578\n0.0621824\n\n\nincome\n10.20964\n2.093263\n4.877381\n0.0000195\n\n\n\n\n# Extract coefficient and standard error\nb2 &lt;- coef(mod1)[[\"income\"]]\nseb2 &lt;- sqrt(vcov(mod1)[2,2])\ndf  &lt;- df.residual(mod1)\n\n# Compute test statistic\nt   &lt;- b2/seb2\n\n# Find critical value for two-tailed test\ntcr &lt;- qt(1-alpha/2, df)\n\nt; tcr\n\n[1] 4.877381\n\n\n[1] 2.024394\n\n# Display results\ncat(\"Test statistic:\", round(t, 3), \"\\n\")\n\nTest statistic: 4.877 \n\ncat(\"Critical value (Â±):\", round(tcr, 3), \"\\n\")\n\nCritical value (Â±): 2.024 \n\ncat(\"Decision:\", ifelse(abs(t) &gt; tcr, \"Reject H0\", \"Fail to reject H0\"), \"\\n\")\n\nDecision: Reject H0 \n\n\nInterpretation:\n\nOur test statistic is r round(t, 2), which is much larger than the critical value Â±r round(tcr, 2)\nWe reject the null hypothesis\nThere is strong evidence that income affects food expenditure\nRightâ€‘tail and leftâ€‘tail versions:\n\nOptional: ğŸ“Š Visualizing the Test:\n\n# Create visualization of t-distribution and test\ncurve(dt(x, df), from = -5, to = 5, \n      main = \"Two-Tailed Hypothesis Test\",\n      xlab = \"t-value\", ylab = \"Density\",\n      lwd = 2)\n\n# Shade rejection regions\npolygon(c(-4, seq(-4, -tcr, 0.01), -tcr),\n        c(0, dt(seq(-4, -tcr, 0.01), df), 0),\n        col = \"red\", border = NA, density = 20)\npolygon(c(tcr, seq(tcr, 4, 0.01), 4),\n        c(0, dt(seq(tcr, 4, 0.01), df), 0),\n        col = \"red\", border = NA, density = 20)\n\n# Mark critical values and test statistic\nabline(v = c(-tcr, tcr), col = \"red\", lty = 2, lwd = 2)\n\nabline(v = t, col = \"blue\", lwd = 2)\n\nlegend(\"topright\", \n       legend = c(\"Rejection region (Î±/2 each)\", \n                  \"Critical values\", \n                  \"Test statistic\"),\n       col = c(\"red\", \"red\", \"blue\"),\n       lty = c(1, 2, 1), lwd = 2, bty = \"n\")\n\n\n\n\n\n\n\n\nExample 2: One-Tailed Tests Sometimes we have a directional hypothesis based on economic theory. Right-Tailed Test: Is the effect of income on food expenditure greater than $5.50?\n\\[\nH_0: \\beta_2 â‰¤ 5.5, \\qquad H_A: \\beta_2 &gt; 5.5.\n\\]\n\n# Hypothesized value\nc &lt;- 5.5\n\n# Compute test statistic\nt_right &lt;- (b2 - c)/seb2\n\n# Critical value (right tail only)\ntcr_right &lt;- qt(1-alpha, df)\n\ncat(\"Test statistic:\", round(t_right, 3), \"\\n\")\n\nTest statistic: 2.25 \n\ncat(\"Critical value:\", round(tcr_right, 3), \"\\n\")\n\nCritical value: 1.686 \n\ncat(\"Decision:\", ifelse(t_right &gt; tcr_right, \"Reject H0\", \"Fail to reject H0\"), \"\\n\")\n\nDecision: Reject H0 \n\n\nLeft-Tailed Test: Is the effect of income on food expenditure less than $15?\n\\[\nH_0: \\beta_2 â‰¥ 15, \\qquad H_A: \\beta_2 &lt; 15.\n\\]\n\n# Left-tail: H0: beta2 &gt;= 15; HA: beta2 &lt; 15\nc &lt;- 15\nt_left &lt;- (b2 - c)/seb2\ntcr_left &lt;- qt(alpha, df)\n\nc(t_right=t_right, tcr_right=tcr_right, t_left=t_left, tcr_left=tcr_left)\n\n  t_right tcr_right    t_left  tcr_left \n 2.249904  1.685954 -2.288463 -1.685954 \n\ncat(\"Test statistic:\", round(t_left, 3), \"\\n\")\n\nTest statistic: -2.288 \n\ncat(\"Critical value:\", round(tcr_left, 3), \"\\n\")\n\nCritical value: -1.686 \n\ncat(\"Decision:\", ifelse(t_left &lt; tcr_left, \"Reject H0\", \"Fail to reject H0\"), \"\\n\")\n\nDecision: Reject H0 \n\n\nâš ï¸ Common Mistake: Students often confuse the direction of the inequality in \\(H_0\\)â€‹ and \\(H_A\\). Remember: the alternative hypothesis represents what youâ€™re trying to find evidence for.\n\n\n\nPART B - The pâ€‘Value Approach\nThe p-value represents the probability of observing a t-statistic as extreme as, or more extreme than, the one obtained from the sample, assuming the null hypothesis is true. In other words, it measures how compatible the data are with the null hypothesis. We reject the null hypothesis when the p-value is smaller than the chosen significance level (e.g., 0.05).\nFor a right-tailed test, the p-value corresponds to the area to the right of the calculated t-statistic. For a left-tailed test, it is the area to the left of the calculated t-statistic. For a two-tailed test, the total p-value is divided equally between both tailsâ€”p/2 in the left tail and p/2 in the right tail.\nIn R, p-values are obtained using the function pt(t, df), where t is the calculated t-ratio and df is the modelâ€™s degrees of freedom. For two-tailed tests, the result should be adjusted to account for both tails, typically using 2*(1 - pt(abs(t), df)).\nDecision rule: Reject \\(H_0\\)â€‹ if \\(p-value&lt;Î±p\\) Fail to reject \\(H_0\\)â€‹ if \\(p-valueâ‰¥Î±p\\)\n\n\n\n\n\n\nCommon Significance Levels\n\n\n\n\nÎ± = 0.05 (most common): Weâ€™re willing to accept a 5% chance of incorrectly rejecting Hâ‚€\nÎ± = 0.01 (more conservative): Only 1% chance of Type I error\nÎ± = 0.10 (more lenient): Accept 10% chance of error\n\nThe smaller the p-value, the stronger the evidence against Hâ‚€\n\n\nComputing p-Values: \\(F_t\\) stands for the cumulative distribution function (CDF) of the t-distribution.\nRightâ€‘tail: \\(p = 1 - F_t(t)\\) (probability in the upper tail)\nLeftâ€‘tail: \\(p = F_t(t)\\) (probability in the lower tail)\nTwoâ€‘tail: \\(p = 2 * (1-F_t(|t|)\\) (probability in both tails)\nExample 1: Right-Tail Test Question: Is the marginal effect of income on food expenditure greater than $5.50?\n\\[H_0: \\beta_2 \\leq 5.5 \\quad \\text{vs} \\quad H_A: \\beta_2 &gt; 5.5\\]\n\n# Right-tail test (H0: Î²2 â‰¤ 5.5)\nc &lt;- 5.5\nt &lt;- (b2-c)/seb2\np_right &lt;- 1-pt(t, df)\n\ncat(\"=== RIGHT-TAIL TEST ===\\n\")\n\n=== RIGHT-TAIL TEST ===\n\ncat(\"H0: Î²2 â‰¤\", c, \"  (effect is at most $5.50)\\n\")\n\nH0: Î²2 â‰¤ 5.5   (effect is at most $5.50)\n\ncat(\"HA: Î²2 &gt;\", c, \"  (effect exceeds $5.50)\\n\\n\")\n\nHA: Î²2 &gt; 5.5   (effect exceeds $5.50)\n\ncat(\"Test statistic:\", round(t, 3), \"\\n\")\n\nTest statistic: 2.25 \n\ncat(\"p-value:\", round(p_right, 4), \"\\n\")\n\np-value: 0.0152 \n\ncat(\"Decision at Î±=0.05:\", ifelse(p_right &lt; 0.05, \"âœ“ REJECT H0\", \"âœ— FAIL TO REJECT H0\"), \"\\n\")\n\nDecision at Î±=0.05: âœ“ REJECT H0 \n\ncat(\"\\nInterpretation:\", ifelse(p_right &lt; 0.05, \n           \"Strong evidence that Î²2 &gt; 5.5 (the effect exceeds $5.50)\", \"Insufficient evidence that Î²2 &gt; 5.5\"), \"\\n\\n\")\n\n\nInterpretation: Strong evidence that Î²2 &gt; 5.5 (the effect exceeds $5.50) \n\n\nExample 2: Left-Tail Test\nQuestion: Is the marginal effect of income on food expenditure less than $15?\n\\[H_0: \\beta_2 \\geq 15 \\quad \\text{vs} \\quad H_A: \\beta_2 &lt; 15\\]\n\n# Left-tail test (H0: Î²2 â‰¥ 15)\nc &lt;- 15\nt &lt;- (b2-c)/seb2\np_left &lt;- pt(t, df)\n\ncat(\"=== LEFT-TAIL TEST ===\\n\")\n\n=== LEFT-TAIL TEST ===\n\ncat(\"H0: Î²2 â‰¥\", c, \"  (effect is at least $15)\\n\")\n\nH0: Î²2 â‰¥ 15   (effect is at least $15)\n\ncat(\"HA: Î²2 &lt;\", c, \"  (effect is below $15)\\n\\n\")\n\nHA: Î²2 &lt; 15   (effect is below $15)\n\ncat(\"Test statistic:\", round(t, 3), \"\\n\")\n\nTest statistic: -2.288 \n\ncat(\"p-value:\", round(p_left, 4), \"\\n\")\n\np-value: 0.0139 \n\ncat(\"Decision at Î±=0.05:\", ifelse(p_left &lt; 0.05, \"âœ“ REJECT H0\", \"âœ— FAIL TO REJECT H0\"), \"\\n\")\n\nDecision at Î±=0.05: âœ“ REJECT H0 \n\ncat(\"\\nInterpretation:\", \n    ifelse(p_left &lt; 0.05, \n           \"Strong evidence that Î²2 &lt; 15 (the effect is below $15)\",\n           \"Insufficient evidence that Î²2 &lt; 15\"), \"\\n\\n\")\n\n\nInterpretation: Strong evidence that Î²2 &lt; 15 (the effect is below $15) \n\n\nExample 3: Two-Tail Test (Most Common)\nQuestion: Is there any relationship between income and food expenditure?\n\\[H_0: \\beta_2 = 0 \\quad \\text{vs} \\quad H_A: \\beta_2 \\neq 0\\]\n\n# Two-tail test (H0: Î²2 = 0)\nc &lt;- 0  \nt &lt;- (b2-c)/seb2\np_two &lt;- 2*(1-pt(abs(t), df))\n\ncat(\"=== TWO-TAIL TEST ===\\n\")\n\n=== TWO-TAIL TEST ===\n\ncat(\"H0: Î²2 = 0  (no relationship)\\n\")\n\nH0: Î²2 = 0  (no relationship)\n\ncat(\"HA: Î²2 â‰  0  (income affects food expenditure)\\n\\n\")\n\nHA: Î²2 â‰  0  (income affects food expenditure)\n\ncat(\"Test statistic:\", round(t, 3), \"\\n\")\n\nTest statistic: 4.877 \n\ncat(\"p-value:\", format.pval(p_two, digits = 4), \"\\n\")\n\np-value: 1.946e-05 \n\ncat(\"Decision at Î±=0.05:\", ifelse(p_two &lt; 0.05, \"âœ“ REJECT H0\", \"âœ— FAIL TO REJECT H0\"), \"\\n\")\n\nDecision at Î±=0.05: âœ“ REJECT H0 \n\ncat(\"\\nInterpretation:\", \n    ifelse(p_two &lt; 0.05, \n           \"Strong evidence of a relationship between income and food expenditure\",\n           \"Insufficient evidence of a relationship\"), \"\\n\\n\")\n\n\nInterpretation: Strong evidence of a relationship between income and food expenditure \n\n\n\n\n\n\n\n\n\nSummary of All Three Tests\n\n\n\nSUMMARY TABLE\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nTest Type        | H0          | p-value | Decision\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nRight-tail (&gt;)   | Î²2 â‰¤ 5.5    | %.4f    | if p_right &lt; 0.05, \"REJECT\"\nLeft-tail (&lt;)    | Î²2 â‰¥ 15     | %.4f    | if p_left &lt; 0.05, \"REJECT\"\nTwo-tail (â‰ )     | Î²2 = 0      | %.4f    | if p_two &lt; 0.05, \"REJECT\"\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n\nğŸ’¡ Interpretation Guide:\n\\(p &lt; 0.01\\): Very strong evidence against \\(H_0\\)\nâ€‹ \\(0.01 â‰¤ p &lt; 0.05\\): Strong evidence against \\(H_0\\)\nâ€‹ \\(0.05 â‰¤ p &lt; 0.10\\): Weak evidence against \\(H_0\\)\nâ€‹ \\(p â‰¥ 0.10\\): Insufficient evidence against \\(H_0\\)\nâš ï¸ Common Student Mistakes to Avoid:\n\nâ€œp-value is the probability that Hâ‚€ is trueâ€ â†’ âŒ WRONG!\n\nCorrect: p-value is the probability of getting our data if Hâ‚€ were true\n\nâ€œFail to reject Hâ‚€ means Hâ‚€ is trueâ€ â†’ âŒ WRONG!\n\nCorrect: We simply donâ€™t have enough evidence to reject it\n\nâ€œReject Hâ‚€ means HA is proven trueâ€ â†’ âŒ WRONG!\n\nCorrect: We have strong evidence against Hâ‚€, supporting HA\n\nConfusing the direction in one-tailed tests â†’ Common error!\n\nRight-tail: Testing if parameter is greater than c\nLeft-tail: Testing if parameter is less than c\n\n\n\n\nPART C - Testing Linear Combinations of Parameters\nWe want to test hypotheses about combinations of parameters, not individual coefficients.Example: What is the expected food expenditure for a household earning $2,000/week (income = 20 in our units)?\n\\[\nL = E(\\text{food\\_exp}\\mid \\text{income}=20) = \\beta_1 + 20\\,\\beta_2\n\\]\nVariance identities:\n\\[\n\\operatorname{var}(aX+bY) = a^2\\,\\operatorname{var}(X) + b^2\\,\\operatorname{var}(Y) + 2ab\\,\\operatorname{cov}(X,Y)\n\\]\n\\[\n\\operatorname{var}(b_1+20b_2) = \\operatorname{var}(b_1) + 20^2\\operatorname{var}(b_2) + 2\\cdot20\\operatorname{cov}(b_1,b_2)\n\\]\nâš ï¸ Critical Point: Many students forget the covariance term! Omitting it leads to incorrect standard errors.\n\n# Significance level and x-value of interest\nalpha &lt;- 0.05\nx &lt;- 20\n\n# the model\nm1 &lt;- lm(food_exp ~ income, data=food)\n\n\n# Extract coefficients and variance-covariance matrix\nb1 &lt;- m1$coef[1]\nb2 &lt;- m1$coef[2]\nvarb1 &lt;- vcov(m1)[1,1]\nvarb2 &lt;- vcov(m1)[2,2]\ncovb1b2 &lt;- vcov(m1)[1,2]\n\n# Compute the linear combination and its variance\nL &lt;- b1 + b2*x\nvarL &lt;- varb1 + x^2 * varb2 + 2*x*covb1b2\nseL &lt;- sqrt(varL)\n\n# Confidence interval for L\ndf &lt;- df.residual(m1)\ntcr &lt;- qt(1-alpha/2, df)\nlowbL &lt;- L - tcr*seL; upbL &lt;- L + tcr*seL\nc(L=L, seL=seL, low=lowbL, up=upbL)\n\n  L.(Intercept)             seL low.(Intercept)  up.(Intercept) \n      287.60886        14.17804       258.90692       316.31081 \n\ncat(\"Expected food expenditure at income = $2000:\\n\")\n\nExpected food expenditure at income = $2000:\n\ncat(\"  Point estimate:\", round(L, 2), \"\\n\")\n\n  Point estimate: 287.61 \n\ncat(\"  Standard error:\", round(seL, 2), \"\\n\")\n\n  Standard error: 14.18 \n\ncat(\"  95% CI: [\", round(lowbL, 2), \",\", round(upbL, 2), \"]\\n\\n\")\n\n  95% CI: [ 258.91 , 316.31 ]\n\n\n\n\nHypothesis Test for Linear Combination\nQuestion: Is expected food expenditure significantly different from $250 at this income level?\n\\[\nH_0: L = 250, \\qquad H_A: L \\ne 250.\n\\]\n\n# Hypothesized value\nc &lt;- 250\n\n# Test statistic\nt &lt;- (L - c)/seL\n\n# p-value (two-tailed)\np_value &lt;- 2*(1-pt(abs(t), df))\nc(t=t, p_value=p_value)\n\n      t.(Intercept) p_value.(Intercept) \n         2.65261316          0.01159078 \n\ncat(\"Hypothesis test: H0: L = 250\\n\")\n\nHypothesis test: H0: L = 250\n\ncat(\"  Test statistic:\", round(t, 3), \"\\n\")\n\n  Test statistic: 2.653 \n\ncat(\"  p-value:\", round(p_value, 4), \"\\n\")\n\n  p-value: 0.0116 \n\ncat(\"  Decision at Î±=0.05:\", ifelse(p_value &lt; 0.05, \"Reject H0\", \"Fail to reject H0\"), \"\\n\")\n\n  Decision at Î±=0.05: Reject H0 \n\n\nInterpretation: We fail to reject the null hypothesis. There is insufficient evidence that expected food expenditure differs from $250 for households earning $2,000/week.\n\n\nPART D - MCQs\n\n\n1) In hypothesis testing, Hâ‚€ represents:\n\n\n The research hypothesis What we hope to prove The status quo or claim being tested The alternative explanation\n\n\n\n\nHint\n\nThink of it as the â€œinnocent until proven guiltyâ€ claim.\n\n\n\n2) The test statistic t = (b - c) / se(b) measures:\n\n\n The absolute error The confidence level How many standard errors b is from c The sample size\n\n\n\n\nHint\n\nItâ€™s a standardized distance.\n\n\n\n3) For a two-tailed test at Î± = 0.05 with df = 38, the critical value is approximately:\n\n\n 1.645 1.686 Â±2.024 Â±1.96\n\n\n\n\nHint\n\nUse qt(0.975, 38) for the upper tail.\n\n\n\n4) A p-value of 0.03 means:\n\n\n Hâ‚€ has 3% chance of being true There's a 3% chance the result is wrong If Hâ‚€ were true, there's a 3% chance of results this extreme The effect size is 3%\n\n\n\n\n5) We reject Hâ‚€ when:\n\n\n p-value &lt; Î± p-value &gt; Î± t-statistic &lt; 0 Confidence interval includes zero\n\n\n\n\n6) For a right-tailed test, the p-value is calculated as:\n\n\n pt(t, df) 2 * pt(abs(t), df) 1 - pt(t, df) pt(-t, df)\n\n\n\n\nHint\n\nWe want the upper tail probability.\n\n\n\n7) A 95% confidence interval for Î²â‚‚ is [3, 8]. In a two-tailed test at Î± = 0.05, we would:\n\n\n Reject Hâ‚€: Î²â‚‚ = 5 Fail to reject Hâ‚€: Î²â‚‚ = 5 Reject Hâ‚€: Î²â‚‚ = 0 Need more information\n\n\n\n\nHint\n\n5 is inside the interval.\n\n\n\n8) The same confidence interval [3, 8] tells us we would:\n\n\n Fail to reject Hâ‚€: Î²â‚‚ = 0 Reject Hâ‚€: Î²â‚‚ = 0 Accept Hâ‚€: Î²â‚‚ = 10 Reject Hâ‚€: Î²â‚‚ = 6\n\n\n\n\nHint\n\n0 is outside the interval.\n\n\n\n9) To test L = Î²â‚ + 20Î²â‚‚, we need:\n\n\n Only var(Î²â‚) and var(Î²â‚‚) Only the covariance Var(Î²â‚), var(Î²â‚‚), and cov(Î²â‚, Î²â‚‚) Just the point estimate\n\n\n\n\n10) The variance of L = aÂ·X + bÂ·Y includes the term:\n\n\n ab Â· var(X) ab Â· var(Y) 2ab Â· cov(X,Y) (a+b)Â² Â· var(X)\n\n\n\n\nHint\n\nDonâ€™t forget the â€œ2â€ and the product ab.\n\n\n\n11) vcov(model) returns:\n\n\n Only variances Only covariances A matrix with variances on diagonal and covariances off-diagonal Confidence intervals\n\n\n\n\n12) A Type I error occurs when:\n\n\n We fail to reject a false Hâ‚€ We reject a true Hâ‚€ Our sample size is too small The p-value is large\n\n\n\n\nHint\n\nItâ€™s controlled by Î±.\n\n\n\n13) The significance level Î± represents:\n\n\n The p-value The probability Hâ‚€ is true The probability of Type I error The power of the test\n\n\n\n\n14) For Hâ‚€: Î²â‚‚ = 0, if the 95% CI is [2.5, 7.5], then:\n\n\n p-value &gt; 0.05 p-value &lt; 0.05 We cannot reject Hâ‚€ The estimate is unbiased\n\n\n\n\nHint\n\n0 is not in the interval, so we reject.\n\n\n\n15) In R, to get the p-value for a right-tailed test with t = 2.5 and df = 30:\n\n\n pt(2.5, 30) 2*pt(2.5, 30) 1 - pt(2.5, 30) pt(2.5, 30, lower.tail=TRUE)\n\n\n\n\n16) A larger sample size generally:\n\n\n Increases standard errors Decreases standard errors Doesn't affect hypothesis tests Increases the critical value\n\n\n\n\nHint\n\nMore data = more precision.\n\n\n\n17) The degrees of freedom for a simple regression (N=40) is:\n\n\n 40 39 38 37\n\n\n\n\nHint\n\ndf = N - K where K=2 (intercept + slope).\n\n\n\n18) If |t| &lt; t_critical, we:\n\n\n Reject Hâ‚€ Fail to reject Hâ‚€ Accept Hâ‚ Recalculate the test\n\n\n\n\n19) A one-tailed test is appropriate when:\n\n\n We don't know the direction We want more power Theory predicts a specific direction The sample size is small\n\n\n\n\n20) The t-distribution approaches the normal distribution as:\n\n\n Î± decreases Î± increases n decreases n increases\n\n\n\n\nHint\n\nWith large df, t â‰ˆ z.\n\n\n\n\n\n\nSummary\n\nHypothesis testing provides a formal framework for evaluating claims about parameters\nTest statistics measure how far estimates are from hypothesized values in standard error units\np-values quantify the strength of evidence against Hâ‚€\nConfidence intervals and hypothesis tests are mathematically equivalent\nLinear combinations require accounting for covariances in variance calculations\n\n\nğŸ End of lab 9 ğŸ›‘ Remember to save your script ğŸ’¾\n\n\n\n\nReferences\nAcemoglu, D., Laibson, D., & List, J. A. (2015). Microeconomics (2nd ed.). Pearson. pp.Â 512â€“530. (For conceptual understanding of regression interpretation and causal inference basics.)\nColonescu, C. (2022). Principles of Econometrics with R (Version 2.0). Open Textbook Library. Chapters 7â€“9. (Used directly in labs, matches the PoEdata package and R examples.)\nGujarati, D. N., Porter, D. C., & Gunasekar, S. (2017). Basic Econometrics (5th ed.). McGraw-Hill Education. pp.Â 126â€“150, 175â€“195. (For theory and assumptions behind simple and multiple linear regression.)\nStock, J. H., & Watson, M. W. (2020). Introduction to Econometrics (4th ed.). Pearson. pp.Â 97â€“121, 145â€“165. (For hypothesis testing, p-values, and model diagnostics.)\n\n\n\nFormative Test (This test does NOT carry any marks)\n or link: MCQs test",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Supervision 2</span>"
    ]
  },
  {
    "objectID": "supervision_3.html",
    "href": "supervision_3.html",
    "title": "Supervision 3",
    "section": "",
    "text": "Lab 10 â€” Data Prep and Best Variable Selection",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Supervision 3</span>"
    ]
  },
  {
    "objectID": "supervision_3.html#lab-10-data-prep-and-best-variable-selection",
    "href": "supervision_3.html#lab-10-data-prep-and-best-variable-selection",
    "title": "Supervision 3",
    "section": "",
    "text": "ğŸ¯ Learning outcomes\n\nLoad the Hitters data and inspect dimensions.\nHandle missing values (Salary) to avoid errors in model fitting.\nGet comfortable with the packages weâ€™ll use: ISLR and leaps.\nFit best subset models with regsubsets().\nRetrieve the best model for each size.\nInspect classical metrics: RSS, RÂ², Adjusted RÂ², Cp, BIC.\n\n\n\n\nğŸ§° Prerequisites\n\nR and RStudio installed.\nPackages: ISLR, leaps.\n\n\n\n\nOverview: Why Variable Selection Matters\nIn regression modeling, we often face a critical question: Which variables should we include?\nThe Problem:\n\nInclude too few variables â†’ underfitting (bias, missing important relationships)\nInclude too many variables â†’ overfitting (poor predictions, unstable estimates)\n\nWhen valuing agricultural land, we might have 20+ potential predictors: soil quality, rainfall, distance to markets, elevation, slope, nearby infrastructure, etc. Which combination best predicts land value without overfitting?\n\n# install.packages(c(\"ISLR\", \"leaps\", \"ggplot2\", \"dplyr\")) if needed\nlibrary(ISLR)\n\nWarning: package 'ISLR' was built under R version 4.4.3\n\nlibrary(leaps)\n\nWarning: package 'leaps' was built under R version 4.4.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Load the baseball players dataset\ndata(Hitters)\n\n# Understanding the Dataset\n# Inspect\nstr(Hitters)\n\n'data.frame':   322 obs. of  20 variables:\n $ AtBat    : int  293 315 479 496 321 594 185 298 323 401 ...\n $ Hits     : int  66 81 130 141 87 169 37 73 81 92 ...\n $ HmRun    : int  1 7 18 20 10 4 1 0 6 17 ...\n $ Runs     : int  30 24 66 65 39 74 23 24 26 49 ...\n $ RBI      : int  29 38 72 78 42 51 8 24 32 66 ...\n $ Walks    : int  14 39 76 37 30 35 21 7 8 65 ...\n $ Years    : int  1 14 3 11 2 11 2 3 2 13 ...\n $ CAtBat   : int  293 3449 1624 5628 396 4408 214 509 341 5206 ...\n $ CHits    : int  66 835 457 1575 101 1133 42 108 86 1332 ...\n $ CHmRun   : int  1 69 63 225 12 19 1 0 6 253 ...\n $ CRuns    : int  30 321 224 828 48 501 30 41 32 784 ...\n $ CRBI     : int  29 414 266 838 46 336 9 37 34 890 ...\n $ CWalks   : int  14 375 263 354 33 194 24 12 8 866 ...\n $ League   : Factor w/ 2 levels \"A\",\"N\": 1 2 1 2 2 1 2 1 2 1 ...\n $ Division : Factor w/ 2 levels \"E\",\"W\": 1 2 2 1 1 2 1 2 2 1 ...\n $ PutOuts  : int  446 632 880 200 805 282 76 121 143 0 ...\n $ Assists  : int  33 43 82 11 40 421 127 283 290 0 ...\n $ Errors   : int  20 10 14 3 4 25 7 9 19 0 ...\n $ Salary   : num  NA 475 480 500 91.5 750 70 100 75 1100 ...\n $ NewLeague: Factor w/ 2 levels \"A\",\"N\": 1 2 1 2 2 1 1 1 2 1 ...\n\ndim(Hitters)          # rows x columns\n\n[1] 322  20\n\nsum(is.na(Hitters$Salary))\n\n[1] 59\n\n# What are we predicting?\ncat(\"Response variable: Salary (1987 annual salary in thousands of dollars)\\n\\n\")\n\nResponse variable: Salary (1987 annual salary in thousands of dollars)\n\n# Predictor variables\ncat(\"Predictor variables:\\n\")\n\nPredictor variables:\n\nnames(Hitters)[-19]   # All except Salary\n\n [1] \"AtBat\"     \"Hits\"      \"HmRun\"     \"Runs\"      \"RBI\"       \"Walks\"    \n [7] \"Years\"     \"CAtBat\"    \"CHits\"     \"CHmRun\"    \"CRuns\"     \"CRBI\"     \n[13] \"CWalks\"    \"League\"    \"Division\"  \"PutOuts\"   \"Assists\"   \"Errors\"   \n[19] \"NewLeague\"\n\n# Handling Missing Values, drop rows with any NA (Salary has some NAs)\nHitters &lt;- na.omit(Hitters)\ndim(Hitters)\n\n[1] 263  20\n\nsum(is.na(Hitters))\n\n[1] 0\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe remove rows with missing Salary because selection routines (like regsubsets) require complete cases. Alternative approaches (not covered here): imputation, missing data models\n\n\n\n\nPART A - Best Subset Selection\nConcept: Fit every possible combination of predictors and choose the best according to some criterion.\nComputational Reality: With p predictors, there are 2^p possible models. For Hitters (19 predictors): 2^19 = 524,288 models! Feasible for p â‰¤ 20-30, but impractical for larger p.\n\n#library(leaps)\n\n# Exhaustive search over all subsets\nregfit.full &lt;- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)\n\n# Extract summary information\nreg.summary &lt;- summary(regfit.full)\n\n# What does summary() return? What information is available?\nnames(reg.summary)\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n# Quick look at R^2 growth as we add variables\nreg.summary$rsq\n\n [1] 0.3214501 0.4252237 0.4514294 0.4754067 0.4908036 0.5087146 0.5141227\n [8] 0.5285569 0.5346124 0.5404950 0.5426153 0.5436302 0.5444570 0.5452164\n[15] 0.5454692 0.5457656 0.5459518 0.5460945 0.5461159\n\n\n\n\n\n\n\n\nTip\n\n\n\nregsubsets() searches the model space and stores the best model for each size (1..nvmax). Use summary() to see a which matrix and metrics across sizes.\n\n\nUnderstanding the ouptut: The â€˜whichâ€™ matrix shows selected variables for each model size; TRUE = variable included, FALSE = excluded.\n\nhead(reg.summary$which, 10)\n\n   (Intercept) AtBat  Hits HmRun  Runs   RBI Walks Years CAtBat CHits CHmRun\n1         TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  FALSE FALSE  FALSE\n2         TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  FALSE FALSE  FALSE\n3         TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  FALSE FALSE  FALSE\n4         TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  FALSE FALSE  FALSE\n5         TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  FALSE FALSE  FALSE\n6         TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE  FALSE FALSE  FALSE\n7         TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE   TRUE  TRUE   TRUE\n8         TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE  FALSE FALSE   TRUE\n9         TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE   TRUE FALSE  FALSE\n10        TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE   TRUE FALSE  FALSE\n   CRuns  CRBI CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  FALSE  TRUE  FALSE   FALSE     FALSE   FALSE   FALSE  FALSE      FALSE\n2  FALSE  TRUE  FALSE   FALSE     FALSE   FALSE   FALSE  FALSE      FALSE\n3  FALSE  TRUE  FALSE   FALSE     FALSE    TRUE   FALSE  FALSE      FALSE\n4  FALSE  TRUE  FALSE   FALSE      TRUE    TRUE   FALSE  FALSE      FALSE\n5  FALSE  TRUE  FALSE   FALSE      TRUE    TRUE   FALSE  FALSE      FALSE\n6  FALSE  TRUE  FALSE   FALSE      TRUE    TRUE   FALSE  FALSE      FALSE\n7  FALSE FALSE  FALSE   FALSE      TRUE    TRUE   FALSE  FALSE      FALSE\n8   TRUE FALSE   TRUE   FALSE      TRUE    TRUE   FALSE  FALSE      FALSE\n9   TRUE  TRUE   TRUE   FALSE      TRUE    TRUE   FALSE  FALSE      FALSE\n10  TRUE  TRUE   TRUE   FALSE      TRUE    TRUE    TRUE  FALSE      FALSE\n\n\nFor each model size (1 to 19 variables), regsubsets finds the BEST model according to RSS (equivalently, training RÂ²)\nModel Selection Criteria Explained\n\nRSS (Residual Sum of Squares)\n\n\nMeasures training error\nAlways decreases as we add variables\nProblem: Will always prefer the full model (overfitting)\n\n\nRÂ² (R-squared)\n\n\nProportion of variance explained\nAlways increases with more variables\nProblem: Same as RSS - no penalty for complexity\n\n\nAdjusted RÂ²\n\n\nPenalizes model complexity\nAccounts for degrees of freedom\nUse: Prefer model where this peaks\n\n\nCp (Mallowâ€™s Cp)\n\n\nEstimates test error, penalizes complexity\nSmall Cp indicates good model\nUse: Minimize Cp\n\n\nBIC (Bayesian Information Criterion)\n\n\nSimilar to Cp but with stronger penalty: \\(BIC=nlogâ¡(RSS/n)+plogâ¡(n)\\)\nTends to select smaller models than Cp\nUse: Minimize BIC\n\n\n\nVisual diagnostics for choosing model size\n\npar(mfrow = c(2,2))\n\n# 1. RSS - always decreases\nplot(reg.summary$rss, xlab = \"Number of Variables\", ylab = \"RSS\", type = \"l\")\ntitle(\"Training RSS (not useful for selection)\")\n\n# Adjusted R^2 (highlight maximum)\nplot(reg.summary$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted R^2\", type = \"l\")\nbest.adjr2 &lt;- which.max(reg.summary$adjr2)\npoints(best.adjr2, reg.summary$adjr2[best.adjr2], col = \"red\", cex = 2, pch = 20)\ntitle(\"Adjusted RÂ² (maximize)\")\n\n# Cp (highlight minimum)\nplot(reg.summary$cp, xlab = \"Number of Variables\", ylab = \"Cp\", type = \"l\")\nbest.cp &lt;- which.min(reg.summary$cp)\npoints(best.cp, reg.summary$cp[best.cp], col = \"red\", cex = 2, pch = 20)\ntitle(\"Mallow's Cp (minimize)\")\n\n# BIC (highlight minimum)\nplot(reg.summary$bic, xlab = \"Number of Variables\", ylab = \"BIC\", type = \"l\")\nbest.bic &lt;- which.min(reg.summary$bic)\npoints(best.bic, reg.summary$bic[best.bic], col = \"red\", cex = 2, pch = 20)\ntitle(\"BIC (minimize)\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n# Interpretation\ncat(\"Optimal model sizes according to different criteria:\\n\")\n\nOptimal model sizes according to different criteria:\n\ncat(\"  Adjusted RÂ²:\", best.adjr2, \"variables\\n\")\n\n  Adjusted RÂ²: 11 variables\n\ncat(\"  Cp:        \", best.cp, \"variables\\n\")\n\n  Cp:         10 variables\n\ncat(\"  BIC:       \", best.bic, \"variables\\n\\n\")\n\n  BIC:        6 variables\n\ncat(\"Note: BIC typically selects the most parsimonious (smallest) model\\n\")\n\nNote: BIC typically selects the most parsimonious (smallest) model\n\n\n\n\n\n\n\n\nNote\n\n\n\nRule of thumb: prefer the model size where Adjusted RÂ² peaks, or Cp/BIC are minimized. These are proxies for test error.\n\n\n\nInspect variables in the chosen model(s)\n\n# Example: variables in the BIC-optimal model\ncoef(regfit.full, best.bic)\n\n (Intercept)        AtBat         Hits        Walks         CRBI    DivisionW \n  91.5117981   -1.8685892    7.6043976    3.6976468    0.6430169 -122.9515338 \n     PutOuts \n   0.2643076 \n\n\n\n\n\n\n\n\nTip\n\n\n\nReading the leaps Plots\nBlack squares = variable included in model Top row = model with best (minimum/maximum) criterion value Each row = one model size Look for consistent variables that appear across different model sizes\n\n\n\n# regsubsets has its own plot helper:\nplot(regfit.full, scale = \"bic\")\n\n\n\n\n\n\n\nplot(regfit.full, scale = \"adjr2\")\n\n\n\n\n\n\n\nplot(regfit.full, scale = \"Cp\")\n\n\n\n\n\n\n\nplot(regfit.full, scale = \"r2\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nCommon Mistake Donâ€™t use training RÂ² to select model size! It will always prefer the full model. Use Adjusted RÂ², Cp, or BIC instead.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Supervision 3</span>"
    ]
  },
  {
    "objectID": "supervision_3.html#lab-11-forward-stepwise-selection",
    "href": "supervision_3.html#lab-11-forward-stepwise-selection",
    "title": "Supervision 3",
    "section": "Lab 11 â€” Forward Stepwise Selection",
    "text": "Lab 11 â€” Forward Stepwise Selection\n\nğŸ¯ Learning outcomes\nBy the end of this lab you will be able to:\n\nUnderstand the greedy algorithm behind forward selection\nImplement forward stepwise using method = â€œforwardâ€\nCompare results with best subset selection\nRecognize when forward stepwise is preferable (computational constraints)\n\n\n\n\nThe Forward Stepwise Algorithm\nStep-by-step process:\n\nStart with null model (intercept only)\nFor each of p predictors, fit a model adding that predictor\nAdd the predictor that most improves the model (lowest RSS)\nRepeat: given k variables, try adding each of the (p-k) remaining\nContinue until all p predictors are included\n\nKey Properties:\n\nEvaluates only 1 + p(p+1)/2 models (vs.Â 2^p for best subset)\nFor p=19: 191 models (vs.Â 524,288!)\nGreedy: Once a variable enters, it stays\nMay not find the globally optimal model\n\n\n# Forward stepwise selection\nregfit.fwd &lt;- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = \"forward\")\n\nsummary(regfit.fwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"forward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: forward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\n# Compare a specific size (e.g., 7 variables) across methods\ncoef(regfit.full, 7)  # best subset\n\n (Intercept)         Hits        Walks       CAtBat        CHits       CHmRun \n  79.4509472    1.2833513    3.2274264   -0.3752350    1.4957073    1.4420538 \n   DivisionW      PutOuts \n-129.9866432    0.2366813 \n\ncoef(regfit.fwd, 7)   # forward stepwise\n\n (Intercept)        AtBat         Hits        Walks         CRBI       CWalks \n 109.7873062   -1.9588851    7.4498772    4.9131401    0.8537622   -0.3053070 \n   DivisionW      PutOuts \n-127.1223928    0.2533404 \n\n\n\nAre the 7-variable-models identical?\nWhich variables differ?\n\n\n\n\n\n\n\nWarning\n\n\n\nForward stepwise is greedy: after a variable enters, it stays. It evaluates far fewer models than best subsetâ€”great for speedâ€”but it may miss the global optimum.\n\n\n\nWhen to Use Forward Stepwise\nAdvantages:\n\nMuch faster than best subset (critical for p &gt; 20)\nOften finds nearly optimal solutions\nRequired when n &lt; p (best subset impossible)\n\nDisadvantages:\n\nNot guaranteed to find global optimum\nOrder of variable entry matters\nCannot remove variables once added",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Supervision 3</span>"
    ]
  },
  {
    "objectID": "supervision_3.html#lab-12-backward-stepwise-selection",
    "href": "supervision_3.html#lab-12-backward-stepwise-selection",
    "title": "Supervision 3",
    "section": "Lab 12 â€” Backward Stepwise Selection",
    "text": "Lab 12 â€” Backward Stepwise Selection\n\nğŸ¯ Learning outcomes\nBy the end of this lab you will be able to:\n\nUnderstand the backward elimination algorithm\nImplement backward stepwise using method = â€œbackwardâ€\nRecognize the n &gt; p requirement\nCompare all three methods systematically\n\n\n\n\nThe Backward Stepwise Selection\nStep-by-step process:\nStart with full model (all p predictors) Remove the predictor with smallest contribution (largest p-value) Fit all (p-1) models with one variable removed Choose model with lowest RSS (or highest RÂ²) Repeat until intercept-only model\nKey Properties:\nAlso evaluates 1 + p(p+1)/2 models Requires n &gt; p (must fit full model first) Greedy: Once removed, variable cannot re-enter May differ from forward stepwise\n\n# Backward stepwise selection\nregfit.bwd &lt;- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = \"backward\")\nsummary(regfit.bwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"backward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: backward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n4  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\n# Compare at size 7 again\ncoef(regfit.bwd, 7)   # backward stepwise\n\n (Intercept)        AtBat         Hits        Walks        CRuns       CWalks \n 105.6487488   -1.9762838    6.7574914    6.0558691    1.1293095   -0.7163346 \n   DivisionW      PutOuts \n-116.1692169    0.3028847 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nBackward stepwise starts from the full model and removes variables. It requires n &gt; p so that the full least squares fit exists.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Supervision 3</span>"
    ]
  },
  {
    "objectID": "supervision_3.html#lab-13-validation-set-approach-reading-the-plots-coefficients",
    "href": "supervision_3.html#lab-13-validation-set-approach-reading-the-plots-coefficients",
    "title": "Supervision 3",
    "section": "Lab 13 â€” Validation Set Approach, Reading the Plots & Coefficients",
    "text": "Lab 13 â€” Validation Set Approach, Reading the Plots & Coefficients\n\nğŸ¯ Learning outcomes\nBy the end of this lab you will be able to:\n\nSplit data into training and test sets\nFit models on training data only\nEvaluate test set performance (true prediction error)\nUnderstand why training error is optimistic\nSelect model size based on test error.\n\n\n# Choose a final size by any one of the criteria\nk_final &lt;- best.bic     # example: BIC-minimizer\n\n# Coefficients and variables\nfinal_coef &lt;- coef(regfit.full, k_final)\nfinal_coef\n\n (Intercept)        AtBat         Hits        Walks         CRBI    DivisionW \n  91.5117981   -1.8685892    7.6043976    3.6976468    0.6430169 -122.9515338 \n     PutOuts \n   0.2643076 \n\n# A compact report\ndata.frame(term = names(final_coef), estimate = as.numeric(final_coef))\n\n         term     estimate\n1 (Intercept)   91.5117981\n2       AtBat   -1.8685892\n3        Hits    7.6043976\n4       Walks    3.6976468\n5        CRBI    0.6430169\n6   DivisionW -122.9515338\n7     PutOuts    0.2643076\n\n\n\n\n\n\n\n\nTip\n\n\n\nReporting tip: Always state (i) the criterion used (e.g., BIC), (ii) the model size, and (iii) the selected variables with their coefficients. Avoid training RÂ² aloneâ€”prefer Cp/BIC/Adjusted RÂ².\n\n\n\n\n\nâœ… What you should now be able to do\n\nPrepare data and run best subset and stepwise selection.\nUse Adjusted RÂ², Cp, and BIC to choose model size.\nExtract and communicate the chosen variables and coefficients.\n\n\nNext session weâ€™ll compare these selections using a validation set and crossâ€‘validation, and then move to ridge and lasso.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Supervision 3</span>"
    ]
  },
  {
    "objectID": "supervision_4.html",
    "href": "supervision_4.html",
    "title": "Supervision 4",
    "section": "",
    "text": "Lab 14 â€” Collinearity",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Supervision 4</span>"
    ]
  },
  {
    "objectID": "supervision_4.html#lab-14-collinearity",
    "href": "supervision_4.html#lab-14-collinearity",
    "title": "Supervision 4",
    "section": "",
    "text": "ğŸ¯ Learning outcomes\n\nUnderstand what collinearity (multicollinearity) is and why it matters for regression inference.\nRecognise how collinearity inflates standard errors, weakening tâ€‘tests for individual coefficients.\nDiagnose collinearity using the Variance Inflation Factor (VIF) and interpret typical thresholds.\nImplement VIF diagnostics in R with car::vif() and report results clearly (tables, captions).\nConsider practical remedies (drop variables, transform/aggregate predictors, or rethink specification).\n\n\n\n\nğŸ§° Prerequisites\n\nMultiple regression basics (OLS, \\(R^2\\), Fâ€‘test, tâ€‘tests).\nR and RStudio installed.\nPackages: PoEdata, broom, knitr, car.\n\n\n\n\nOverview: why collinearity matters\nCollinearity does not bias OLS, but it inflates the variance of estimated coefficients. As a result, tâ€‘tests lose power and coefficient signs may look unstable across specificationsâ€”even when the model has a high \\(R^2\\). Diagnostics such as VIF help decide whether variables overlap so much that individual effects are hard to interpret.\n\nCollinearity (multicollinearity) among regressors occurs when two or more move closely together or have limited variability. It inflates the variance of estimated parameters (less precise t-tests), even though OLS estimates remain unbiased. As a result, a model may have a high \\(R^2\\) or a large Fâ€‘statistic but insignificant individual coefficients.\n\n\nğŸ§° Packages\nPoEdata, broom, knitr, car\n\n# install.packages(c(\"PoEdata\",\"broom\",\"knitr\",\"car\")) ; install once if not already installed\n\nlibrary(PoEdata)  # example datasets used in the course\nlibrary(broom)    # tidier model outputs (tidy/glance/augment)\n\nWarning: package 'broom' was built under R version 4.4.3\n\nlibrary(knitr)    # neat tables/printing (e.g., kable)\n\nWarning: package 'knitr' was built under R version 4.4.3\n\nlibrary(car)      # regression diagnostics (e.g., VIF)\n\nLoading required package: carData\n\n# Example data: cars (mpg = miles per gallon; cyl = cylinders; disp = engine displacement; wt = weight)\ndata(mtcars)\n\n# Simple model\n# fit a linear regression: mpg explained by cylinders\nmod1 &lt;- lm(mpg ~ cyl, data = mtcars)\n# convert model output to a neat table (broom::tidy + knitr::kable)\nkable(tidy(mod1), caption = \"A simple linear 'mpg' model\")\n\n\nA simple linear â€˜mpgâ€™ model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n37.88458\n2.0738436\n18.267808\n0\n\n\ncyl\n-2.87579\n0.3224089\n-8.919699\n0\n\n\n\n\n\nNow add more regressors:\n\n# fit a multiple regression with 3 predictors (cyl, disp, wt)\nmod2 &lt;- lm(mpg ~ cyl + disp + wt, data = mtcars)\n# format the model output into a clean table\nkable(tidy(mod2), caption = \"Multivariate 'mpg' model\")\n\n\nMultivariate â€˜mpgâ€™ model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n41.1076776\n2.8424260\n14.4621802\n0.0000000\n\n\ncyl\n-1.7849435\n0.6071105\n-2.9400638\n0.0065117\n\n\ndisp\n0.0074729\n0.0118447\n0.6309079\n0.5332173\n\n\nwt\n-3.6356770\n1.0401375\n-3.4953811\n0.0015955\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen adding eng and wgt, the coefficient for cyl can turn insignificantâ€”a classic sign of collinearity because these vehicle traits tend to move together.\n\n\n\n\nVariance Inflation Factor (VIF)\nThe VIF for regressor \\(x_k\\) is \\(VIF_k = \\dfrac{1}{1-R_k^2}\\), where \\(R_k^2\\) is from regressing \\(x_k\\) on the other regressors.\n\nv &lt;- car::vif(mod2)                           # calculate VIF values to diagnose multicollinearity\ntab &lt;- data.frame(regressor = names(v),       # create a small table with regressor names\n                  VIF = as.numeric(v),        # convert VIF values to numeric for display\n                  row.names = NULL)           # avoid row names in the table\nkable(tab, caption = \"Variance inflation factors for the 'mpg' regression model.\")  # nicely format output\n\n\nVariance inflation factors for the â€˜mpgâ€™ regression model.\n\n\nregressor\nVIF\n\n\n\n\ncyl\n5.413600\n\n\ndisp\n9.924054\n\n\nwt\n4.769703\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nRules of thumb\n- VIF &gt; 10 â†’ strong collinearity (commonly used cutoff)\n- VIF &gt; 5 â†’ moderate collinearity (early warning) These cut-offs are only heuristics. Evaluate VIFs alongside theory, sample size, and model purpose; high VIFs donâ€™t automatically require dropping variables (Oâ€™Brien, 2007).\n\n\n\n\nMitigations (when VIFs are high)\n\nRemove or combine collinear variables (e.g., combine cyl and eng into a power index).\n\nUse PCA to produce orthogonal components.\n\nStandardise/centre variables (helps especially with interactions/polynomials (it wonâ€™t â€œfixâ€ genuine overlap among distinct variables).\n\nUse Ridge regression if retaining all variables is preferred.\n\nConsider transformations (e.g., logs).\n\n\n\n\nSummary\n\nCollinearity leaves OLS unbiased but makes estimates imprecise.\nAlways check VIFs when several predictors are conceptually related.\nIf VIFs are high, consider dropping, combining, or reâ€‘specifying predictorsâ€”and justify choices with theory.\n\nReferences Oâ€™Brien, Robert. (2007). A Caution Regarding Rules of Thumb for Variance Inflation Factors. Quality & Quantity. 41. 673-690. 10.1007/s11135-006-9018-6.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Supervision 4</span>"
    ]
  },
  {
    "objectID": "supervision_4.html#lab-15-heteroskedasticity",
    "href": "supervision_4.html#lab-15-heteroskedasticity",
    "title": "Supervision 4",
    "section": "Lab 15 â€” Heteroskedasticity",
    "text": "Lab 15 â€” Heteroskedasticity\n\nğŸ¯ Learning outcomes\n\nUnderstand the key concept(s) introduced in this lab and their purpose in empirical analysis.\nCorrectly implement the core R workflow for this topic and present clean, well-captioned results.\nInterpret model output in context and link diagnostics to modeling choices.\nRecognise common pitfalls and apply simple robustness checks.\n\n\n\n\nğŸ§° Prerequisites\n\nPrior exposure to multiple regression and basic inference (t-tests, F-tests).\nR and RStudio installed.\nPackages: PoEdata, broom, knitr, plus any additional packages used in this lab.\n\n\n\n\nOverview\nThis lab introduces the core idea, why it matters for inference and decision-making, and where it fits in the broader modeling workflow. You will practise the implementation in R, interpret outputs carefully, and connect diagnostics back to modeling decisions.\n\nThe Gaussâ€“Markov theorem assumes homoskedasticity (constant error variance \\(\\sigma^2\\)). In many economic datasets, variance grows with some regressors (e.g., higherâ€‘income households show more dispersion in expenditure). With heteroskedasticity, OLS coefficients remain unbiased, but standard errors and therefore, with heteroskedasticity, OLS coefficients remain unbiased (under exogeneity) but the usual SEs are inconsistentâ€”use HC (White) or other robust SEs. For completeness, apart from homoskedasticity, Gaussâ€“Markov (for OLS to be BLUE) also requires linearity in parameters, no perfect collinearity, and exogeneity. Independence/no autocorrelation is also standard in time-series contexts.\n\n\nğŸ§° Packages\nlmtest, broom, PoEdata, car, sandwich, knitr\n\n# install.packages(c(\"lmtest\",\"broom\",\"PoEdata\",\"car\",\"sandwich\",\"knitr\"))  # install once if needed\n\nlibrary(lmtest)    # tests for regression models (e.g., Breusch-Pagan)\n\nLoading required package: zoo\n\n\nWarning: package 'zoo' was built under R version 4.4.3\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(broom)     # tidy/glance/augment model outputs\nlibrary(PoEdata)   # course datasets\nlibrary(car)       # diagnostics (e.g., VIF)\nlibrary(sandwich)  # robust (heteroskedasticity-consistent) SEs\nlibrary(knitr)     # neat tables (kable)\n\ndata(\"food\", package = \"PoEdata\")            # load the 'food' dataset\nmod1 &lt;- lm(food_exp ~ income, data = food)   # simple OLS: food expenditure explained by income\n\n# Scatter and fitted line\nplot(food$income, food$food_exp,             # scatterplot of income vs. food expenditure\n     xlab = \"income\", ylab = \"food expenditure\",\n     pch = 19, col = \"grey\")                 # filled points, grey color\nabline(mod1, lwd = 2)                        # add fitted regression line with thicker stroke\n\n\n\n\n\n\n\n\n\n\n\nSpotting heteroskedasticity with residual plots\nResidual plots help detect heteroskedasticity visually. Under homoskedasticity, residuals should scatter randomly around zero with constant spread. Look for:\n\nFanning patterns: residuals spread out (or narrow) as x increases\nClusters or groups: different variance across subpopulations\nNon-random patterns: suggests model misspecification\n\n\nres  &lt;- residuals(mod1)          # residuals (observed - fitted)\nyhat &lt;- fitted(mod1)             # fitted (predicted) values from the model\n\n# Residuals vs. income (check for nonlinearity or changing spread with income)\nplot(food$income, res,\n     xlab = \"income\", ylab = \"residuals\",\n     pch = 19, col = \"grey\")\nabline(h = 0, lty = 2)           # reference line at zero residual\n\n\n\n\n\n\n\n# Residuals vs. fitted values (diagnose heteroskedasticity/patterns)\nplot(yhat, res,\n     xlab = \"fitted values\", ylab = \"residuals\",\n     pch = 19, col = \"grey\")\nabline(h = 0, lty = 2)           # reference line at zero residual\n\n\n\n\n\n\n\n\n\n\nBreuschâ€“Pagan test (manual construction)\nThe Breuschâ€“Pagan test is a formal test for heteroskedasticity. The null hypothesis is \\(H_0:\\) \\(\\text{homoskedasticity (constant variance)}\\).\nTest statistic: \\(\\chi^2 = N \\cdot R^2_{\\text{aux}}\\) \nwhere \\(R^2_{\\text{aux}}\\) comes from regressing squared residuals on the original regressors:\n\\(\\hat{e}_i^{\\,2}\n= \\gamma_0 + \\gamma_1 x_{i1} + \\gamma_2 x_{i2} + \\cdots + \\gamma_k x_{ik} + \\text{error}\\) \nUnder \\(H_0\\), \\(\\chi^2 \\sim \\chi^2_k\\) (k = number of regressors, excluding intercept).\n\nalpha &lt;- 0.05                              # significance level (5%)\n\nressq &lt;- resid(mod1)^2                     # squared residuals (used to detect heteroskedasticity)\n\nmodres &lt;- lm(ressq ~ income, data = food)  # auxiliary regression: regress squared residuals on income\n\nN &lt;- nobs(modres)                           # number of observations\nS &lt;- summary(modres)$df[1]-1                  # degrees of freedom for regression (k = number of predictors)\n\nchisqcr &lt;- qchisq(1 - alpha, S)             # chi-square critical value (right tail)\n\nRsqres &lt;- summary(modres)$r.squared         # RÂ² from auxiliary regression\nchisq  &lt;- N * Rsqres                        # Breuschâ€“Pagan test statistic\npval   &lt;- 1 - pchisq(chisq, S)              # p-value (probability of observing that statistic or larger)\n\nc(statistic = chisq, crit = chisqcr, p.value = pval)   # show results: test statistic, critical value, p-value\n\n  statistic        crit     p.value \n7.384424443 3.841458821 0.006579112 \n\n\n\n\n\n\n\n\nInterpreting the Breuschâ€“Pagan test\n\n\n\nDecision rule: - If p-value &lt; 0.05 (or your chosen \\(\\alpha\\)): reject \\(H_0\\) â†’ evidence against homoskedasticity (i.e., evidence of heteroskedasticity) - If p-value â‰¥ 0.05: fail to reject \\(H_0\\) â†’ no strong evidence of heteroskedasticity\nAlternatively, compare the test statistic to the critical value: - If \\(\\chi^2\\) &gt; critical value: reject \\(H_0\\) - If \\(\\chi^2\\) â‰¤ critical value: fail to reject \\(H_0\\)\nIn this example, if p-value is small (&lt; 0.05), we have evidence that variance increases with income.\n\n\n\n\nWhite (quadratic) version\nWhiteâ€™s test extends Breuschâ€“Pagan by including squares and cross-products of regressors in the auxiliary regression. This detects more general forms of heteroskedasticity (not just linear). For a model with one regressor \\(x\\):\n\\(\\hat{e}_i^{\\,2} = \\gamma_0 + \\gamma_1 x_{i1} + \\gamma_2 x^2_{i2} + \\text{error}\\)\nTest statistic: \\(\\chi^2 = N \\cdot R^2_{\\text{aux}}\\) âˆ¼ \\(\\chi^2_k\\), where \\(k\\) is the number of regressors in the auxiliary regression (excluding the intercept).\n\n# auxiliary regression with quadratic term (tests nonlinear heteroskedasticity)\nmodres2 &lt;- lm(ressq ~ income + I(income^2), data = food)    \nRsq2 &lt;- summary(modres2)$r.squared          # RÂ² from auxiliary regression\nS2   &lt;- summary(modres2)$df[1]-1              # degrees of freedom (number of regressors)\nchisq2 &lt;- N * Rsq2                          # Breuschâ€“Pagan-type chi-square statistic\npval2  &lt;- 1 - pchisq(chisq2, S2)            # p-value (right-tail)\nc(statistic = chisq2, df = S2, p.value = pval2)            # show results\n\n statistic         df    p.value \n7.55507856 2.00000000 0.02287892 \n\n\n\n\n\n\n\n\nInterpreting Whiteâ€™s test\n\n\n\nDecision rule: - If p-value &lt; 0.05: reject \\(H_0\\) â†’ evidence of heteroskedasticity (possibly nonlinear) - If p-value â‰¥ 0.05: fail to reject \\(H_0\\) â†’ no strong evidence\nWhiteâ€™s test is more general than Breuschâ€“Pagan (detects nonlinear heteroskedasticity) but uses more degrees of freedom. In small samples, use Breuschâ€“Pagan if you suspect linear heteroskedasticity, or White if you suspect more complex patterns.\n\n\n\n\nBreuschâ€“Pagan with bptest()\n\n# run Breuschâ€“Pagan test and tidy the output\n# display results as a formatted table\nkable(tidy(bptest(mod1)), caption = \"Breuschâ€“Pagan heteroskedasticity test\")   \n\n\nBreuschâ€“Pagan heteroskedasticity test\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n\n\n7.384424\n0.0065791\n1\nstudentized Breusch-Pagan test\n\n\n\n\n\n\n\n\n\n\n\nInterpreting bptest() output\n\n\n\nThe bptest() function reports: - statistic: the BP test statistic (approximately \\(\\chi^2\\) distributed) - p.value: probability of observing this statistic under \\(H_0\\) - parameter: degrees of freedom\nDecision: If p-value &lt; 0.05, reject homoskedasticity and consider using robust standard errors.\n\n\n\n\nGoldfeldâ€“Quandt test (indicator split: metro vs rural)\nTests whether error variance differs between two subgroups. Split the sample by a grouping variable or by ordering observations (e.g., by income level).\nTest statistic:\n\\(F = \\dfrac{\\hat{\\sigma}_1^{\\,2}}{\\hat{\\sigma}_2^{\\,2}}\\) \nwhere \\(\\hat{\\sigma}_1^{\\,2}\\) and \\(\\hat{\\sigma}_2^{\\,2}\\) are estimated error variances from each subsample.\nUnder \\(H_0\\) (equal variances),\n\\(F \\sim F_{df_1, df_2}\\).\n\nalpha &lt;- 0.05                           # significance level (5%)\n\ndata(\"cps2\", package = \"PoEdata\")       # load CPS dataset\nm &lt;- cps2[cps2$metro == 1, ]            # subset: people living in metropolitan areas\nr &lt;- cps2[cps2$metro == 0, ]            # subset: people NOT living in metropolitan areas\n\nwg1 &lt;- lm(wage ~ educ + exper, data = m)           # regression for metro group\nwg0 &lt;- lm(wage ~ educ + exper, data = r)           # regression for non-metro group\n\ndf1 &lt;- wg1$df.residual                   # residual degrees of freedom (metro model)\ndf0 &lt;- wg0$df.residual                   # residual degrees of freedom (non-metro model)\n\nsig1 &lt;- summary(wg1)$sigma^2             # estimated error variance metro\nsig0 &lt;- summary(wg0)$sigma^2             # estimated error variance non-metro\n\nfstat &lt;- sig1 / sig0                     # F-statistic: ratio of variances\n\nFlc &lt;- qf(alpha/2, df1, df0)             # lower critical value from F distribution\nFuc &lt;- qf(1 - alpha/2, df1, df0)         # upper critical value (two-tail test)\n\nc(F = fstat, Flc = Flc, Fuc = Fuc)       # display F statistic and critical bounds\n\n        F       Flc       Fuc \n2.0877623 0.8051984 1.2617297 \n\n\n\n\n\n\n\n\nInterpreting the Goldfeldâ€“Quandt test\n\n\n\nDecision rule (two-tailed test): - If \\(F &lt;\\) lower critical value OR \\(F &gt;\\) upper critical value: reject \\(H_0\\) â†’ variances differ - If \\(F\\) lies between the two critical values: fail to reject \\(H_0\\)\nDecision rule (one-tailed test): - If \\(F &gt;\\) upper critical value (or p-value &lt; 0.05): reject \\(H_0\\) â†’ variance in group 1 is larger\nIn this metro vs.Â rural example, a significant F-statistic suggests systematic differences in wage variance between regions.\n\n\nGoldfeldâ€“Quandt without an indicator (split by median income):\n\nalpha &lt;- 0.05                                   # significance level (5%)\n\nmedianincome &lt;- median(food$income)             # compute median income\n\n# Split sample into low-income and high-income groups\nli &lt;- food[food$income &lt;= medianincome, ]       # low-income group (&lt;= median)\nhi &lt;- food[food$income &gt;= medianincome, ]       # high-income group (&gt;= median)\n\n# Fit the same regression separately in each subgroup\neqli &lt;- lm(food_exp ~ income, data = li)        # model for low-income group\neqhi &lt;- lm(food_exp ~ income, data = hi)        # model for high-income group\n\n# Extract degrees of freedom and residual variances\ndfli &lt;- eqli$df.residual; dfhi &lt;- eqhi$df.residual   # residual degrees of freedom\nsqli &lt;- summary(eqli)$sigma^2; sqhi &lt;- summary(eqhi)$sigma^2  # estimated error variances\n\n# F test comparing variances (heteroskedasticity between groups)\nfstat &lt;- sqhi / sqli                            # F statistic: ratio of variances\nFc    &lt;- qf(1 - alpha, dfhi, dfli)              # critical value (right-tail test)\npval  &lt;- 1 - pf(fstat, dfhi, dfli)              # p-value\n\nc(F = fstat, Fcrit = Fc, p.value = pval)        # display test statistic, critical value, and p-value\n\n         F      Fcrit    p.value \n3.61475572 2.21719713 0.00459643 \n\n\n\n\n\n\n\n\nInterpreting the GQ test (one-tailed)\n\n\n\nDecision rule: - If p-value &lt; 0.05: reject \\(H_0\\) â†’ variance increases with income - If F &gt; critical value: reject \\(H_0\\)\nThis version tests whether high-income households have more variable food expenditure than low-income households.\n\n\nOr use gqtest() directly:\n\nfoodeq &lt;- lm(food_exp ~ income, data = food)          # baseline OLS\n\n# Goldfeldâ€“Quandt test for heteroskedasticity:\n# - order.by = food$income: sort observations by income\n# - point = 0.5: split the sample around the middle (drops central portion)\n# - alternative = \"greater\": tests whether variance is larger in the upper group\ntst &lt;- lmtest::gqtest(foodeq,\n                      point = 0.5,\n                      alternative = \"greater\",\n                      order.by = food$income)\n# tidy + nicely format test output\nkable(tidy(tst),                              \n      caption = \"R function `gqtest()` with the 'food' equation\")\n\nMultiple parameters; naming those columns df1 and df2.\n\n\n\nR function gqtest() with the â€˜foodâ€™ equation\n\n\n\n\n\n\n\n\n\n\ndf1\ndf2\nstatistic\np.value\nmethod\nalternative\n\n\n\n\n18\n18\n3.614756\n0.0045964\nGoldfeld-Quandt test\nvariance increases from segment 1 to 2\n\n\n\n\n\n\n\n\n\n\n\nInterpreting gqtest() output\n\n\n\nThe function reports: - statistic: F-ratio comparing variances - p.value: one-tailed (tests â€œgreaterâ€ alternative by default) - parameter: degrees of freedom for numerator and denominator\nDecision: If p-value &lt; 0.05, conclude that variance is significantly larger in the upper group (heteroskedasticity present).\nNote: point = 0.5 means observations are split around the median. Adjust this parameter if you want to drop more/fewer central observations.\n\n\n\n\nHeteroskedasticityâ€‘consistent (HC) standard errors\nHeteroskedasticity-consistent (robust) standard errors When heteroskedasticity is detected, one solution is to use robust standard errors (also called White standard errors or HC standard errors). These adjust the variance-covariance matrix to account for non-constant variance without changing the coefficient estimates.\nKey types:\nHC0: Whiteâ€™s original estimator HC1: degrees-of-freedom adjustment (recommended for small samples) HC2, HC3: further refinements for leverage and influence\nThe OLS point estimates remain unchangedâ€”only the standard errors (and thus t-statistics and p-values) are corrected.\n\nfoodeq &lt;- lm(food_exp ~ income, data = food)          # baseline regression\nkable(tidy(foodeq),                                   # show coefficients with regular SEs\n      caption = \"Regular standard errors in the 'food' equation\")\n\n\nRegular standard errors in the â€˜foodâ€™ equation\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n83.41600\n43.410163\n1.921578\n0.0621824\n\n\nincome\n10.20964\n2.093263\n4.877381\n0.0000195\n\n\n\n\ncov1 &lt;- car::hccm(foodeq, type = \"hc1\")               # compute robust (HC1) covariance matrix\nfood.HC1 &lt;- lmtest::coeftest(foodeq, vcov. = cov1)    # apply robust SEs to regression output\n\nkable(tidy(food.HC1),                                 # show coefficients with robust SEs\n      caption = \"Robust (HC1) standard errors in the 'food' equation\")\n\n\nRobust (HC1) standard errors in the â€˜foodâ€™ equation\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n83.41600\n27.463748\n3.037313\n0.0042989\n\n\nincome\n10.20964\n1.809077\n5.643566\n0.0000018\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nRobust (HC) errors fix inference under heteroskedasticity, but the point estimates are unchanged.\n\n\n\n\n\nSummary\n\nYou applied the technique in R, produced tidy outputs, and interpreted them in context.\nKeep focusing on clean tables, explicit assumptions, and diagnostics that justify your specification.\nUse the provided patterns as a template for future empirical work.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Supervision 4</span>"
    ]
  },
  {
    "objectID": "supervision_4.html#lab-16-serial-correlation-autocorrelation",
    "href": "supervision_4.html#lab-16-serial-correlation-autocorrelation",
    "title": "Supervision 4",
    "section": "Lab 16 â€” Serial Correlation (Autocorrelation)",
    "text": "Lab 16 â€” Serial Correlation (Autocorrelation)\n\nğŸ¯ Learning outcomes\n\nUnderstand the core ideas and assumptions for this labâ€™s topic.\nImplement the workflow in R and create tidy outputs with clear captions.\nInterpret results in context and connect diagnostics to modeling decisions.\nRecognise common pitfalls and perform basic robustness checks.\n\n\n\n\nğŸ§° Prerequisites\n\nMultiple regression and inference (t-tests, F-tests).\nR and RStudio installed.\nPackages: broom, knitr, and any other packages used in the lab code.\n\n\n\n\nOverview\nSerial correlation occurs when regression errors are correlated across time periods. In a time series regression, if \\(e_t\\) is correlated with \\(e_{t-1}\\)â€‹, \\(e_{t-2}\\)â€‹, etc., we have serial correlation.\nSerial correlation is correlation across time in a series. In regression, autocorrelated errors lead to incorrect standard errors (and misleading t-tests). We may incorrectly conclude that coefficients are statistically significant.\nCommon causes:\n\nOmitted variables that vary systematically over time\nModel misspecification (wrong functional form)\nInertia or persistence in economic variables (e.g., GDP growth tends to persist)\n\nThe fix: Use HAC (heteroskedasticity and autocorrelation consistent) standard errors, or model the autocorrelation structure explicitly (e.g., AR models, dynamic specifications).\nTo diagnose whether growth is autocorrelated, we create scatter plots comparing growth at time t with growth at time \\(t+1\\) (lag 1) and \\(t+2\\) (lag 2).\nWhat to look for:\nPositive correlation (upward diagonal pattern) â†’ high growth tends to follow high growth Negative correlation (downward diagonal pattern) â†’ high growth followed by low growth Random scatter â†’ no autocorrelation\nThe reference lines divide the plot at the means, creating four quadrants. With positive autocorrelation, most points fall in the upper-right and lower-left quadrants.\n\n\nExample: growth vs unemployment (Okunâ€™s data)\nThe okun dataset contains quarterly US data on:\ng: Real GDP growth rate (percentage change) u: Unemployment rate\nOkunâ€™s Law describes the inverse relationship between GDP growth and changes in unemployment. Before running regressions with time series data, we should check whether the series themselves exhibit autocorrelation.\n\ndata(\"okun\", package = \"PoEdata\")     # load Okun's law quarterly dataset\nokun.ts &lt;- ts(okun, start = c(1948, 1), frequency = 4)  # declare the data 'quarterly' time-series object\n\n# Plot time series GDP%\nplot(okun.ts[, \"g\"], ylab = \"growth\")         # real GDP growth over time\n\n\n\n\n\n\n\nplot(okun.ts[, \"u\"], ylab = \"unemployment\")   # unemployment rate over time\n\n\n\n\n\n\n\n# Scatter with lags/leads\nggL1 &lt;- data.frame(\n  g   = okun.ts[, \"g\"],\n  gL1 = stats::lag(okun.ts[, \"g\"], -1)        # lead by 1 quarter (stats::lag with -1 shifts forward)\n)\nplot(ggL1)                                    # scatter of g vs. g(+1)\nabline(h = mean(ggL1$gL1, na.rm = TRUE),      # horizontal line at mean of g(+1)\n       v = mean(ggL1$g,   na.rm = TRUE), lty = 2)       # vertical line at mean of g\n\n\n\n\n\n\n\nggL2 &lt;- data.frame(\n  g   = okun.ts[, \"g\"],\n  gL2 = stats::lag(okun.ts[, \"g\"], -2)        # lead by 2 quarters\n)\nplot(ggL2)                                    # scatter of g vs. g(+2)\nabline(h = mean(ggL2$gL2, na.rm = TRUE),\n       v = mean(ggL2$g,   na.rm = TRUE), lty = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting lag scatter plots\n\n\n\nIn the lag-1 plot (\\(g\\) vs \\(g_{t+1}\\)): - If points cluster along an upward-sloping diagonal, growth exhibits positive autocorrelation - Most points in upper-right and lower-left quadrants â†’ when growth is high today, it tends to be high next quarter - This pattern suggests persistence or momentum in GDP growth\nIn the lag-2 plot (\\(g\\) vs \\(g_{t+2}\\)): - A weaker diagonal pattern suggests autocorrelation diminishes at longer lags - If points become more randomly scattered, correlation decays over time\nWhy this matters for regression: If the residuals from your regression show similar patterns, standard errors will be incorrect.\n\n\nThe Autocorrelation Function (ACF)\nThe ACF provides a formal measure of correlation at different lags:\n\\(\\rho_k = \\text{Corr}(g_t, g_{t-k}) = \\dfrac{\\text{Cov}(g_t, g_{t-k})}{\\text{Var}(g_t)}\\) \nwhere \\(k\\) is the lag (1, 2, 3, â€¦).\n\nHow to read the correlogram:\n\nEach vertical bar shows \\(\\hat{\\rho}_k\\) (the estimated correlation at lag k).\nBlue dashed lines: 95% confidence bounds (approximately \\(\\pm \\dfrac{2}{\\sqrt{n}}\\)).\nBars outside the bounds â†’ statistically significant autocorrelation.\nBars inside the bounds â†’ not significantly different from zero.\n\nBars outside the bounds â†’ statistically significant autocorrelation Bars inside the bounds â†’ not significantly different from zero\n\n# Correlogram (autocorrelation function) for growth\nacf(okun.ts[, \"g\"])\n\n\n\n\n\n\n\n\n\n\nPhillips curve example and BG/DW tests\n\nlibrary(dynlm)                                 # dynamic linear models for time series\n\nWarning: package 'dynlm' was built under R version 4.4.3\n\ndata(\"phillips_aus\", package = \"PoEdata\")      # Australian Phillips-curve data\nphill.ts &lt;- ts(phillips_aus, start = c(1987, 1), end = c(2009, 3), frequency = 4)  # quarterly ts\ninflation &lt;- phill.ts[, \"inf\"]                 # inflation series\nDu &lt;- diff(phill.ts[, \"u\"])                    # change in unemployment (âˆ†u_t)\n\n# FDL model: inf_t = Î²1 + Î²2 * âˆ†u_t + e_t\nphill.dyn &lt;- dynlm(inf ~ diff(u), data = phill.ts)                 # regress inflation on âˆ†unemployment\nkable(tidy(phill.dyn), caption = \"Summary of the `phillips` model\")# tidy+format regression output\n\nWarning: The `tidy()` method for objects of class `dynlm` is not maintained by the broom team, and is only supported through the `lm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n\n\n\nSummary of the phillips model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.7776213\n0.0658249\n11.813474\n0.0000000\n\n\ndiff(u)\n-0.5278638\n0.2294049\n-2.301014\n0.0237539\n\n\n\n\n# Residual plot and correlogram (check for serial correlation)\nehat &lt;- resid(phill.dyn)                       # regression residuals\nplot(ehat); abline(h = 0, lty = 2)             # residuals over time, zero reference\n\n\n\n\n\n\n\nacf(ehat)                                      # ACF of residuals\n\n\n\n\n\n\n\n\nBreuschâ€“Godfrey tests (different orders/statistics):\n\n# NOTE: preserving your object names a, b, c, d and test types.\n# (We only use base::c() below to avoid masking by object 'c')\n# type = \"F\" can be preferable in small samples (finite-sample adjustment)\n\na &lt;- lmtest::bgtest(phill.dyn, order = 1, type = \"F\",     fill = 0)   # BG test AR(1); replace initial NAs by 0\nb &lt;- lmtest::bgtest(phill.dyn, order = 1, type = \"F\",     fill = NA)  # BG test AR(1); drop obs with initial NAs\nc &lt;- lmtest::bgtest(phill.dyn, order = 4, type = \"Chisq\", fill = 0)   # BG test AR(4); large-sample chi-square; fill 0\nd &lt;- lmtest::bgtest(phill.dyn, order = 4, type = \"Chisq\", fill = NA)  # BG test AR(4); drop initial NAs\n\n# Collect results into a neat table\ndfr &lt;- data.frame(\n  Method     = base::c(\"1, F, 0\", \"1, F, NA\", \"4, Chisq, 0\", \"4, Chisq, NA\"),\n  Statistic  = base::c(unname(a$statistic), unname(b$statistic),\n                       unname(c$statistic), unname(d$statistic)),      # test statistic values\n  Parameters = base::c(sprintf(\"df1=%g, df2=%g\", a$parameter[1], a$parameter[2]),\n                       sprintf(\"df1=%g, df2=%g\", b$parameter[1], b$parameter[2]),\n                       sprintf(\"df=%g\",          c$parameter),          # chi-square df\n                       sprintf(\"df=%g\",          d$parameter)),\n  p_value    = base::c(a$p.value, b$p.value, c$p.value, d$p.value)     # p-values\n)\n\nknitr::kable(dfr, caption = \"Breuschâ€“Godfrey test for the Phillips example.\")  # nicely formatted summary\n\n\nBreuschâ€“Godfrey test for the Phillips example.\n\n\nMethod\nStatistic\nParameters\np_value\n\n\n\n\n1, F, 0\n38.46538\ndf1=1, df2=87\n0e+00\n\n\n1, F, NA\n38.69456\ndf1=1, df2=86\n0e+00\n\n\n4, Chisq, 0\n36.67190\ndf=4\n2e-07\n\n\n4, Chisq, NA\n33.59372\ndf=4\n9e-07\n\n\n\n\n\nDurbinâ€“Watson (DW targets AR(1) and isnâ€™t valid with a lagged dependent variable(prefer Breuschâ€“Godfrey for more general cases):\n\nlmtest::dwtest(phill.dyn)   # Durbinâ€“Watson test for AR(1) autocorrelation in residuals (H0: no autocorrelation)\n\n\n    Durbin-Watson test\n\ndata:  phill.dyn\nDW = 0.88729, p-value = 2.198e-09\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\n\n\nSummary\n\nYou implemented the method in R and produced clean, wellâ€‘captioned outputs.\nDiagnostics and assumptions inform how you specify and interpret the model.\nUse these patterns as a template for future empirical work.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Supervision 4</span>"
    ]
  },
  {
    "objectID": "supervision_4.html#lab-17-hac-neweywest-standard-errors",
    "href": "supervision_4.html#lab-17-hac-neweywest-standard-errors",
    "title": "Supervision 4",
    "section": "Lab 17 â€” HAC (Neweyâ€“West) Standard Errors",
    "text": "Lab 17 â€” HAC (Neweyâ€“West) Standard Errors\n\nğŸ¯ Learning outcomes\n\nGrasp the core concept(s) of this lab and where they fit in the modelling workflow.\nImplement the R workflow cleanly, producing tidy outputs with clear captions.\nInterpret results carefully and connect diagnostics/assumptions to modelling choices.\nIdentify pitfalls and perform simple robustness checks.\n\n\n\n\nğŸ§° Prerequisites\n\nMultiple regression, inference (t-tests, F-tests), and model diagnostics basics.\nR and RStudio installed.\nPackages: broom, knitr, plus any additional packages used in this lab.\n\n\n\n\nOverview\nThis lab introduces the concept, explains why it matters for inference and decisionâ€‘making, and demonstrates implementation in R. You will interpret results in context and motivate specification choices using diagnostics.\n\nCorrect standard errors under autocorrelation (and heteroskedasticity) using HAC estimators.\n\nlibrary(sandwich); library(lmtest)                        # robust VCOVs + testing tools\n\ns0 &lt;- coeftest(phill.dyn)                                 # OLS SEs (incorrect if residuals have autocorrelation)\ns1 &lt;- coeftest(phill.dyn, vcov. = vcovHAC(phill.dyn))     # HAC (heteroskedasticity & autocorrelation consistent)\ns2 &lt;- coeftest(phill.dyn, vcov. = NeweyWest(phill.dyn))   # Neweyâ€“West (a common HAC estimator)\ns3 &lt;- coeftest(phill.dyn, vcov. = kernHAC(phill.dyn))     # kernel-based HAC (Andrews-type)\n\n# Collect just the (Std. Error, Pr(&gt;|t|)) rows/cols and compare across methods\ntbl &lt;- data.frame(cbind(s0[c(3, 4)], s1[c(3, 4)], s2[c(3, 4)], s3[c(3, 4)]))\nnames(tbl) &lt;- c(\"Incorrect\", \"vcovHAC\", \"NeweyWest\", \"kernHAC\")\nrow.names(tbl) &lt;- c(\"(Intercept)\", \"Du\")\n\nkable(tbl, digits = 3, caption = \"Comparing standard errors for the Phillips model.\")\n\n\nComparing standard errors for the Phillips model.\n\n\n\nIncorrect\nvcovHAC\nNeweyWest\nkernHAC\n\n\n\n\n(Intercept)\n0.066\n0.095\n0.128\n0.131\n\n\nDu\n0.229\n0.304\n0.331\n0.335\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nHAC fixes inference but not efficiency: OLS with HAC is still not minimumâ€‘variance when errors are autocorrelated.\n\n\n\n\n\nSummary\n\nYou implemented the method in R, produced clean outputs, and interpreted them in context.\nDiagnostics/assumptions inform specification choices and robustness checks.\nReuse this structure as a template for future empirical work.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Supervision 4</span>"
    ]
  },
  {
    "objectID": "supervision_5.html",
    "href": "supervision_5.html",
    "title": "Supervision 5",
    "section": "",
    "text": "Lab 18 - Hedonic Price Modelling: The Golden Regression Challenge",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Supervision 5</span>"
    ]
  },
  {
    "objectID": "supervision_5.html#lab-18---hedonic-price-modelling-the-golden-regression-challenge",
    "href": "supervision_5.html#lab-18---hedonic-price-modelling-the-golden-regression-challenge",
    "title": "Supervision 5",
    "section": "",
    "text": "This supervision is live now. Students have several days to complete the task.\n\n\nğŸ¯ Learning Outcomes\nBy the end of this supervision, you should be able to:\n\nApply the concept of hedonic pricing to a real dataset.\n\nDevelop and justify a regression model explaining variation in house prices.\n\nEvaluate competing model specifications and defend modelling choices.\n\nCommunicate results clearly to peers and engage in critical discussion.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Supervision 5</span>"
    ]
  },
  {
    "objectID": "supervision_5.html#part-a-data-preparation",
    "href": "supervision_5.html#part-a-data-preparation",
    "title": "Supervision 5",
    "section": "Part A â€” Data Preparation",
    "text": "Part A â€” Data Preparation\nIn this supervision, we will use the Ames Housing dataset (De Cock, 2011), a real dataset widely used in econometrics and data science to study housing markets. It contains information on residential properties in Ames, Iowa, USA, including price, structural features, and neighbourhood characteristics.\nRun the following once to produce hedonic_ames.csv for your analysis.\n\n# Install if you have NOT done so.\n# Installl de following package once\n# install.packages(\"AmesHousing\")\n# install.packages(\"dplyr\")\n# install.packages(\"readr\")\n# install.packages(\"janitor\")\n# install.packages(\"stringr\")\n\nlibrary(AmesHousing)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(janitor)\nlibrary(stringr)\n\n# Load full Ames data\names &lt;- AmesHousing::make_ames()\n\n# Minimal cleaning and engineered features\ndf &lt;- ames %&gt;%\n  janitor::clean_names() %&gt;%\n  dplyr::transmute(\n    sale_price,\n    gr_liv_area, total_bsmt_sf, lot_area,\n    bedroom_abv_gr, full_bath, half_bath, kitchen_abv_gr,\n    overall_qual, overall_cond, year_built, year_remod_add,\n    exter_qual, kitchen_qual, central_air,\n    garage_cars, garage_area, fireplace_qu,\n    bldg_type, house_style, neighborhood,\n    ms_zoning, foundation\n  ) %&gt;%\n  dplyr::mutate(\n    central_air = factor(central_air, levels = c(\"N\",\"Y\")),\n    exter_qual = factor(exter_qual, ordered = TRUE),\n    kitchen_qual = factor(kitchen_qual, ordered = TRUE),\n    fireplace_qu = factor(fireplace_qu, ordered = TRUE),\n    bldg_type = factor(bldg_type),\n    house_style = factor(house_style),\n    neighborhood = factor(neighborhood),\n    ms_zoning = factor(ms_zoning),\n    foundation = factor(foundation),\n    baths_total = full_bath + 0.5 * half_bath,\n    age = pmax(0, 2010 - year_built),\n    remod_age = pmax(0, 2010 - year_remod_add)\n  ) %&gt;%\n  dplyr::filter(\n    sale_price &gt; 20000, sale_price &lt; 600000,\n    gr_liv_area &gt; 300, lot_area &lt; 100000\n  ) %&gt;%\n  tidyr::drop_na(sale_price, gr_liv_area, bedroom_abv_gr, baths_total, overall_qual)\n\nset.seed(42)\ndf &lt;- df %&gt;% dplyr::mutate(listing_id = dplyr::row_number()) %&gt;% dplyr::relocate(listing_id)\n\nreadr::write_csv(df, \"hedonic_ames.csv\")\ndplyr::glimpse(df)\n\nRows: 2,918\nColumns: 27\n$ listing_id     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, â€¦\n$ sale_price     &lt;int&gt; 215000, 105000, 172000, 244000, 189900, 195500, 213500,â€¦\n$ gr_liv_area    &lt;int&gt; 1656, 896, 1329, 2110, 1629, 1604, 1338, 1280, 1616, 18â€¦\n$ total_bsmt_sf  &lt;dbl&gt; 1080, 882, 1329, 2110, 928, 926, 1338, 1280, 1595, 994,â€¦\n$ lot_area       &lt;int&gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005, 53â€¦\n$ bedroom_abv_gr &lt;int&gt; 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 1, 4, 4, 1, 2â€¦\n$ full_bath      &lt;int&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, 1, 1â€¦\n$ half_bath      &lt;int&gt; 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0â€¦\n$ kitchen_abv_gr &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1â€¦\n$ overall_qual   &lt;fct&gt; Above_Average, Average, Above_Average, Good, Average, Aâ€¦\n$ overall_cond   &lt;fct&gt; Average, Above_Average, Above_Average, Average, Averageâ€¦\n$ year_built     &lt;int&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 1995, 1â€¦\n$ year_remod_add &lt;int&gt; 1960, 1961, 1958, 1968, 1998, 1998, 2001, 1992, 1996, 1â€¦\n$ exter_qual     &lt;ord&gt; Typical, Typical, Typical, Good, Typical, Typical, Goodâ€¦\n$ kitchen_qual   &lt;ord&gt; Typical, Typical, Good, Excellent, Typical, Good, Good,â€¦\n$ central_air    &lt;fct&gt; Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Yâ€¦\n$ garage_cars    &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2â€¦\n$ garage_area    &lt;dbl&gt; 528, 730, 312, 522, 482, 470, 582, 506, 608, 442, 440, â€¦\n$ fireplace_qu   &lt;ord&gt; Good, No_Fireplace, No_Fireplace, Typical, Typical, Gooâ€¦\n$ bldg_type      &lt;fct&gt; OneFam, OneFam, OneFam, OneFam, OneFam, OneFam, TwnhsE,â€¦\n$ house_style    &lt;fct&gt; One_Story, One_Story, One_Story, One_Story, Two_Story, â€¦\n$ neighborhood   &lt;fct&gt; North_Ames, North_Ames, North_Ames, North_Ames, Gilbertâ€¦\n$ ms_zoning      &lt;fct&gt; Residential_Low_Density, Residential_High_Density, Resiâ€¦\n$ foundation     &lt;fct&gt; CBlock, CBlock, CBlock, CBlock, PConc, PConc, PConc, PCâ€¦\n$ baths_total    &lt;dbl&gt; 1.0, 1.0, 1.5, 2.5, 2.5, 2.5, 2.0, 2.0, 2.0, 2.5, 2.5, â€¦\n$ age            &lt;dbl&gt; 50, 49, 52, 42, 13, 12, 9, 18, 15, 11, 17, 18, 12, 20, â€¦\n$ remod_age      &lt;dbl&gt; 50, 49, 52, 42, 12, 12, 9, 18, 14, 11, 16, 3, 12, 20, 2â€¦\n\n\n\n\nPart B â€” Task\n\nBuild Your Hedonic Model\nUsing hedonic_ames.csv, specify, estimate, and justify an OLS hedonic model explaining sale_price.\nYou may transform variables (e.g., log(sale_price), log(gr_liv_area)), include interactions, and compare alternative specifications.\nAim for theoretical consistency, parsimony, and interpretability.\n\n\nStarter code (illustrative; replace with your own specification)\n\n# Installl de following package once\n# install.packages(c(\"readr\",\"dplyr\",\"broom\",\"car\",\"tseries\"))\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(broom)\nlibrary(car)\nlibrary(lmtest)\nlibrary(tseries)\nlibrary(sandwich)\nlibrary(modelsummary)\n\n# 1. Load data & transform\ndf &lt;- readr::read_csv(\"hedonic_ames.csv\")\n\n# ---------------------------------------------------\n# 1) Load & transform\n# ---------------------------------------------------\ndf &lt;- read_csv(\"hedonic_ames.csv\") %&gt;%\n  mutate(\n    ln_price = log(sale_price),\n    ln_area  = log(gr_liv_area),\n    overall_qual = factor(overall_qual, ordered = TRUE),\n    overall_cond = factor(overall_cond, ordered = TRUE),\n    exter_qual   = factor(exter_qual, ordered = TRUE),\n    kitchen_qual = factor(kitchen_qual, ordered = TRUE),\n    fireplace_qu = factor(fireplace_qu, ordered = TRUE),\n    bldg_type    = factor(bldg_type),\n    house_style  = factor(house_style),\n    neighborhood = factor(neighborhood),\n    baths_total  = full_bath + 0.5 * half_bath\n  )\n\n# ---------------------------------------------------\n# 2) Run regression model\n# ---------------------------------------------------\nm0 &lt;- lm(\n  ln_price ~ ln_area + bedroom_abv_gr + baths_total + \n  total_bsmt_sf + garage_cars + central_air + overall_qual +\n  overall_cond + exter_qual + kitchen_qual + fireplace_qu +\n  bldg_type + house_style + neighborhood,\n  data = df\n)\n\n# ---------------------------------------------------\n# 3) DIAGNOSTICS (same tools/style as Supervision 4)\n# ---------------------------------------------------\nlibrary(car)\nlibrary(lmtest)\n\n# (A) Multicollinearity â€“ VIF\nvif_raw &lt;- car::vif(m0)\n# If factors present, car::vif returns GVIF & Df; convert to VIF-equivalent:\nif (is.matrix(vif_raw)) {\n  vif_values &lt;- vif_raw[, \"GVIF\"]^(1/(2 * vif_raw[, \"Df\"]))\n  names(vif_values) &lt;- rownames(vif_raw)\n} else {\n  vif_values &lt;- vif_raw\n}\nvif_mean &lt;- mean(vif_values, na.rm = TRUE)\nvif_max  &lt;- max(vif_values, na.rm = TRUE)\n\n# As this is a cross-sectional dataset we will not include time-series correlation.\n# Autocorrelation â€“ Breuschâ€“Godfrey (lag 4)\n# bg_test &lt;- lmtest::bgtest(m0, order = 4)\n\n# (B) Heteroskedasticity â€“ White and Goldfeldâ€“Quandt\nwhite_test &lt;- lmtest::bptest(m0, varformula = ~ fitted(m0) + I(fitted(m0)^2))\ngq_test    &lt;- lmtest::gqtest(m0, order.by = ~ ln_area, data = df, fraction = 2/3)\n\n# ---------------------------------------------------\n# 4) Coefficient table with HC1 robust SEs (cross-section)\n# ---------------------------------------------------\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(broom)\nlibrary(knitr)\n\nct_obj &lt;- lmtest::coeftest(m0, vcov. = sandwich::vcovHC(m0, type = \"HC1\"))\nct_df  &lt;- broom::tidy(ct_obj)  # safe data.frame for kable\n\nknitr::kable(\n  ct_df,\n  digits   = 3,\n  col.names = c(\"Term\", \"Estimate\", \"Std. Error\", \"Statistic\", \"Pr(&gt;|t|)\"),\n  caption  = \"OLS with HC1 (heteroskedasticity-robust) standard errors\"\n)\n\n\nOLS with HC1 (heteroskedasticity-robust) standard errors\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nStd. Error\nStatistic\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n8.466\n0.147\n57.436\n0.000\n\n\nln_area\n0.422\n0.021\n20.156\n0.000\n\n\nbedroom_abv_gr\n-0.016\n0.006\n-2.695\n0.007\n\n\nbaths_total\n0.043\n0.008\n5.321\n0.000\n\n\ntotal_bsmt_sf\n0.000\n0.000\n3.104\n0.002\n\n\ngarage_cars\n0.053\n0.005\n9.759\n0.000\n\n\ncentral_airY\n0.086\n0.016\n5.339\n0.000\n\n\noverall_qual.L\n-0.016\n0.056\n-0.286\n0.775\n\n\noverall_qual.Q\n-0.033\n0.048\n-0.683\n0.494\n\n\noverall_qual.C\n-0.073\n0.049\n-1.503\n0.133\n\n\noverall_qual^4\n-0.146\n0.041\n-3.547\n0.000\n\n\noverall_qual^5\n-0.259\n0.037\n-7.025\n0.000\n\n\noverall_qual^6\n-0.010\n0.039\n-0.249\n0.803\n\n\noverall_qual^7\n0.126\n0.044\n2.903\n0.004\n\n\noverall_qual^8\n-0.032\n0.038\n-0.840\n0.401\n\n\noverall_qual^9\n0.347\n0.037\n9.318\n0.000\n\n\noverall_cond.L\n-0.222\n0.043\n-5.146\n0.000\n\n\noverall_cond.Q\n-0.102\n0.042\n-2.407\n0.016\n\n\noverall_cond.C\n-0.133\n0.042\n-3.138\n0.002\n\n\noverall_cond^4\n-0.136\n0.032\n-4.219\n0.000\n\n\noverall_cond^5\n-0.162\n0.021\n-7.636\n0.000\n\n\noverall_cond^6\n-0.096\n0.035\n-2.759\n0.006\n\n\noverall_cond^7\n-0.082\n0.031\n-2.630\n0.009\n\n\noverall_cond^8\n-0.325\n0.030\n-10.995\n0.000\n\n\nexter_qual.L\n-0.033\n0.018\n-1.835\n0.067\n\n\nexter_qual.Q\n0.031\n0.023\n1.323\n0.186\n\n\nexter_qual.C\n-0.041\n0.029\n-1.434\n0.152\n\n\nkitchen_qual.L\n-0.030\n0.018\n-1.662\n0.097\n\n\nkitchen_qual.Q\n0.013\n0.016\n0.846\n0.398\n\n\nkitchen_qual.C\n-0.105\n0.031\n-3.413\n0.001\n\n\nkitchen_qual^4\n0.009\n0.025\n0.368\n0.713\n\n\nfireplace_qu.L\n-0.035\n0.017\n-2.115\n0.035\n\n\nfireplace_qu.Q\n0.040\n0.014\n2.858\n0.004\n\n\nfireplace_qu.C\n0.002\n0.015\n0.145\n0.885\n\n\nfireplace_qu^4\n-0.005\n0.015\n-0.361\n0.718\n\n\nfireplace_qu^5\n-0.023\n0.009\n-2.662\n0.008\n\n\nbldg_typeOneFam\n0.118\n0.018\n6.592\n0.000\n\n\nbldg_typeTwnhs\n-0.018\n0.025\n-0.731\n0.465\n\n\nbldg_typeTwnhsE\n0.048\n0.023\n2.077\n0.038\n\n\nbldg_typeTwoFmCon\n0.112\n0.026\n4.308\n0.000\n\n\nhouse_styleOne_and_Half_Unf\n0.032\n0.026\n1.216\n0.224\n\n\nhouse_styleOne_Story\n0.066\n0.014\n4.734\n0.000\n\n\nhouse_styleSFoyer\n0.166\n0.018\n9.082\n0.000\n\n\nhouse_styleSLvl\n0.072\n0.015\n4.699\n0.000\n\n\nhouse_styleTwo_and_Half_Fin\n0.032\n0.071\n0.442\n0.659\n\n\nhouse_styleTwo_and_Half_Unf\n0.034\n0.033\n1.049\n0.294\n\n\nhouse_styleTwo_Story\n0.020\n0.015\n1.351\n0.177\n\n\nneighborhoodBlueste\n-0.025\n0.046\n-0.548\n0.584\n\n\nneighborhoodBriardale\n-0.102\n0.030\n-3.432\n0.001\n\n\nneighborhoodBrookside\n-0.120\n0.031\n-3.843\n0.000\n\n\nneighborhoodClear_Creek\n0.067\n0.033\n2.072\n0.038\n\n\nneighborhoodCollege_Creek\n0.030\n0.023\n1.286\n0.198\n\n\nneighborhoodCrawford\n0.058\n0.029\n1.984\n0.047\n\n\nneighborhoodEdwards\n-0.101\n0.033\n-3.071\n0.002\n\n\nneighborhoodGilbert\n-0.003\n0.025\n-0.109\n0.913\n\n\nneighborhoodGreen_Hills\n0.511\n0.051\n10.052\n0.000\n\n\nneighborhoodGreens\n0.102\n0.037\n2.752\n0.006\n\n\nneighborhoodIowa_DOT_and_Rail_Road\n-0.215\n0.035\n-6.174\n0.000\n\n\nneighborhoodLandmark\n0.026\n0.027\n0.971\n0.331\n\n\nneighborhoodMeadow_Village\n-0.134\n0.031\n-4.298\n0.000\n\n\nneighborhoodMitchell\n-0.007\n0.026\n-0.264\n0.792\n\n\nneighborhoodNorth_Ames\n-0.058\n0.026\n-2.267\n0.023\n\n\nneighborhoodNorthpark_Villa\n-0.046\n0.027\n-1.697\n0.090\n\n\nneighborhoodNorthridge\n0.117\n0.026\n4.478\n0.000\n\n\nneighborhoodNorthridge_Heights\n0.112\n0.026\n4.334\n0.000\n\n\nneighborhoodNorthwest_Ames\n-0.062\n0.026\n-2.381\n0.017\n\n\nneighborhoodOld_Town\n-0.203\n0.028\n-7.282\n0.000\n\n\nneighborhoodSawyer\n-0.045\n0.028\n-1.607\n0.108\n\n\nneighborhoodSawyer_West\n-0.018\n0.026\n-0.717\n0.474\n\n\nneighborhoodSomerset\n0.074\n0.023\n3.243\n0.001\n\n\nneighborhoodSouth_and_West_of_Iowa_State_University\n-0.094\n0.035\n-2.711\n0.007\n\n\nneighborhoodStone_Brook\n0.151\n0.033\n4.575\n0.000\n\n\nneighborhoodTimberland\n0.047\n0.027\n1.776\n0.076\n\n\nneighborhoodVeenker\n0.035\n0.046\n0.776\n0.438\n\n\n\n\n# ---------------------------------------------------\n# 5) Diagnostics summary table (simple & readable)\n#     (No autocorrelation; cross-sectional focus)\n# ---------------------------------------------------\ndiag_tab &lt;- data.frame(\n  Diagnostic = c(\n    \"Mean VIF\",\n    \"Max VIF\",\n    \"White test p-value\",\n    \"Goldfeldâ€“Quandt p-value\"\n  ),\n  Result = c(\n    round(vif_mean, 3),\n    round(vif_max, 3),\n    signif(white_test$p.value, 3),\n    signif(gq_test$p.value, 3)\n  ),\n  check.names = FALSE\n)\n\nknitr::kable(\n  diag_tab,\n  digits  = 3,\n  caption = \"Diagnostics: multicollinearity & heteroskedasticity\"\n)\n\n\nDiagnostics: multicollinearity & heteroskedasticity\n\n\nDiagnostic\nResult\n\n\n\n\nMean VIF\n1.418\n\n\nMax VIF\n2.471\n\n\nWhite test p-value\n0.000\n\n\nGoldfeldâ€“Quandt p-value\n0.002\n\n\n\n\n\n\nYou may create graphs in your analysis, but do not include images in the report (text and tables only).\n\n\n\n\n\nPart C â€” Competition, Criteria & Automated Assessment\n\nThe Golden Regression Challenge\nYour model competes on quality and clarity, not just raw fit.\n\n\n\nJudging Criteria (used by the automation)\n\nTheoretical consistency â€” expected signs and economic reasoning.\n\nSpecification quality â€” relevant variables, parsimony, no redundancy.\n\nInterpretability â€” meaningful coefficients, multicollinearity and heteroskedasticity controlled.\n\nDiagnostics and fit â€” AdjustedÂ RÂ², AIC, RMSE, residual checks.\n\nExplanation of results â€” clear, concise, audienceâ€‘appropriate summary.\n\n\n\nScoring (overview)\n\nModel metrics (e.g., AdjustedÂ RÂ², BIC, RMSE, VIF mean) complement the rubric.\n\nA composite score ranks entries; the winner receives the ğŸ… Golden Regression Medal.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Supervision 5</span>"
    ]
  },
  {
    "objectID": "supervision_5.html#submission",
    "href": "supervision_5.html#submission",
    "title": "Supervision 5",
    "section": "Submission",
    "text": "Submission\nYou can submit your files using the form below. If your browser or preview blocks embedded content, use the direct link provided under the form.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Supervision 5</span>"
    ]
  },
  {
    "objectID": "supervision_5.html#feedback-form",
    "href": "supervision_5.html#feedback-form",
    "title": "Supervision 5",
    "section": "Feedback form",
    "text": "Feedback form\nOpen in a new tab:\nFill out the form\n\n\n \n\nNotes\n- Do not include plots or screenshots in the report. Keep it textâ€‘only.\n- Your identity is recorded at submission.\n\nğŸ End of lab 18 ğŸ›‘ Remember to save your script ğŸ’¾",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Supervision 5</span>"
    ]
  },
  {
    "objectID": "supervision_6.html",
    "href": "supervision_6.html",
    "title": "Supervision 6",
    "section": "",
    "text": "Lab 19 â€” Qualitative Dependent Variables: Logit vs Probit on the Hedonic Model",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Supervision 6</span>"
    ]
  },
  {
    "objectID": "supervision_6.html#lab-19-qualitative-dependent-variables-logit-vs-probit-on-the-hedonic-model",
    "href": "supervision_6.html#lab-19-qualitative-dependent-variables-logit-vs-probit-on-the-hedonic-model",
    "title": "Supervision 6",
    "section": "",
    "text": "This supervision builds directly on Supervision 5. We will keep the same hedonic right-hand-side (RHS) variables, but transform the dependent variable into High vs Low Price using the sample median. You will fit logit and probit models, interpret marginal effects, and assess predictive performance.\n\n\n\nğŸ¯ Learning Outcomes\nBy the end of this lab you will be able to:\n\nTransform a continuous target into a binary target using the median.\nFit Logit and Probit models in R with the same hedonic covariates used previously.\nInterpret Average Marginal Effects (AMEs) in probability terms.\nCompare model performance via ROC curves and AUC.\nBuild confusion matrices at a chosen threshold and compute basic metrics.\n\n\nâœ”ï¸ Step-by-step workflow\n\nThe sections follow a logical sequence:\nData setup\nBinary transformation\nModel estimation\nInterpretation\nForecasting â†’ performance evaluation\n\n\n\n\nğŸ“¦ Setup & Data\nWe reuse the dataset prepared in Supervision 5 (we called the dataset df, see Sup 5 - â€œData Preparationâ€).\n\n# If a package is missing on your machine, run install.packages(\"package name\")\n# Example: install.packages(\"readr\")\n\n# Load core packages used in this lab\nlibrary(readr)     # reading data\nlibrary(dplyr)     # data manipulation\nlibrary(ggplot2)   # plotting\n\n# Optional/advanced packages for inference and diagnostics\n# If missing, install as needed:\n# install.packages(c(\"margins\", \"sandwich\", \"lmtest\", \"pROC\"))\nlibrary(margins)   # marginal effects\nlibrary(sandwich)  # robust (HC) variance\nlibrary(lmtest)    # coeftest with robust SE\n#git addinstall.packages(\"pROC\", repos = \"https://cloud.r-project.org\")\nlibrary(pROC)      # ROC and AUC\n\n# ---- Load the data using Supervision 5 - Section: data preparation\n# Adjust the path if your file is elsewhere.\n\n# Quick preview\ndplyr::glimpse(df)\n\nRows: 2,918\nColumns: 29\n$ listing_id     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, â€¦\n$ sale_price     &lt;dbl&gt; 215000, 105000, 172000, 244000, 189900, 195500, 213500,â€¦\n$ gr_liv_area    &lt;dbl&gt; 1656, 896, 1329, 2110, 1629, 1604, 1338, 1280, 1616, 18â€¦\n$ total_bsmt_sf  &lt;dbl&gt; 1080, 882, 1329, 2110, 928, 926, 1338, 1280, 1595, 994,â€¦\n$ lot_area       &lt;dbl&gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005, 53â€¦\n$ bedroom_abv_gr &lt;dbl&gt; 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 1, 4, 4, 1, 2â€¦\n$ full_bath      &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, 1, 1â€¦\n$ half_bath      &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0â€¦\n$ kitchen_abv_gr &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1â€¦\n$ overall_qual   &lt;ord&gt; Above_Average, Average, Above_Average, Good, Average, Aâ€¦\n$ overall_cond   &lt;ord&gt; Average, Above_Average, Above_Average, Average, Averageâ€¦\n$ year_built     &lt;dbl&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 1995, 1â€¦\n$ year_remod_add &lt;dbl&gt; 1960, 1961, 1958, 1968, 1998, 1998, 2001, 1992, 1996, 1â€¦\n$ exter_qual     &lt;ord&gt; Typical, Typical, Typical, Good, Typical, Typical, Goodâ€¦\n$ kitchen_qual   &lt;ord&gt; Typical, Typical, Good, Excellent, Typical, Good, Good,â€¦\n$ central_air    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", â€¦\n$ garage_cars    &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2â€¦\n$ garage_area    &lt;dbl&gt; 528, 730, 312, 522, 482, 470, 582, 506, 608, 442, 440, â€¦\n$ fireplace_qu   &lt;ord&gt; Good, No_Fireplace, No_Fireplace, Typical, Typical, Gooâ€¦\n$ bldg_type      &lt;fct&gt; OneFam, OneFam, OneFam, OneFam, OneFam, OneFam, TwnhsE,â€¦\n$ house_style    &lt;fct&gt; One_Story, One_Story, One_Story, One_Story, Two_Story, â€¦\n$ neighborhood   &lt;fct&gt; North_Ames, North_Ames, North_Ames, North_Ames, Gilbertâ€¦\n$ ms_zoning      &lt;chr&gt; \"Residential_Low_Density\", \"Residential_High_Density\", â€¦\n$ foundation     &lt;chr&gt; \"CBlock\", \"CBlock\", \"CBlock\", \"CBlock\", \"PConc\", \"PConcâ€¦\n$ baths_total    &lt;dbl&gt; 1.0, 1.0, 1.5, 2.5, 2.5, 2.5, 2.0, 2.0, 2.0, 2.5, 2.5, â€¦\n$ age            &lt;dbl&gt; 50, 49, 52, 42, 13, 12, 9, 18, 15, 11, 17, 18, 12, 20, â€¦\n$ remod_age      &lt;dbl&gt; 50, 49, 52, 42, 12, 12, 9, 18, 14, 11, 16, 3, 12, 20, 2â€¦\n$ ln_price       &lt;dbl&gt; 12.27839, 11.56172, 12.05525, 12.40492, 12.15425, 12.18â€¦\n$ ln_area        &lt;dbl&gt; 7.412160, 6.797940, 7.192182, 7.654443, 7.395722, 7.380â€¦\n\n\n\n\n\nğŸ§  From Continuous to Binary: Define High vs Low Price\nWe create a binary target equal to 1 if sale_price is at least the sample median and 0 otherwise. The median splits the sample into two balanced groups.\n\n# Compute the sample median of the sale price\nprice_med &lt;- median(df$sale_price, na.rm = TRUE)\n\n# Create the binary dependent variable\ndf &lt;- df %&gt;% mutate(high_price = as.integer(sale_price &gt;= price_med))\n\n# Check class balance\ntable(df$high_price)\n\n\n   0    1 \n1442 1476 \n\nprop.table(table(df$high_price))\n\n\n        0         1 \n0.4941741 0.5058259 \n\n\n\n\n\n\n\n\nâœ¨ Why logit & probit exist (the aha! moment)\n\n\n\nOLS fails when the dependent variable is 0/1 because predicted values can be &lt;0 or &gt;1.\nInstead of predicting price category directly, logit and probit assume:\n\nThere is an unobserved (latent) index of â€œhousing quality/valueâ€ and we only observe whether it crosses a threshold.\n\n\\(y_i^* = X_i \\beta + \\varepsilon_i\n\\quad\\rightarrow\\quad\ny_i = \\begin{cases}\n1 & \\text{if } y_i^* &gt; 0 \\\\\n0 & \\text{otherwise}\n\\end{cases}\\)\nThe difference is the distribution of the error term \\(\\varepsilon\\):\n\n\n\nModel\nAssumption for ( )\nEffect\n\n\n\n\nLogit\nLogistic distribution\nCoefficients scale with odds.\n\n\nProbit\nNormal distribution\nCoefficients on z-score scale.\n\n\n\nBecause logistic and normal are very similar:\n\nLogit coefficient â‰ˆ 1.6 Ã— probit coefficient\n\nBut both produce nearly identical predicted probabilities and classification.\n\n\n\n\n\nğŸ§© Model Specification (same RHS as Supervision 5)\nWe keep the hedonic RHS unchanged. Only the outcome changed (continuous -&gt; binary).\n\n# Hedonic formula reused for binary models\nf_bin &lt;- high_price ~ ln_area + bedroom_abv_gr + baths_total +\n  total_bsmt_sf + garage_cars + central_air + overall_qual +\n  overall_cond + exter_qual + kitchen_qual + fireplace_qu +\n  bldg_type + house_style + neighborhood\n\n\n\n\nâš™ï¸ Estimation (fit only) + ğŸª“ Backward Selection (BIC)\nWe now estimate the latent model using glm():\n\\(P(y = 1 \\mid X) = \\text{link}^{-1}(X\\beta)\\)\n\nlogit â†’ inverse-logit (plogis()): returns a probability via odds\nprobit â†’ normal CDF (pnorm()): returns a probability via z-score\n\nLogit uses the logistic CDF (cumulative distribution function); probit uses the (scaled) normal CDF. They almost overlap, which is why results are so similar and logit â‰ˆ 1.6 Ã— probit in scale. Letâ€™s compare the CDFs:\n\n# Logistic vs Probit (variance-matched) â€” super short CDF plot\nsd_match &lt;- pi / sqrt(3)              # match Normal variance to Logistic\ncurve(plogis(x), from=-6, to=6, lwd=2, xlab=\"Latent index (x)\", ylab=\"CDF\",\n      main=\"CDF: Logistic vs Normal (variance-matched)\")\ncurve(pnorm(x, sd=sd_match), from=-6, to=6, lwd=2, lty=2, add=TRUE)\nlegend(\"topleft\", c(\"Logistic (logit)\", \"Normal scaled (probit)\"),\n       lwd=2, lty=c(1,2), bty=\"n\")\n\n\n\n\n\n\n\n\nIn the next chunk we suppress printing baseline models to avoid overwhelming output (too many variables). However, we run backward stepwise (BIC) and present a compact, readable table of that selected model. Coefficient signs and significance are typically very similar; logit coefficients are usually ~1.6Ã— probit coefficients (scale difference).\n\n# --- Packages used in this chunk -------------------------------------------\nlibrary(dplyr)      # data wrangling\nlibrary(broom)      # tidy model outputs (term, estimate, std.error, statistic, p.value)\nlibrary(stringr)    # clean up long variable names\nlibrary(gt)         # neat tables\nlibrary(lmtest)     # coeftest() with robust SEs\nlibrary(sandwich)   # vcovHC() for robust (HC) variance\n\n# --- Convert ordered factors to plain factors --------------------------------\n# Why? Ordered factors produce polynomial contrasts (.L, .Q, ...) that are hard to interpret.\n# Using plain factors yields standard dummies and lets step() drop/keep whole factors cleanly.\ndf &lt;- df |&gt;\n  mutate(\n    overall_qual = as.factor(overall_qual),\n    overall_cond = as.factor(overall_cond),\n    exter_qual   = as.factor(exter_qual),\n    kitchen_qual = as.factor(kitchen_qual),\n    fireplace_qu = as.factor(fireplace_qu),\n    central_air  = as.factor(central_air)  # ensure \"Y\"/\"N\" is categorical\n  )\n\n# --- Fit full LOGIT & PROBIT (we suppress long summaries for readability) ----\nlogit_full  &lt;- glm(f_bin, data = df, family = binomial(\"logit\"))\nprobit_full &lt;- glm(f_bin, data = df, family = binomial(\"probit\"))\n\n# --- Backward stepwise with BIC (k = log(n)) on both models ------------------\n# BIC penalizes complexity more than AIC -&gt; typically a leaner, more generalizable model.\nk_bic &lt;- log(nrow(df))\nlogit_bic  &lt;- step(logit_full,  direction = \"backward\", k = k_bic, trace = 0)\nprobit_bic &lt;- step(probit_full, direction = \"backward\", k = k_bic, trace = 0)\n\n# --- Quick fit comparison (lower is better) ----------------------------------\nbic_tab &lt;- tibble(\n  model = c(\"logit_bic\", \"probit_bic\"),\n  AIC   = c(AIC(logit_bic),  AIC(probit_bic)),\n  BIC   = c(BIC(logit_bic),  BIC(probit_bic))\n)\nbic_tab  # glance: which BIC-selected model scores better?\n\n# A tibble: 2 Ã— 3\n  model        AIC   BIC\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 logit_bic  1251. 1418.\n2 probit_bic 1266. 1433.\n\n# --- Keep BOTH models for downstream sections --------------------------------\nmain_logit &lt;- logit_bic   # LOGIT (BIC-selected)\nmain_probit &lt;- probit_bic  # PROBIT (BIC-selected)\n\n# --- Also clarify which one is \"best\" by BIC ---------------------------------\nbest_model &lt;- if (BIC(logit_bic) &lt; BIC(probit_bic)) {\n  message(\"âœ… BIC selects the LOGIT model as best (more parsimonious).\")\n  logit_bic\n} else {\n  message(\"âœ… BIC selects the PROBIT model as best (more parsimonious).\")\n  probit_bic\n}\n\n# --- Helper: compact robust-SE table for any glm -----------------------------\nmake_compact_table &lt;- function(mod, title_text) {\n  # Robust HC1 variance-covariance and tidy coefficients\n  rob_vcov &lt;- sandwich::vcovHC(mod, type = \"HC1\")\n  out &lt;- broom::tidy(lmtest::coeftest(mod, vcov. = rob_vcov)) |&gt;\n    filter(term != \"(Intercept)\") |&gt;\n    arrange(p.value) |&gt;\n    # Friendlier labels for students (extend as needed)\n    mutate(\n      term = term |&gt;\n        str_replace(\"^ln_area$\", \"ln(living area)\") |&gt;\n        str_replace(\"^baths_total$\", \"Total baths\") |&gt;\n        str_replace(\"^garage_cars$\", \"Garage capacity\") |&gt;\n        str_replace(\"^central_airY$\", \"Central air: Yes\") |&gt;\n        str_replace(\"^bldg_type\", \"Bldg type: \") |&gt;\n        str_replace(\"^house_style\", \"Style: \") |&gt;\n        str_replace(\"^neighborhood\", \"Nbh: \")\n    ) |&gt;\n    slice_head(n = 20) |&gt;\n    mutate(\n      estimate  = round(estimate, 4),\n      std.error = round(std.error, 4),\n      statistic = round(statistic, 4),\n      p.value   = round(p.value, 4)\n    )\n\n  # Render a gt table (no exotic functions to avoid errors)\n  out |&gt;\n    gt() |&gt;\n    tab_header(title = title_text) |&gt;\n    cols_label(\n      term = \"Term\", estimate = \"Coef\", std.error = \"Robust SE\",\n      statistic = \"z\", p.value = \"p\"\n    ) |&gt;\n    tab_options(table.width = pct(100), data_row.padding = px(3))\n}\n\n# --- Print compact tables for BOTH models ------------------------------------\nmake_compact_table(main_logit, \"Logit (BIC-selected): Key Terms â€” Robust SEs\")\n\n\n\n\n\n\n\nLogit (BIC-selected): Key Terms â€” Robust SEs\n\n\nTerm\nCoef\nRobust SE\nz\np\n\n\n\n\noverall_qual^9\n15.1397\n0.7831\n19.3326\n0.0000\n\n\nexter_qual.L\n-10.6344\n0.5561\n-19.1225\n0.0000\n\n\noverall_qual^4\n-12.5784\n0.7454\n-16.8753\n0.0000\n\n\nexter_qual.Q\n7.0991\n0.6307\n11.2550\n0.0000\n\n\nTotal baths\n1.7096\n0.2143\n7.9788\n0.0000\n\n\nln(living area)\n3.9021\n0.5224\n7.4689\n0.0000\n\n\ntotal_bsmt_sf\n0.0020\n0.0003\n7.1608\n0.0000\n\n\noverall_qual.C\n-8.3438\n1.1840\n-7.0472\n0.0000\n\n\noverall_qual^5\n-5.4813\n0.7807\n-7.0212\n0.0000\n\n\nexter_qual.C\n-4.1839\n0.7068\n-5.9199\n0.0000\n\n\nGarage capacity\n0.8858\n0.1569\n5.6446\n0.0000\n\n\noverall_qual.L\n-4.1405\n0.9283\n-4.4603\n0.0000\n\n\nBldg type: OneFam\n2.0304\n0.5295\n3.8341\n0.0001\n\n\nbedroom_abv_gr\n-0.5331\n0.1417\n-3.7617\n0.0002\n\n\noverall_qual^8\n3.7655\n1.0726\n3.5106\n0.0004\n\n\nCentral air: Yes\n1.9202\n0.5563\n3.4518\n0.0006\n\n\nfireplace_qu.Q\n0.9649\n0.3096\n3.1168\n0.0018\n\n\nfireplace_qu^5\n-0.4994\n0.2292\n-2.1787\n0.0294\n\n\nBldg type: TwoFmCon\n1.5261\n0.7411\n2.0594\n0.0395\n\n\noverall_qual^6\n1.1504\n0.6616\n1.7389\n0.0820\n\n\n\n\n\n\nmake_compact_table(main_probit, \"Probit (BIC-selected): Key Terms â€” Robust SEs  ğŸ“Œ We will also use this downstream\")\n\n\n\n\n\n\n\nProbit (BIC-selected): Key Terms â€” Robust SEs ğŸ“Œ We will also use this downstream\n\n\nTerm\nCoef\nRobust SE\nz\np\n\n\n\n\nexter_qual.L\n-3.8454\n0.2067\n-18.5996\n0.0000\n\n\noverall_qual^4\n-3.2031\n0.2120\n-15.1082\n0.0000\n\n\noverall_qual^9\n4.2012\n0.3855\n10.8982\n0.0000\n\n\nexter_qual.Q\n2.4456\n0.2911\n8.4024\n0.0000\n\n\nTotal baths\n0.9290\n0.1245\n7.4616\n0.0000\n\n\noverall_qual.C\n-2.6018\n0.3538\n-7.3531\n0.0000\n\n\nln(living area)\n2.0371\n0.2844\n7.1631\n0.0000\n\n\ntotal_bsmt_sf\n0.0011\n0.0002\n6.7114\n0.0000\n\n\nGarage capacity\n0.4570\n0.0933\n4.8984\n0.0000\n\n\nexter_qual.C\n-1.6890\n0.3597\n-4.6950\n0.0000\n\n\nCentral air: Yes\n1.0294\n0.2772\n3.7132\n0.0002\n\n\nBldg type: OneFam\n1.0286\n0.3022\n3.4032\n0.0007\n\n\nbedroom_abv_gr\n-0.2609\n0.0787\n-3.3141\n0.0009\n\n\noverall_qual^7\n-1.1811\n0.3584\n-3.2956\n0.0010\n\n\nfireplace_qu.Q\n0.5618\n0.1757\n3.1978\n0.0014\n\n\noverall_qual^5\n-1.3032\n0.4239\n-3.0741\n0.0021\n\n\noverall_qual.Q\n0.8228\n0.3084\n2.6678\n0.0076\n\n\noverall_qual.L\n-0.7386\n0.2871\n-2.5723\n0.0101\n\n\nfireplace_qu^5\n-0.2574\n0.1272\n-2.0234\n0.0430\n\n\nBldg type: TwoFmCon\n0.7736\n0.4246\n1.8219\n0.0685\n\n\n\n\n\n\n# --- (Optional) Kept vs Dropped list for transparency ------------------------\n# Uncomment to show what BIC dropped vs. kept for each model.\n# kept1   &lt;- attr(terms(main_logit), \"term.labels\"); full1 &lt;- attr(terms(logit_full),  \"term.labels\")\n# kept2   &lt;- attr(terms(main_probit), \"term.labels\"); full2 &lt;- attr(terms(probit_full), \"term.labels\")\n# dropped1 &lt;- setdiff(full1, kept1); dropped2 &lt;- setdiff(full2, kept2)\n# tibble(model = \"LOGIT\", status = \"Kept\", term = kept1) |&gt;\n#   bind_rows(tibble(model = \"LOGIT\", status = \"Dropped\", term = dropped1)) |&gt;\n#   bind_rows(tibble(model = \"PROBIT\", status = \"Kept\", term = kept2)) |&gt;\n#   bind_rows(tibble(model = \"PROBIT\", status = \"Dropped\", term = dropped2)) |&gt;\n#   arrange(model, status, term) |&gt;\n#   gt() |&gt;\n#   tab_header(title = \"Backward BIC: Kept vs Dropped Terms (Logit & Probit)\") |&gt;\n#   tab_options(table.width = pct(100))\n\nRobust Standard Errors (recommended for cross-sections)\n\n# HC1 robust SEs using sandwich + lmtest\ncoeftest(main_logit,  vcov. = vcovHC(main_logit,  type = \"HC1\"))\n\n\nz test of coefficients:\n\n                     Estimate  Std. Error  z value  Pr(&gt;|z|)    \n(Intercept)       -3.5899e+01  3.8264e+00  -9.3819 &lt; 2.2e-16 ***\nln_area            3.9021e+00  5.2244e-01   7.4689 8.086e-14 ***\nbedroom_abv_gr    -5.3308e-01  1.4171e-01  -3.7617 0.0001688 ***\nbaths_total        1.7096e+00  2.1427e-01   7.9788 1.478e-15 ***\ntotal_bsmt_sf      1.9908e-03  2.7802e-04   7.1608 8.021e-13 ***\ngarage_cars        8.8578e-01  1.5693e-01   5.6446 1.656e-08 ***\ncentral_airY       1.9202e+00  5.5629e-01   3.4518 0.0005569 ***\noverall_qual.L    -4.1405e+00  9.2831e-01  -4.4603 8.185e-06 ***\noverall_qual.Q     1.2659e+00  1.1710e+00   1.0811 0.2796449    \noverall_qual.C    -8.3438e+00  1.1840e+00  -7.0472 1.825e-12 ***\noverall_qual^4    -1.2578e+01  7.4537e-01 -16.8753 &lt; 2.2e-16 ***\noverall_qual^5    -5.4813e+00  7.8068e-01  -7.0212 2.199e-12 ***\noverall_qual^6     1.1504e+00  6.6158e-01   1.7389 0.0820451 .  \noverall_qual^7    -1.3063e-01  7.9330e-01  -0.1647 0.8692110    \noverall_qual^8     3.7655e+00  1.0726e+00   3.5106 0.0004472 ***\noverall_qual^9     1.5140e+01  7.8312e-01  19.3326 &lt; 2.2e-16 ***\nexter_qual.L      -1.0634e+01  5.5612e-01 -19.1225 &lt; 2.2e-16 ***\nexter_qual.Q       7.0991e+00  6.3075e-01  11.2550 &lt; 2.2e-16 ***\nexter_qual.C      -4.1839e+00  7.0676e-01  -5.9199 3.222e-09 ***\nfireplace_qu.L    -1.9306e-01  3.6918e-01  -0.5230 0.6010086    \nfireplace_qu.Q     9.6486e-01  3.0957e-01   3.1168 0.0018281 ** \nfireplace_qu.C     3.9833e-01  3.8274e-01   1.0407 0.2980044    \nfireplace_qu^4    -7.1073e-02  3.7366e-01  -0.1902 0.8491463    \nfireplace_qu^5    -4.9939e-01  2.2922e-01  -2.1787 0.0293574 *  \nbldg_typeOneFam    2.0304e+00  5.2955e-01   3.8341 0.0001260 ***\nbldg_typeTwnhs    -5.7459e-01  6.8362e-01  -0.8405 0.4006233    \nbldg_typeTwnhsE    1.9953e-01  6.4498e-01   0.3094 0.7570455    \nbldg_typeTwoFmCon  1.5261e+00  7.4106e-01   2.0594 0.0394569 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncoeftest(main_probit, vcov. = vcovHC(main_probit, type = \"HC1\"))\n\n\nz test of coefficients:\n\n                     Estimate  Std. Error  z value  Pr(&gt;|z|)    \n(Intercept)       -1.8833e+01  2.0729e+00  -9.0853 &lt; 2.2e-16 ***\nln_area            2.0371e+00  2.8439e-01   7.1631 7.889e-13 ***\nbedroom_abv_gr    -2.6092e-01  7.8730e-02  -3.3141 0.0009193 ***\nbaths_total        9.2897e-01  1.2450e-01   7.4616 8.546e-14 ***\ntotal_bsmt_sf      1.0688e-03  1.5925e-04   6.7114 1.927e-11 ***\ngarage_cars        4.5703e-01  9.3300e-02   4.8984 9.660e-07 ***\ncentral_airY       1.0294e+00  2.7722e-01   3.7132 0.0002046 ***\noverall_qual.L    -7.3856e-01  2.8713e-01  -2.5723 0.0101034 *  \noverall_qual.Q     8.2278e-01  3.0841e-01   2.6678 0.0076351 ** \noverall_qual.C    -2.6018e+00  3.5383e-01  -7.3531 1.936e-13 ***\noverall_qual^4    -3.2031e+00  2.1201e-01 -15.1082 &lt; 2.2e-16 ***\noverall_qual^5    -1.3032e+00  4.2393e-01  -3.0741 0.0021117 ** \noverall_qual^6    -2.6588e-01  3.2419e-01  -0.8201 0.4121312    \noverall_qual^7    -1.1811e+00  3.5838e-01  -3.2956 0.0009821 ***\noverall_qual^8     3.6818e-01  5.6090e-01   0.6564 0.5115536    \noverall_qual^9     4.2012e+00  3.8550e-01  10.8982 &lt; 2.2e-16 ***\nexter_qual.L      -3.8454e+00  2.0675e-01 -18.5996 &lt; 2.2e-16 ***\nexter_qual.Q       2.4456e+00  2.9106e-01   8.4024 &lt; 2.2e-16 ***\nexter_qual.C      -1.6890e+00  3.5974e-01  -4.6950 2.666e-06 ***\nfireplace_qu.L    -1.3497e-01  1.9931e-01  -0.6772 0.4982982    \nfireplace_qu.Q     5.6178e-01  1.7568e-01   3.1978 0.0013850 ** \nfireplace_qu.C     2.1859e-01  2.1709e-01   1.0069 0.3139763    \nfireplace_qu^4    -4.2936e-03  2.1376e-01  -0.0201 0.9839743    \nfireplace_qu^5    -2.5744e-01  1.2724e-01  -2.0234 0.0430357 *  \nbldg_typeOneFam    1.0286e+00  3.0224e-01   3.4032 0.0006659 ***\nbldg_typeTwnhs    -3.3721e-01  3.8970e-01  -0.8653 0.3868682    \nbldg_typeTwnhsE    1.3499e-01  3.6188e-01   0.3730 0.7091348    \nbldg_typeTwoFmCon  7.7362e-01  4.2463e-01   1.8219 0.0684741 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\nWhy marginal effects?\n\n\n\nLogit/probit coefficients are not changes in probability â€” they change the latent index.\nMarginal effects compute:\n\nâ€œHolding everything else constant, how does \\(P(y=1)\\) change when \\(x\\) increases by 1?â€\n\nThat is why AMEs are interpretable in percentage points.\n\n\n\n\nğŸ“ˆ Interpreting Effects with Marginal Effects (AMEs)\nRaw coefficients are on latent/odds scales. Average Marginal Effects (AMEs) translate them to percentage-point changes in probability for a one-unit change in each predictor.\nLogit/probit coefficients are not probabilitiesâ€”they shift the latent score, not \\(P(Y=1)\\) directly. Marginal effects translate results into probability terms: â€œby how many percentage points does \\(P(Y=1)\\) change when a variable increases by one unit, holding others constant?â€ Average Marginal Effects (AMEs) average this change over all observations, so the numbers can be read directly in percentage-point terms. Use AMEs for clear, substantive interpretation.\n\n# Average Marginal Effects for both models\nme_logit  &lt;- margins(main_logit)\nme_probit &lt;- margins(main_probit)\n\nsummary(me_logit)\n\n                     factor     AME      SE       z      p    lower   upper\n                baths_total  0.1052  0.0103 10.1961 0.0000   0.0850  0.1254\n             bedroom_abv_gr -0.0328  0.0080 -4.1198 0.0000  -0.0484 -0.0172\n            bldg_typeOneFam  0.1235  0.0282  4.3879 0.0000   0.0684  0.1787\n             bldg_typeTwnhs -0.0367  0.0370 -0.9921 0.3212  -0.1093  0.0358\n            bldg_typeTwnhsE  0.0125  0.0348  0.3608 0.7183  -0.0556  0.0807\n          bldg_typeTwoFmCon  0.0933  0.0407  2.2886 0.0221   0.0134  0.1731\n               central_airY  0.1196  0.0323  3.7083 0.0002   0.0564  0.1829\n             exter_qualFair -0.4655  0.7340 -0.6341 0.5260  -1.9041  0.9732\n             exter_qualGood -0.4043  0.7312 -0.5529 0.5803  -1.8373  1.0288\n          exter_qualTypical -0.5041  0.7311 -0.6895 0.4905  -1.9369  0.9288\n           fireplace_quFair -0.0314  0.1720 -0.1823 0.8554  -0.3685  0.3058\n           fireplace_quGood -0.0345  0.1703 -0.2025 0.8395  -0.3683  0.2993\n   fireplace_quNo_Fireplace -0.0953  0.1702 -0.5600 0.5755  -0.4289  0.2383\n           fireplace_quPoor -0.0475  0.1730 -0.2747 0.7835  -0.3866  0.2915\n        fireplace_quTypical  0.0002  0.1703  0.0013 0.9989  -0.3336  0.3341\n                garage_cars  0.0545  0.0085  6.4286 0.0000   0.0379  0.0711\n                    ln_area  0.2401  0.0299  8.0421 0.0000   0.1816  0.2986\n        overall_qualAverage -0.0792  0.0153 -5.1632 0.0000  -0.1093 -0.0492\n  overall_qualBelow_Average -0.1616  0.0354 -4.5675 0.0000  -0.2310 -0.0923\n      overall_qualExcellent -0.0424  0.0888 -0.4776 0.6329  -0.2166  0.1317\n           overall_qualFair -0.4835  0.9774 -0.4947 0.6208  -2.3991  1.4321\n           overall_qualGood  0.0595  0.0181  3.2782 0.0010   0.0239  0.0951\n           overall_qualPoor -0.4812  1.4431 -0.3334 0.7388  -3.3097  2.3473\n overall_qualVery_Excellent  0.4305 23.4932  0.0183 0.9854 -45.6152 46.4763\n      overall_qualVery_Good  0.2160  0.0462  4.6738 0.0000   0.1254  0.3066\n      overall_qualVery_Poor -0.4822  1.2336 -0.3909 0.6959  -2.9000  1.9356\n              total_bsmt_sf  0.0001  0.0000  8.2160 0.0000   0.0001  0.0002\n\nsummary(me_probit)\n\n                     factor     AME      SE       z      p    lower   upper\n                baths_total  0.1077  0.0106 10.1842 0.0000   0.0870  0.1285\n             bedroom_abv_gr -0.0303  0.0081 -3.7268 0.0002  -0.0462 -0.0143\n            bldg_typeOneFam  0.1178  0.0273  4.3118 0.0000   0.0643  0.1713\n             bldg_typeTwnhs -0.0397  0.0371 -1.0709 0.2842  -0.1124  0.0330\n            bldg_typeTwnhsE  0.0157  0.0340  0.4622 0.6439  -0.0510  0.0824\n          bldg_typeTwoFmCon  0.0888  0.0413  2.1505 0.0315   0.0079  0.1698\n               central_airY  0.1208  0.0320  3.7760 0.0002   0.0581  0.1836\n             exter_qualFair -0.4737  0.9976 -0.4749 0.6349  -2.4290  1.4815\n             exter_qualGood -0.3996  0.9951 -0.4016 0.6880  -2.3501  1.5508\n          exter_qualTypical -0.5063  0.9951 -0.5088 0.6109  -2.4566  1.4441\n           fireplace_quFair -0.0386  0.1353 -0.2854 0.7753  -0.3038  0.2266\n           fireplace_quGood -0.0419  0.1330 -0.3150 0.7528  -0.3027  0.2189\n   fireplace_quNo_Fireplace -0.1030  0.1330 -0.7746 0.4386  -0.3636  0.1576\n           fireplace_quPoor -0.0593  0.1370 -0.4327 0.6652  -0.3277  0.2092\n        fireplace_quTypical -0.0040  0.1331 -0.0303 0.9758  -0.2650  0.2569\n                garage_cars  0.0530  0.0087  6.0966 0.0000   0.0360  0.0700\n                    ln_area  0.2363  0.0302  7.8258 0.0000   0.1771  0.2955\n        overall_qualAverage -0.0868  0.0159 -5.4681 0.0000  -0.1179 -0.0557\n  overall_qualBelow_Average -0.1528  0.0339 -4.5071 0.0000  -0.2192 -0.0863\n      overall_qualExcellent -0.1140  0.0652 -1.7473 0.0806  -0.2418  0.0139\n           overall_qualFair -0.4761  2.2142 -0.2150 0.8298  -4.8159  3.8637\n           overall_qualGood  0.0638  0.0184  3.4662 0.0005   0.0277  0.0999\n           overall_qualPoor -0.3782 29.5342 -0.0128 0.9898 -58.2641 57.5077\n overall_qualVery_Excellent  0.0921 26.3486  0.0035 0.9972 -51.5502 51.7344\n      overall_qualVery_Good  0.1886  0.0420  4.4916 0.0000   0.1063  0.2709\n      overall_qualVery_Poor -0.4340 35.3525 -0.0123 0.9902 -69.7235 68.8555\n              total_bsmt_sf  0.0001  0.0000  8.2283 0.0000   0.0001  0.0002\n\n# Tip for reporting:\n# \"A 0.01 increase in ln_area (~1% more area) changes the probability of being high-priced by X percentage points (AME).\"\n\n\n\n\nğŸ”® Predicted Probabilities & Agreement Between Models\n\n# Predicted probabilities from each model\ndf &lt;- df %&gt;%\n  mutate(\n    p_logit  = predict(main_logit,  type = \"response\"),\n    p_probit = predict(main_probit, type = \"response\")\n  )\n\n# Compare probability predictions\ncor(df$p_logit, df$p_probit, use = \"complete.obs\")\n\n[1] 0.9993592\n\nsummary(df$p_logit)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00000 0.02626 0.49132 0.50583 0.97866 1.00000 \n\nsummary(df$p_probit)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00000 0.02449 0.48779 0.50484 0.98036 1.00000 \n\n\n\n\n\nğŸ§ª Classification Diagnostics: ROC & AUC\nAn ROC (Receiver Operating Characteristic) curve is a graph that shows a classifierâ€™s performance by plotting the true positive rate against the false positive rate at various threshold settings. The AUC (Area Under the Curve) is a single summary metric that measures how well a model can distinguish between classes, with a higher value indicating better performance. A perfect classifier has an AUC of (1.0), while a random classifier has an AUC of (0.5).Â \nWe evaluate ranking performance via ROC curves and AUC.\n\n\n\n\n\n\nROC intuition\n\n\n\nROC doesnâ€™t evaluate the predicted probability, but the ranking quality.\n\nâ€œAmong all pairs of houses, how often does the model rank the expensive one higher?â€\n\n\nAUC = 0.5 â†’ random odds\nAUC = 1.0 â†’ perfect ranking\n\n\n\nA ROC curve evaluates how well the model ranks positives above negatives across all possible thresholds. It does not measure overall â€œpercent correct,â€ but the trade-off between true positive rate (sensitivity) and false positive rate as the cutoff moves. The AUC summarizes ranking ability from 0.5 (random) to 1.0 (perfect). Two models can have similar accuracy at one cutoff yet different AUCs, which is why ROC/AUC is informative.\n\n# ROC objects (treating 1 as the positive class)\nroc_logit  &lt;- roc(df$high_price, df$p_logit)\nroc_probit &lt;- roc(df$high_price, df$p_probit)\n\n# AUC values\nauc(roc_logit)\n\nArea under the curve: 0.9746\n\nauc(roc_probit)\n\nArea under the curve: 0.9745\n\n# Simple ROC plots with ggplot2\nautoplot_roc &lt;- function(roc_obj, title){\n  # Build a data frame with TPR (sensitivity) and FPR (1 - specificity)\n  plot_df &lt;- data.frame(\n    tpr = roc_obj$sensitivities,\n    fpr = 1 - roc_obj$specificities\n  )\n  ggplot(plot_df, aes(x = fpr, y = tpr)) +\n    geom_line() +\n    geom_abline(slope = 1, intercept = 0, linetype = 2) +\n    labs(x = \"False Positive Rate\", y = \"True Positive Rate\", title = title)\n}\n\nautoplot_roc(roc_logit,  paste0(\"ROC â€“ Logit (AUC = \", round(auc(roc_logit), 3), \")\"))\n\n\n\n\n\n\n\nautoplot_roc(roc_probit, paste0(\"ROC â€“ Probit (AUC = \", round(auc(roc_probit), 3), \")\"))\n\n\n\n\n\n\n\n\n\n\n\nâœ‚ï¸ Choosing a Threshold & Confusion Matrices\nClassification requires a threshold on predicted probabilities. The Youden J criterion often gives a better cut-off than the naÃ¯ve 0.5.\nChoosing a single cutoff is a separate decision from estimating probabilities. The default 0.5 is arbitrary and may be sub-optimal when the costs of false positives and false negatives differ. The Youden J statistic selects the threshold that maximizes sensitivity + specificity âˆ’ 1, giving a balanced operating point. We will compare results at 0.5 and at the Youden-optimal threshold\n\n# Optimal thresholds by Youden's J\nthr_logit  &lt;- coords(roc_logit,  x = \"best\", best.method = \"youden\", transpose = TRUE)[\"threshold\"]\nthr_probit &lt;- coords(roc_probit, x = \"best\", best.method = \"youden\", transpose = TRUE)[\"threshold\"]\n\nthr_logit; thr_probit\n\nthreshold \n0.4980051 \n\n\nthreshold \n0.4479988 \n\n\nA confusion matrix maps predictions to actuals and classifies outcomes into four cells: True Positive (correct 1), False Positive (false alarm), False Negative (missed 1), and True Negative (correct 0). Sensitivity = TP/(TP+FN) is a rate â€œhow many actual true positives we catch from all true positives?â€\nSpecificity = TN/(TN+FP) asks â€œhow many actual 0s did we avoid flagging?â€\n\n# Helper to compute a confusion matrix and basic metrics without extra packages\nconfusion_metrics &lt;- function(y_true, y_prob, thr = 0.5) {\n  y_pred &lt;- ifelse(y_prob &gt;= thr, 1L, 0L)\n  TP &lt;- sum(y_pred == 1 & y_true == 1, na.rm = TRUE)\n  TN &lt;- sum(y_pred == 0 & y_true == 0, na.rm = TRUE)\n  FP &lt;- sum(y_pred == 1 & y_true == 0, na.rm = TRUE)\n  FN &lt;- sum(y_pred == 0 & y_true == 1, na.rm = TRUE)\n  acc &lt;- (TP + TN) / (TP + TN + FP + FN)\n  tpr &lt;- TP / (TP + FN)  # sensitivity / recall\n  tnr &lt;- TN / (TN + FP)  # specificity\n  ppv &lt;- TP / (TP + FP)  # precision\n  npv &lt;- TN / (TN + FN)  # negative predicted value\n  list(\n    threshold = thr,\n    matrix = matrix(c(TN, FP, FN, TP), nrow = 2,\n                    dimnames = list(\"Predicted\" = c(\"0\",\"1\"), \"Actual\" = c(\"0\",\"1\"))),\n    accuracy = acc, sensitivity = tpr, specificity = tnr, precision = ppv, npv = npv\n  )\n}\n\n# Confusion matrices at 0.5 and at the optimal Youden threshold\ncm_logit_05 &lt;- confusion_metrics(df$high_price, df$p_logit,  thr = 0.5)\ncm_probit_05 &lt;- confusion_metrics(df$high_price, df$p_probit, thr = 0.5)\ncm_logit_opt &lt;- confusion_metrics(df$high_price, df$p_logit,  thr = as.numeric(thr_logit))\ncm_probit_opt &lt;- confusion_metrics(df$high_price, df$p_probit, thr = as.numeric(thr_probit))\n\ncm_logit_05\n\n$threshold\n[1] 0.5\n\n$matrix\n         Actual\nPredicted    0    1\n        0 1333  134\n        1  109 1342\n\n$accuracy\n[1] 0.9167238\n\n$sensitivity\n[1] 0.9092141\n\n$specificity\n[1] 0.9244105\n\n$precision\n[1] 0.9248794\n\n$npv\n[1] 0.9086571\n\ncm_probit_05\n\n$threshold\n[1] 0.5\n\n$matrix\n         Actual\nPredicted    0    1\n        0 1330  140\n        1  112 1336\n\n$accuracy\n[1] 0.9136395\n\n$sensitivity\n[1] 0.9051491\n\n$specificity\n[1] 0.9223301\n\n$precision\n[1] 0.9226519\n\n$npv\n[1] 0.9047619\n\ncm_logit_opt\n\n$threshold\n[1] 0.4980051\n\n$matrix\n         Actual\nPredicted    0    1\n        0 1333  132\n        1  109 1344\n\n$accuracy\n[1] 0.9174092\n\n$sensitivity\n[1] 0.9105691\n\n$specificity\n[1] 0.9244105\n\n$precision\n[1] 0.9249828\n\n$npv\n[1] 0.9098976\n\ncm_probit_opt\n\n$threshold\n[1] 0.4479988\n\n$matrix\n         Actual\nPredicted    0    1\n        0 1310  109\n        1  132 1367\n\n$accuracy\n[1] 0.9174092\n\n$sensitivity\n[1] 0.9261518\n\n$specificity\n[1] 0.9084605\n\n$precision\n[1] 0.9119413\n\n$npv\n[1] 0.9231853\n\n\n\n\nFinal takeaway\n\nLogit and probit tell the same economic story\nAMEs let us interpret effects in percentage points\nROC/Youden threshold evaluates predictive/classification performance\n\n\n\n\nğŸ“ What to Report (suggested structure)\n\nModel setup: how the binary target was defined; why the median.\nEstimation: logit & probit results; include robust SE tables.\nInterpretation: AMEs for key variables (e.g., ln_area, overall_qual, baths_total).\nPerformance: AUC for both models; comment on similarity/differences.\nClassification: confusion matrices at 0.5 and at the Youden-optimal threshold; discuss trade-offs.\nReflection: when would you prefer logit vs probit? (Hint: results are usually very similar; choice often driven by convention or interpretability.)\n\n\n\n\nâœ… Checklist\n\nCode runs from top to bottom without manual edits.\nAll variables in your RHS are present/constructed.\nYou report robust SEs and AMEs.\nYou include AUC and describe threshold choice.\nYour interpretations are in probability (percentage-point) terms.\n\n\nğŸ End of Supervision 6\nğŸ›‘ Remember to save your script ğŸ’¾",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Supervision 6</span>"
    ]
  },
  {
    "objectID": "supervision_7.html",
    "href": "supervision_7.html",
    "title": "Supervision 7",
    "section": "",
    "text": "ğŸ§­ Session Overview\nPrerequisites: - Platform skills: Qualtrics, Prolific/MTurk, consent flows\nPurpose: Bridge from distributing surveys to designing questions that elicit valid, credible qualitative data_, especially on potentially sensitive topics. This supervision emphasizes qualitative research approaches within survey design.\nKey Transition: You now know how to build and deploy a survey. Today we focus on what to ask and how to ask itâ€”particularly when seeking rich, nuanced responses rather than simple numerical scales.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#core-materials",
    "href": "supervision_7.html#core-materials",
    "title": "Supervision 7",
    "section": "Core Materials",
    "text": "Core Materials\n\nEVS 2017 Master Questionnaire (English):\nhttps://access.gesis.org/dbk/69554?download_purpose=-99\nEVS data & documentation hub:\nhttps://europeanvaluesstudy.eu/methodology-data-documentation/survey-2017/full-release-evs2017/",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#optional-background",
    "href": "supervision_7.html#optional-background",
    "title": "Supervision 7",
    "section": "Optional Background",
    "text": "Optional Background\n\nEVS/WVS joint release: https://europeanvaluesstudy.eu/methodology-data-documentation/survey-2017/joint-evs-wvs/",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#platform-reminder",
    "href": "supervision_7.html#platform-reminder",
    "title": "Supervision 7",
    "section": "Platform Reminder",
    "text": "Platform Reminder\n\nYour Qualtrics account (from Lab 5)\nEVS questionnaire as exemplar of large-scale survey design\n\n\nğŸ’¡ Why EVS? This authentic instrument demonstrates how major cross-national studies navigate sensitive topics, cultural variation, and measurement challenges. Weâ€™ll use it to examine both strengths and limitations.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#part-1-mini-lecture-15-min",
    "href": "supervision_7.html#part-1-mini-lecture-15-min",
    "title": "Supervision 7",
    "section": "Part 1: Mini-Lecture (15 min)",
    "text": "Part 1: Mini-Lecture (15 min)",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#qualitative-vs.-quantitative-questions-in-surveys",
    "href": "supervision_7.html#qualitative-vs.-quantitative-questions-in-surveys",
    "title": "Supervision 7",
    "section": "1.1 Qualitative vs.Â Quantitative Questions in Surveys",
    "text": "1.1 Qualitative vs.Â Quantitative Questions in Surveys\n\nQuantitative Approach\n\nStructure: Closed-ended, fixed response categories\nOutput: Numerical data for statistical analysis\nExample: â€œOn a scale of 1-5, how satisfied are you withâ€¦?â€\nStrength: Comparable, analyzable at scale\nLimitation: May miss nuance, context, unexpected perspectives\n\n\n\nQualitative Approach\n\nStructure: Open-ended, narrative responses, or scenario-based\nOutput: Text data requiring thematic/content analysis\nExample: â€œDescribe a situation where you felt satisfied/dissatisfiedâ€¦â€\nStrength: Rich detail, unexpected insights, contextual understanding\nLimitation: Time-intensive to analyze; harder to generalize numerically\n\n\n\nWhen to Choose Each\n\nQuantitative: Testing hypotheses, measuring prevalence, comparing groups\nQualitative: Exploring meanings, understanding processes, generating hypotheses\nMixed: Often most powerfulâ€”use qualitative to inform quantitative scales or explain quantitative patterns",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#representativity-external-validity",
    "href": "supervision_7.html#representativity-external-validity",
    "title": "Supervision 7",
    "section": "1.2 Representativity â†’ External Validity",
    "text": "1.2 Representativity â†’ External Validity\n\nCore Concept\nRepresentativity means your sample composition resembles your target population on characteristics that matter for your inference.\n\n\nWhy It Matters\n\nThreat: If key groups are under/over-represented, results may not generalize (external validity risk)\nExample: Surveying only landlords when you need tenant perspectives too\nReal Estate Context: Property investors vs.Â owner-occupiers vs.Â renters may have systematically different views on housing policy\n\n\n\nPragmatic Reality\n\nPerfect representativity is unattainable\nGoal: Credible coverage + transparent documentation of limitations\nYour RM01 surveys: Consider who youâ€™re reaching via Prolific/MTurk/personal networks",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#survey-weights-high-level-understanding",
    "href": "supervision_7.html#survey-weights-high-level-understanding",
    "title": "Supervision 7",
    "section": "1.3 Survey Weights (High-Level Understanding)",
    "text": "1.3 Survey Weights (High-Level Understanding)\nYou wonâ€™t compute weights in this course, but understand their purpose:\n\nDesign Weights\n\nPurpose: Correct unequal selection probabilities from sampling design\nExample: If rural respondents had 1/3 the chance of selection, weight them 3Ã— higher\n\n\n\nPost-Stratification Weights\n\nPurpose: After fieldwork, adjust to population benchmarks (e.g., ageÃ—sexÃ—region)\nUse case: Your Prolific sample skews young? Reweight to match UK census margins\nLimitation: Weighting reduces certain biases but cannot fix poor question design\n\n\n\nKey Takeaway\nRepresentative sampling + good weights help external validity.\nGood question design helps internal validity (measuring what you intend to measure).\nYou need both.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#question-formulation-pitfalls",
    "href": "supervision_7.html#question-formulation-pitfalls",
    "title": "Supervision 7",
    "section": "1.4 Question Formulation Pitfalls",
    "text": "1.4 Question Formulation Pitfalls\n\nLeading/Loaded Wording\nâŒ â€œDonâ€™t you agree that affordable housing is essential?â€\nâœ… â€œTo what extent do you think affordable housing is important?â€\n\n\nDouble-Barrelled Questions\n\nâœ… Heuristic: If a question contains and, it is probably doubleâ€‘barrelled. âŒ â€œHow satisfied are you with your income and job security?â€\nâœ… Split into two questionsâ€”income satisfaction and job security are distinct constructs\n\n\n\nAmbiguity & Vagueness\nâŒ â€œHow often do you visit properties?â€ (Whatâ€™s â€œoftenâ€? Personal viewings? Client viewings?)\nâœ… â€œIn the past month, approximately how many property viewings did you conduct?â€\n\n\nSocial Desirability Bias\n\nRisk: Respondents answer what sounds â€œgoodâ€ rather than truthfully\nTopics: Income, discriminatory attitudes, rule compliance, controversial opinions\nStrategy: Indirect elicitation (see next section)",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#sensitive-topics-direct-vs.-indirect-elicitation",
    "href": "supervision_7.html#sensitive-topics-direct-vs.-indirect-elicitation",
    "title": "Supervision 7",
    "section": "1.5 Sensitive Topics: Direct vs.Â Indirect Elicitation",
    "text": "1.5 Sensitive Topics: Direct vs.Â Indirect Elicitation\n\nDirect Questions\n\nApproach: Straightforward, explicit\nExample: â€œHave you ever discriminated against a tenant on the basis of ethnicity?â€\nPro: Simple, transparent\nCon: Likely underreporting due to social desirability\n\n\n\nIndirect Approaches\n\nQualitative Techniques\n\nThird-person framing: â€œHow common do you think ethnic discrimination is among landlords in your area?â€\nProjective prompts: â€œImagine a landlord who prefers not to rent to certain ethnic groups. What might their reasoning be?â€\nVignettes/scenarios: Present a hypothetical situation; ask for interpretation or advice\nList experiments (concept): Compare item counts between control/treatment groups to infer prevalence without individual disclosure\n\n\n\nExample: Immigration Attitudes\n\nDirect: â€œDo you support stricter immigration controls?â€ (May trigger socially desirable response)\nIndirect: Present vignette of immigrant family seeking housing; ask open-ended: â€œWhat factors should a landlord consider when evaluating this application?â€\n\n\n\n\nWhen to Use Indirect Methods\n\nStigmatized behaviors or attitudes\nIllegal/unethical practices\nTopics where respondent may fear judgment\nExploratory research where you want honest, detailed narratives",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#part-2-activity-a-25-min-rewriting-sensitive-questions",
    "href": "supervision_7.html#part-2-activity-a-25-min-rewriting-sensitive-questions",
    "title": "Supervision 7",
    "section": "Part 2: Activity A (25 min) â€“ Rewriting Sensitive Questions",
    "text": "Part 2: Activity A (25 min) â€“ Rewriting Sensitive Questions",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#task-overview",
    "href": "supervision_7.html#task-overview",
    "title": "Supervision 7",
    "section": "Task Overview",
    "text": "Task Overview\nWorking in pairs, choose one sensitive theme below. For your chosen theme:\n\nDraft a direct question (quantitative or closed-ended)\nDraft an indirect/qualitative alternative using one or more of:\n\nVignette with open-ended follow-up\nProjective/third-person framing\nScenario-based question\n\nCompare: What kind of data would each approach yield? What are the trade-offs?",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#themes-pick-one",
    "href": "supervision_7.html#themes-pick-one",
    "title": "Supervision 7",
    "section": "Themes (Pick One)",
    "text": "Themes (Pick One)\n\nWe analyze wording and data quality â€” not beliefs or personal views.\n\n\nA. Immigration & Housing\nContext: Attitudes toward immigrant access to housing/social housing\n\n\nB. Regulatory Compliance\nContext: Landlordsâ€™ adherence to safety regulations, licensing, tax reporting\n\n\nC. Political Tolerance\nContext: Acceptance of diverse political views in community/workplace\n\n\nD. Gender Norms at Work\nContext: Beliefs about women in property development/construction leadership\n\n\nE. Religious Practice & Real Estate\nContext: Religious considerations in property transactions, development decisions",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#deliverable",
    "href": "supervision_7.html#deliverable",
    "title": "Supervision 7",
    "section": "Deliverable",
    "text": "Deliverable\nPrepare to share both versions (2 minutes per pair): - Direct question - Indirect alternative - One insight about what changes between approaches",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#quality-checklist",
    "href": "supervision_7.html#quality-checklist",
    "title": "Supervision 7",
    "section": "Quality Checklist",
    "text": "Quality Checklist\n\nâœ… Neutral wording (no leading language)?\nâœ… Single construct (not double-barrelled)?\nâœ… Clear, defined terms (no ambiguous â€œoften,â€ â€œregularlyâ€)?\nâœ… Acknowledges social desirability pressure?\nâœ… Indirect version genuinely reduces pressure while still addressing topic?",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#part-3-activity-b-40-min-evs-question-audit",
    "href": "supervision_7.html#part-3-activity-b-40-min-evs-question-audit",
    "title": "Supervision 7",
    "section": "Part 3: Activity B (40 min) â€“ EVS Question Audit",
    "text": "Part 3: Activity B (40 min) â€“ EVS Question Audit",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#overview",
    "href": "supervision_7.html#overview",
    "title": "Supervision 7",
    "section": "Overview",
    "text": "Overview\nThe European Values Study surveys thousands of respondents across Europe on values, beliefs, and attitudes. Weâ€™ll audit how EVS handles sensitive/complex topics and consider qualitative improvements.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#step-1-form-groups-choose-subtopic-5-min",
    "href": "supervision_7.html#step-1-form-groups-choose-subtopic-5-min",
    "title": "Supervision 7",
    "section": "Step 1: Form Groups & Choose Subtopic (5 min)",
    "text": "Step 1: Form Groups & Choose Subtopic (5 min)\nGroups of 3â€“5. Each group selects one subtopic:\n\nReligion & Morality (e.g., items on religiosity, moral absolutes)\nFamily & Gender Equality (e.g., womenâ€™s roles, work-family balance)\nPolitics, Democracy & Trust (e.g., institutional trust, political engagement)\nNational Identity, Migration & Diversity (e.g., immigration attitudes, national pride)\nWork, Economy & Inequality (e.g., income fairness, job values)\nEnvironment & Climate Attitudes (e.g., willingness to sacrifice for environment)",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#step-2-locate-select-items-10-min",
    "href": "supervision_7.html#step-2-locate-select-items-10-min",
    "title": "Supervision 7",
    "section": "Step 2: Locate & Select Items (10 min)",
    "text": "Step 2: Locate & Select Items (10 min)\n\nOpen the EVS 2017 Master Questionnaire (link above)\nFind the section(s) relevant to your subtopic\nSelect 2â€“3 items (not 3â€“5â€”weâ€™re prioritizing depth over breadth)\nScreenshot or note item codes for reference",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#step-3-analyze-each-item-15-min",
    "href": "supervision_7.html#step-3-analyze-each-item-15-min",
    "title": "Supervision 7",
    "section": "Step 3: Analyze Each Item (15 min)",
    "text": "Step 3: Analyze Each Item (15 min)\nFor each selected item, consider:\n\nWording Analysis\n\nIs language neutral or leading/loaded?\nAny double-barrelled issues (two concepts in one question)?\nAre terms clearly defined or ambiguous?\n\n\n\nElicitation Strategy\n\nDirect or indirect approach?\nWhat assumptions does the question make?\nWhat might respondents be reluctant to reveal?\n\n\n\nResponse Format\n\nClosed scales (e.g., 1â€“10, agree/disagree)?\nDoes the format constrain nuanced responses?\nWould open-ended follow-ups add value?\n\n\n\nQualitative Potential\n\nHow might you reframe this as an open-ended question?\nCould a vignette or scenario elicit richer data?\nWould third-person framing reduce social desirability pressure?",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#step-4-propose-improvements-10-min",
    "href": "supervision_7.html#step-4-propose-improvements-10-min",
    "title": "Supervision 7",
    "section": "Step 4: Propose Improvements (10 min)",
    "text": "Step 4: Propose Improvements (10 min)\nFor one of your selected items, draft: - Improved qualitative version (open-ended, vignette, or projective) - Brief rationale (what does your version capture that the original doesnâ€™t?)",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#group-output-template",
    "href": "supervision_7.html#group-output-template",
    "title": "Supervision 7",
    "section": "Group Output Template",
    "text": "Group Output Template\nPrepare 3â€“5 bullet points to share with class:\n\nEVS item citation (short quote or paraphrase + item code if available)\nStrengths (1â€“2): What does EVS do well here?\nLimitations (1â€“2): Whatâ€™s constrained or problematic?\nYour improved qualitative question\nSensitivity/ethical considerations (if relevant)\n\n\nExample Output\n\nEVS Item V185: â€œOn this list are various groups of people. Could you please mention any that you would not like to have as neighbours?â€\nStrengths:\n- Indirect approach reduces pressure to deny prejudice\n- List format allows multiple selections\nLimitations:\n- Still fairly direct (asks about personal preferences)\n- Closed list may miss relevant groups\n- Doesnâ€™t capture reasoning or context\nImproved qualitative version:\nâ€œThink about your neighbourhood. What kinds of qualities or characteristics do you think make for a good neighbour? What kinds of situations might create tension between neighbours?â€\n(Follow-up probe: â€œCan you give an example of a situation youâ€™ve observed or heard about?â€)\nRationale: Shifts focus to abstract qualities and situations rather than demographic groups, reducing defensiveness while still revealing underlying attitudes. Open-ended format invites storytelling and context.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#sharing-10-min",
    "href": "supervision_7.html#sharing-10-min",
    "title": "Supervision 7",
    "section": "Sharing (10 min)",
    "text": "Sharing (10 min)\nEach group presents one key insight from their EVS audit: - How does question wording shape the data you get? - What do you gain (or lose) moving from quantitative to qualitative?",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#synthesis-5-min",
    "href": "supervision_7.html#synthesis-5-min",
    "title": "Supervision 7",
    "section": "Synthesis (5 min)",
    "text": "Synthesis (5 min)\nSupervisor highlights cross-cutting themes: - Trade-offs between standardization and nuance - Role of context in qualitative responses - Ethical considerations in sensitive topic research - Application to real estate/land economy contexts",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#reflection-prompt",
    "href": "supervision_7.html#reflection-prompt",
    "title": "Supervision 7",
    "section": "Reflection Prompt",
    "text": "Reflection Prompt\n\nâ€œWhat will you do differently when designing your own survey questions for RM01?â€\n\nWrite 3â€“5 sentences addressing: - One specific wording pitfall youâ€™ll avoid - One context where youâ€™ll use qualitative (open-ended) questions - One sensitive topic youâ€™ll approach indirectly (if relevant to your project)\nSave this reflection with your RM01 project notes.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#optional-peer-exchange",
    "href": "supervision_7.html#optional-peer-exchange",
    "title": "Supervision 7",
    "section": "Optional: Peer Exchange",
    "text": "Optional: Peer Exchange\nIf time permits, share your reflection with one other student and discuss: - Are your projects facing similar design challenges? - Could you pilot questions on each other before wider distribution?",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#task-audit-your-own-rm01-survey-draft",
    "href": "supervision_7.html#task-audit-your-own-rm01-survey-draft",
    "title": "Supervision 7",
    "section": "Task: Audit Your Own RM01 Survey Draft",
    "text": "Task: Audit Your Own RM01 Survey Draft\nReview three questions from your own survey design:\n\nCheck for pitfalls:\n\nLeading/loaded language? â†’ Neutralize\nDouble-barrelled? â†’ Split into separate items\nVague terms? â†’ Define or provide examples\n\nConsider elicitation strategy:\n\nIs any topic sensitive enough to warrant indirect approach?\nWould a qualitative (open-ended) version add value?\nCould you mix quantitative and qualitative for richer data?\n\nRevise and document:\n\nShow original version\nShow revised version\nExplain your reasoning (1â€“2 sentences per change)\n\n\nUpload before next session for optional peer review.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#a.-question-wording-checklist",
    "href": "supervision_7.html#a.-question-wording-checklist",
    "title": "Supervision 7",
    "section": "A. Question Wording Checklist",
    "text": "A. Question Wording Checklist\n\n\n\nâŒ Avoid\nâœ… Aim For\n\n\n\n\nLeading/loaded phrasing\nNeutral, balanced language\n\n\nDouble-barrelled (two ideas at once)\nSingle construct per question\n\n\nVague terms (â€œoften,â€ â€œregularlyâ€)\nSpecific timeframes, quantities\n\n\nOverly long/complex stems\nConcise, clear phrasing\n\n\nJargon or technical terms\nPlain language (or define terms)\n\n\nAssume respondent knowledge\nInclude context if needed",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#b.-indirect-elicitation-techniques",
    "href": "supervision_7.html#b.-indirect-elicitation-techniques",
    "title": "Supervision 7",
    "section": "B. Indirect Elicitation Techniques",
    "text": "B. Indirect Elicitation Techniques\n\nFor Qualitative Survey Design\n\n\n\n\n\n\n\n\nTechnique\nDescription\nExample\n\n\n\n\nThird-person\nAsk about others, not respondent\nâ€œHow do landlords in your area typically handle tenant disputes?â€\n\n\nProjective\nRespondent interprets someone elseâ€™s behavior\nâ€œImagine a developer who opposes affordable housing. What might their concerns be?â€\n\n\nVignette\nPresent scenario; ask for reaction/advice\nâ€œSarah is a single mother applying to rent. [Details]. If you were the landlord, what factors would you consider?â€\n\n\nBest-worst scaling\nForce trade-offs between options\nâ€œWhich of these housing features would you prioritize? Which would you sacrifice?â€\n\n\nFree listing\nOpen brainstorming before rating\nâ€œList all the factors that influence your property investment decisions.â€ (Then follow with structured questions)\n\n\n\n\n\nFor Quantitative Prevalence Estimates (Conceptualâ€”Not Implemented in This Course)\n\n\n\n\n\n\n\n\nTechnique\nPurpose\nHow It Works\n\n\n\n\nList experiment\nEstimate prevalence of sensitive behavior\nControl group sees innocuous list; treatment group sees same list + sensitive item. Difference in means estimates prevalence.\n\n\nRandomized response\nProtect individual privacy while estimating prevalence\nRespondent flips coin privately; answers YES if heads (regardless of truth), answers truthfully if tails. Randomness protects individuals; statistics recover population rate.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#c.-representativity-validity",
    "href": "supervision_7.html#c.-representativity-validity",
    "title": "Supervision 7",
    "section": "C. Representativity & Validity",
    "text": "C. Representativity & Validity\n\nKey Relationships\nRepresentativity â†’ External Validity\n    â†“\nCan we generalize findings beyond our sample?\n\nGood Question Design â†’ Internal Validity  \n    â†“\nAre we measuring what we intend to measure?\n\n\nSurvey Weights (Context Only)\n\nDesign weights: Correct for unequal selection probabilities\nPost-stratification weights: Align sample to population benchmarks\nNote: Weighting helps external validity but cannot fix poor measurement\n\n\n\nFor Your RM01 Projects\nConsider: - Target population: Who are you trying to understand? - Sampling frame: Who can you actually reach (Prolific, MTurk, professional networks)? - Coverage gaps: Whoâ€™s missing? Document these as limitations. - Generalizability claims: Be precise about scope of inference",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#d.-connecting-to-lab-5-platform-skills",
    "href": "supervision_7.html#d.-connecting-to-lab-5-platform-skills",
    "title": "Supervision 7",
    "section": "D. Connecting to Lab 5 Platform Skills",
    "text": "D. Connecting to Lab 5 Platform Skills\n\nLab 5 Taught You:\n\nSet up Qualtrics surveys\nConfigure consent flows\nCreate Prolific/MTurk accounts\nUnderstand fee structures\n\n\n\nSupervision 7 Builds On This:\n\nWhat questions to put in your Qualtrics survey\nHow to word those questions to get valid data\nWhen to use qualitative vs.Â quantitative approaches\nHow to handle sensitive topics ethically\n\n\n\nIntegration Task (If Time Permits)\nOpen your Qualtrics account and: 1. Create one quantitative question (scale/multiple choice) 2. Create one qualitative question (open text with clear prompt) 3. Set up branch logic so qualitative question only appears if quantitative response meets certain criteria (e.g., â€œIf dissatisfied, please explain why:â€)",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#survey-design-classics",
    "href": "supervision_7.html#survey-design-classics",
    "title": "Supervision 7",
    "section": "Survey Design Classics",
    "text": "Survey Design Classics\n\nFowler, F. J. (2014). Survey Research Methods (5th ed.). SAGE.\nTourangeau, R., & Yan, T. (2007). â€œSensitive questions in surveys.â€ Psychological Bulletin, 133(5), 859-883.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#qualitative-survey-methods",
    "href": "supervision_7.html#qualitative-survey-methods",
    "title": "Supervision 7",
    "section": "Qualitative Survey Methods",
    "text": "Qualitative Survey Methods\n\nBraun, V., & Clarke, V. (2006). â€œUsing thematic analysis in psychology.â€ Qualitative Research in Psychology, 3(2), 77-101.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#sensitive-topics",
    "href": "supervision_7.html#sensitive-topics",
    "title": "Supervision 7",
    "section": "Sensitive Topics",
    "text": "Sensitive Topics\n\nLee, R. M., & Renzetti, C. M. (1993). â€œThe problems of researching sensitive topics.â€ American Behavioral Scientist, 33(5), 510-528.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "supervision_7.html#evswvs-methodology",
    "href": "supervision_7.html#evswvs-methodology",
    "title": "Supervision 7",
    "section": "EVS/WVS Methodology",
    "text": "EVS/WVS Methodology\n\nEVS (2020). European Values Study 2017: Method Report. GESIS.\n\n\nEnd of Supervision 7\nNext supervision: [Topic TBDâ€”likely analysis of qualitative survey data or pilot testing findings]",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Supervision 7</span>"
    ]
  },
  {
    "objectID": "formative_test_sup3.html",
    "href": "formative_test_sup3.html",
    "title": "In-Class Exercise Sup 3",
    "section": "",
    "text": "In-Class Formative Assessment: Variable Selection (Housing Prices)",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>In-Class Exercise Sup 3</span>"
    ]
  },
  {
    "objectID": "formative_test_sup3.html#in-class-formative-assessment-variable-selection-housing-prices",
    "href": "formative_test_sup3.html#in-class-formative-assessment-variable-selection-housing-prices",
    "title": "In-Class Exercise Sup 3",
    "section": "",
    "text": "â± Duration\nApprox. 40 minutes (in class)\n\n\n\nğŸ¯ Objective\nApply the variable selection methods learned in Supervision 3 to a new dataset on housing prices. You will replicate the steps used previously with the Hitters dataset, but this time using the Housing Prices Dataset from Kaggle.\nYour task is to specify, estimate, and evaluate one or two regression models explaining house prices.\n\n\n\nğŸ“¦ Dataset (Preloaded) or Download the housing dataset\nThis dataset has already been provided in your RStudio environment.\ninstall.packages(\"RKaggle\")\nlibrary(RKaggle)\n# Download and load dataset\nhouse_price_data &lt;- get_dataset(\"yasserh/housing-prices-dataset\")\nstr(house_price_data)\nDependent variable: price\nPredictors include: area, bedrooms, bathrooms, stories, mainroad, airconditioning, parking, furnishingstatus, etc.\n\n\n\nğŸ§© Task\nYou have 40 minutes to complete the following:\n\nModel setup (5 min)\n\nInspect the dataset and identify relevant variables.\nChoose predictors that you expect to influence price.\n\nModel estimation (20 min)\n\nEstimate one OLS model explaining price.\nPerform stepwise variable selection (AIC or BIC).\n\nOptionally include one interaction term (e.g., area Ã— stories).\n\nDiagnostics and reporting (15 min)\n\nReport the final regression equation.\nRecord Adjusted RÂ², AIC/BIC, and RMSE (if possible).\n\nBriefly comment on which predictors are most influential and whether signs/magnitudes make sense.\n\n\n\n\n\nğŸ§® Example Structure (you can adapt quickly)\nlibrary(MASS)\nlibrary(dplyr)\n\n# Example base model\ndf &lt;- house_price_data\nmodel0 &lt;- lm(price ~ area + bathrooms + stories + airconditioning + parking, data = df)\nsummary(model0)\n\n# Stepwise selection (AIC)\nstep_aic &lt;- stepAIC(model0, direction = \"both\", trace = FALSE)\nsummary(step_aic)\n\n# Optional: interaction example\nmodel_int &lt;- lm(price ~ area*stories + bathrooms + airconditioning, data = df)\nsummary(model_int)\n\n\n\nğŸ§¾ What to Submit (in class)\n\nFinal model equation\n\nAdjusted RÂ², AIC/BIC, and any other metric you computed\n\nInterpretation of one key variable or interaction term\n\n1â€“2 sentences on whether the results make economic sense",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>In-Class Exercise Sup 3</span>"
    ]
  },
  {
    "objectID": "formative_test_sup3.html#submit-your-in-class-results",
    "href": "formative_test_sup3.html#submit-your-in-class-results",
    "title": "In-Class Exercise Sup 3",
    "section": "Submit your in-class results",
    "text": "Submit your in-class results\nIf the form doesnâ€™t load below, use the backup link: Open the form in a new tab\n\n \n\n\n\nğŸ’¡ Tip\n\nUse your Supervision 3 (Hitters) scripts as a reference.\n\nKeep your model parsimonious but explanatory.\n\nFocus on clarity and reasoning â€” not the number of variables.",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>In-Class Exercise Sup 3</span>"
    ]
  },
  {
    "objectID": "formative_test_sup4.html",
    "href": "formative_test_sup4.html",
    "title": "In-Class Exercise Sup 4",
    "section": "",
    "text": "Learning goals (40 min)",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>In-Class Exercise Sup 4</span>"
    ]
  },
  {
    "objectID": "formative_test_sup4.html#learning-goals-40-min",
    "href": "formative_test_sup4.html#learning-goals-40-min",
    "title": "In-Class Exercise Sup 4",
    "section": "",
    "text": "Fit a clear production model with meaningful regressors.\nDiagnose multicollinearity (VIF), autocorrelation (DW/BG & residual ACF), and heteroskedasticity (BP/White) and modify/refine the baseline model.\nApply robust/HAC standard errors for valid inference.\nCommunicate findings succinctly.\n\n\nMarking (formative)\n\nTask 1 (multicollinearity) â€“ 25 pts\n\nTask 2 (autocorrelation) â€“ 25 pts\n\nTask 3 (heteroskedasticity & robust SE) â€“ 25 pts\n\nTask 4 (report results: final model spec, economic analysis & evaluation) â€“ 25 pts\nTotal: 100 pts",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>In-Class Exercise Sup 4</span>"
    ]
  },
  {
    "objectID": "formative_test_sup4.html#setup",
    "href": "formative_test_sup4.html#setup",
    "title": "In-Class Exercise Sup 4",
    "section": "Setup",
    "text": "Setup\nData: The rice dataset contains a time series on rice production from Asian farms (1990-1997). Variables include: - prod: output (tons) - area: land (hectares) - fert: fertilizer (kg) - labor: labor hours (person-days)\n\n# Packages (minimal)\n# install the following packages if needed.\n\nlibrary(PoEdata); library(dplyr); library(car); library(lmtest); library(sandwich)\n\n\nLoad rice, order by year.\n\nDrop the firm identifier (firm).\n\nPreview the data.\n\n\n# Load the dataset 'rice' from the PoEdata package (must have library(PoEdata) loaded)\ndata(rice)\n\n# Sort the rows by the time variable so observations are in chronological order\nrice &lt;- rice[order(rice$year), ]\n\n# Drop the identifier column 'firm' (it's just an ID, not an economic variable)\nrice &lt;- subset(rice, select = -firm)\n\n# Look at the first 6 rows to get a quick sense of the data\nhead(rice, 6)\n\n   year  prod area labor  fert\n1  1990  7.87  2.5   160 207.5\n9  1990 10.35  3.8   184 303.5\n17 1990  9.98  3.4   170 252.0\n25 1990  4.83  1.4    68  88.0\n33 1990  8.74  3.6   130 149.8\n41 1990  1.84  0.5    34  21.0\n\n# List all column names so you know what variables are available\nnames(rice)\n\n[1] \"year\"  \"prod\"  \"area\"  \"labor\" \"fert\" \n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nWhy drop firm? Itâ€™s an identifier, not an economic input. Keep IDs only for estimations involving grouping/panels, not as regressors.",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>In-Class Exercise Sup 4</span>"
    ]
  },
  {
    "objectID": "formative_test_sup4.html#task-1-multicollinearity-25-pts",
    "href": "formative_test_sup4.html#task-1-multicollinearity-25-pts",
    "title": "In-Class Exercise Sup 4",
    "section": "Task 1 â€” Multicollinearity (25 pts)",
    "text": "Task 1 â€” Multicollinearity (25 pts)\nFit the baseline model that you will need to modify/refine if needed: [ ]\n\nm &lt;- lm(prod ~ area + fert + labor, data = rice)\nsummary(m)\n\n\nCall:\nlm(formula = prod ~ area + fert + labor, data = rice)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2849 -0.9268  0.0351  0.9402  8.9061 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.138800   0.201714  -0.688 0.491845    \narea         1.321321   0.208748   6.330 7.57e-10 ***\nfert         0.004591   0.001289   3.562 0.000419 ***\nlabor        0.027505   0.003898   7.057 9.27e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.106 on 348 degrees of freedom\nMultiple R-squared:  0.8294,    Adjusted R-squared:  0.8279 \nF-statistic:   564 on 3 and 348 DF,  p-value: &lt; 2.2e-16\n\n\nCompute VIF and interpret.\n\nvif(m)\n\n    area     fert    labor \n7.266083 3.736222 7.064198 \n\n\nAnswer (brief, 150 words max):\n1. State the VIF values and threshold used 2. Identify which variables show problematic collinearity 3. Suggest ONE specific econometric solution appropriate for this production function",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>In-Class Exercise Sup 4</span>"
    ]
  },
  {
    "objectID": "formative_test_sup4.html#task-2-autocorrelation-25-pts",
    "href": "formative_test_sup4.html#task-2-autocorrelation-25-pts",
    "title": "In-Class Exercise Sup 4",
    "section": "Task 2 â€” Autocorrelation (25 pts)",
    "text": "Task 2 â€” Autocorrelation (25 pts)\nRun Durbinâ€“Watson and Breuschâ€“Godfrey (AR1) tests.\nThen plot the residual ACF to visualise the pattern. For example, the current model specification:\n\n# Durbinâ€“Watson test for first-order autocorrelation in OLS residuals\n# H0: no autocorrelation (rho = 0). DW ~ 2 means no autocorr; DW &lt; 2 = positive autocorr.\ndwtest(m)\n\n\n    Durbin-Watson test\n\ndata:  m\nDW = 1.7928, p-value = 0.02453\nalternative hypothesis: true autocorrelation is greater than 0\n\n# Breuschâ€“Godfrey test for serial correlation of order 1 (more general than DW)\n# H0: no serial correlation up to the specified order.\nbgtest(m, order = 1)\n\n\n    Breusch-Godfrey test for serial correlation of order up to 1\n\ndata:  m\nLM test = 3.711, df = 1, p-value = 0.05406\n\n# Extract OLS residuals from model m\nres_m &lt;- residuals(m)\n\n# Plot the autocorrelation function of residuals\n# Significant spike at lag 1 (outside blue bands) suggests AR(1)-type positive autocorrelation\nacf(res_m, main = \"ACF of OLS residuals (m)\")\n\n\n\n\n\n\n\n\nAnswer (brief, 150 words max):\n- Do tests indicate autocorrelation? Which sign? did you modify/refine the model? - What does the residual ACF suggest (e.g., AR(1)-type)? How would you address the issue for inference?\n\n\n\n\n\n\nTip\n\n\n\n\n\nHint: DW &lt; 2 â†’ positive serial correlation. A large spike at lag 1 in ACF is consistent with AR(1).",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>In-Class Exercise Sup 4</span>"
    ]
  },
  {
    "objectID": "formative_test_sup4.html#task-3-heteroskedasticity-robusthac-se-25-pts",
    "href": "formative_test_sup4.html#task-3-heteroskedasticity-robusthac-se-25-pts",
    "title": "In-Class Exercise Sup 4",
    "section": "Task 3 â€” Heteroskedasticity & Robust/HAC SE (25 pts)",
    "text": "Task 3 â€” Heteroskedasticity & Robust/HAC SE (25 pts)\n\nTest for heteroskedasticity: Breuschâ€“Pagan and a White-style auxiliary test.\n\nReport HAC (Neweyâ€“West) standard errors for valid inference under both heteroskedasticity and autocorrelation.\n\n\n# --- Heteroskedasticity tests -----------------------------------------------\n\n# Breuschâ€“Pagan test for heteroskedasticity\n# H0: constant variance (homoskedasticity). Small p-value =&gt; heteroskedasticity present.\nbptest(m)\n\n\n    studentized Breusch-Pagan test\n\ndata:  m\nBP = 81.47, df = 3, p-value &lt; 2.2e-16\n\n# \"White-style\" test: regress squared residuals on fitted and fitted^2 (more general form)\nfit &lt;- fitted(m)\nbptest(m, ~ fit + I(fit^2))  # small p-value =&gt; heteroskedasticity\n\n\n    studentized Breusch-Pagan test\n\ndata:  m\nBP = 73.769, df = 2, p-value &lt; 2.2e-16\n\n# --- Robust inference when heteroskedasticity and/or autocorrelation exist ---\n\n# Neweyâ€“West (HAC) robust standard errors for valid t-stats/p-values\n# Note: This adjusts SEs, not coefficients; residual patterns/tests remain the same.\ncoeftest(m, vcov. = NeweyWest(m, lag = 1, prewhite = FALSE))\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) -0.1388003  0.1754394 -0.7912    0.4294    \narea         1.3213211  0.3112151  4.2457 2.799e-05 ***\nfert         0.0045905  0.0024470  1.8760    0.0615 .  \nlabor        0.0275053  0.0059720  4.6057 5.774e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#**White test specification**: The `~ fit + I(fit^2)` is one \n# version; this is a simplified White test.\n\nAnswer (brief, 150 words max):\n- Are BP/White significant? Compare classical vs HAC p-values: which coefficients remain significant?\n\n\n\n\n\n\nTip\n\n\n\n\n\nNote: Robust/HAC SEs do not change the residuals or DW/BG results; they change inference (SEs/p-values).",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>In-Class Exercise Sup 4</span>"
    ]
  },
  {
    "objectID": "formative_test_sup4.html#task-4-report-results-final-model-spec-economic-analysis-evaluation-25-pts",
    "href": "formative_test_sup4.html#task-4-report-results-final-model-spec-economic-analysis-evaluation-25-pts",
    "title": "In-Class Exercise Sup 4",
    "section": "Task 4 â€” report results: final model spec, economic analysis & evaluation (25 pts)",
    "text": "Task 4 â€” report results: final model spec, economic analysis & evaluation (25 pts)\nSummarise (250 words max) (25 pts): 1. Your best/final model specification (equation) 2. The key diagnostic issues you found (which tests, what they showed) 3. The remedies you applied (e.g.Â variable interactions, lags, HAC) 4. Why your final inference is valid for decision-making, results analysis and evaluation (brief economic interpretation).",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>In-Class Exercise Sup 4</span>"
    ]
  },
  {
    "objectID": "formative_test_sup4.html#submit-your-answers",
    "href": "formative_test_sup4.html#submit-your-answers",
    "title": "In-Class Exercise Sup 4",
    "section": "Submit your answers",
    "text": "Submit your answers\nPlease submit your completed answers using the embedded form below ğŸ‘‡ If the form does not display correctly, you can open it directly using this link:\nğŸ‘‰ Open Microsoft Form",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>In-Class Exercise Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup3.html",
    "href": "solution_formative_test_sup3.html",
    "title": "Solution to In-class Sup3",
    "section": "",
    "text": "Overview\nThis document reports the best model and key diagnostics based on an exhaustive subset selection using leaps::regsubsets. It also reproduces the R code used (with a few comment tweaks for clarity) and provides concise answers to:",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Solution to In-class Sup3</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup3.html#overview",
    "href": "solution_formative_test_sup3.html#overview",
    "title": "Solution to In-class Sup3",
    "section": "",
    "text": "Best model equation\n\nModel metrics (Adjusted RÂ², BIC, RMSE)\n\nInterpretation of one key variable\n\nWhether the results make economic sense",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Solution to In-class Sup3</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup3.html#r-code-reproducible-workflow",
    "href": "solution_formative_test_sup3.html#r-code-reproducible-workflow",
    "title": "Solution to In-class Sup3",
    "section": "R Code (reproducible workflow)",
    "text": "R Code (reproducible workflow)\n\n\nCode\n# Load libraries\n# If needed: install.packages(c(\"ISLR\", \"leaps\", \"ggplot2\", \"dplyr\", \"RKaggle\"))\nlibrary(ISLR)\nlibrary(leaps)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(RKaggle)\n\n# Download and load dataset from Kaggle\nhouse_price_data &lt;- get_dataset(\"yasserh/housing-prices-dataset\")\n\n# Quick structure checks\nstr(house_price_data)\n\n\nspc_tbl_ [545 Ã— 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ price           : num [1:545] 13300000 12250000 12250000 12215000 11410000 ...\n $ area            : num [1:545] 7420 8960 9960 7500 7420 7500 8580 16200 8100 5750 ...\n $ bedrooms        : num [1:545] 4 4 3 4 4 3 4 5 4 3 ...\n $ bathrooms       : num [1:545] 2 4 2 2 1 3 3 3 1 2 ...\n $ stories         : num [1:545] 3 4 2 2 2 1 4 2 2 4 ...\n $ mainroad        : chr [1:545] \"yes\" \"yes\" \"yes\" \"yes\" ...\n $ guestroom       : chr [1:545] \"no\" \"no\" \"no\" \"no\" ...\n $ basement        : chr [1:545] \"no\" \"no\" \"yes\" \"yes\" ...\n $ hotwaterheating : chr [1:545] \"no\" \"no\" \"no\" \"no\" ...\n $ airconditioning : chr [1:545] \"yes\" \"yes\" \"no\" \"yes\" ...\n $ parking         : num [1:545] 2 3 2 3 2 2 2 0 2 1 ...\n $ prefarea        : chr [1:545] \"yes\" \"no\" \"yes\" \"yes\" ...\n $ furnishingstatus: chr [1:545] \"furnished\" \"furnished\" \"semi-furnished\" \"furnished\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   price = col_double(),\n  ..   area = col_double(),\n  ..   bedrooms = col_double(),\n  ..   bathrooms = col_double(),\n  ..   stories = col_double(),\n  ..   mainroad = col_character(),\n  ..   guestroom = col_character(),\n  ..   basement = col_character(),\n  ..   hotwaterheating = col_character(),\n  ..   airconditioning = col_character(),\n  ..   parking = col_double(),\n  ..   prefarea = col_character(),\n  ..   furnishingstatus = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nCode\ndim(house_price_data)\n\n\n[1] 545  13\n\n\nCode\nsum(is.na(house_price_data$price))\n\n\n[1] 0\n\n\nCode\n# Brief data description\ncat(\"Response variable: price (1987 currency units).\\n\")\n\n\nResponse variable: price (1987 currency units).\n\n\nCode\ncat(\"Predictor variables:\\n\")\n\n\nPredictor variables:\n\n\nCode\nnames(house_price_data)[-13]\n\n\n [1] \"price\"           \"area\"            \"bedrooms\"        \"bathrooms\"      \n [5] \"stories\"         \"mainroad\"        \"guestroom\"       \"basement\"       \n [9] \"hotwaterheating\" \"airconditioning\" \"parking\"         \"prefarea\"       \n\n\nCode\n# Handle missing values (drop any rows with NAs)\nhouse_price_data &lt;- na.omit(house_price_data)\ndim(house_price_data)\n\n\n[1] 545  13\n\n\nCode\nsum(is.na(house_price_data))\n\n\n[1] 0\n\n\nCode\n# Ensure categorical variables are treated as factors (for lm consistency)\nchr_cols &lt;- sapply(house_price_data, is.character)\nhouse_price_data[chr_cols] &lt;- lapply(house_price_data[chr_cols], factor)\n\n# Exhaustive search over all subsets (up to 13 variables)\nregfit.full &lt;- regsubsets(price ~ ., data = house_price_data, nvmax = 13)\nreg.summary &lt;- summary(regfit.full)\n\n# Inspect what is available\nnames(reg.summary)\n\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n\nCode\nreg.summary$rsq[1:13]\n\n\n [1] 0.2872932 0.4650855 0.5446850 0.5815220 0.6149073 0.6365979 0.6520416\n [8] 0.6634325 0.6721634 0.6771562 0.6802180 0.6817071 0.6818018\n\n\nCode\nhead(reg.summary$which, 10)\n\n\n   (Intercept) area bedrooms bathrooms stories mainroadyes guestroomyes\n1         TRUE TRUE    FALSE     FALSE   FALSE       FALSE        FALSE\n2         TRUE TRUE    FALSE      TRUE   FALSE       FALSE        FALSE\n3         TRUE TRUE    FALSE      TRUE   FALSE       FALSE        FALSE\n4         TRUE TRUE    FALSE      TRUE    TRUE       FALSE        FALSE\n5         TRUE TRUE    FALSE      TRUE    TRUE       FALSE        FALSE\n6         TRUE TRUE    FALSE      TRUE    TRUE       FALSE        FALSE\n7         TRUE TRUE    FALSE      TRUE    TRUE       FALSE        FALSE\n8         TRUE TRUE    FALSE      TRUE    TRUE       FALSE        FALSE\n9         TRUE TRUE    FALSE      TRUE    TRUE       FALSE        FALSE\n10        TRUE TRUE    FALSE      TRUE    TRUE        TRUE        FALSE\n   basementyes hotwaterheatingyes airconditioningyes parking prefareayes\n1        FALSE              FALSE              FALSE   FALSE       FALSE\n2        FALSE              FALSE              FALSE   FALSE       FALSE\n3        FALSE              FALSE               TRUE   FALSE       FALSE\n4        FALSE              FALSE               TRUE   FALSE       FALSE\n5        FALSE              FALSE               TRUE   FALSE        TRUE\n6        FALSE              FALSE               TRUE    TRUE        TRUE\n7         TRUE              FALSE               TRUE    TRUE        TRUE\n8         TRUE              FALSE               TRUE    TRUE        TRUE\n9         TRUE               TRUE               TRUE    TRUE        TRUE\n10        TRUE               TRUE               TRUE    TRUE        TRUE\n   furnishingstatussemi-furnished furnishingstatusunfurnished\n1                           FALSE                       FALSE\n2                           FALSE                       FALSE\n3                           FALSE                       FALSE\n4                           FALSE                       FALSE\n5                           FALSE                       FALSE\n6                           FALSE                       FALSE\n7                           FALSE                       FALSE\n8                           FALSE                        TRUE\n9                           FALSE                        TRUE\n10                          FALSE                        TRUE\n\n\nCode\n# Selection plots\npar(mfrow = c(2,2))\nplot(reg.summary$rss, xlab = \"Number of Variables\", ylab = \"RSS\", type = \"l\")\ntitle(\"Training RSS (monotonically decreases)\")\n\nplot(reg.summary$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted R^2\", type = \"l\")\nbest.adjr2 &lt;- which.max(reg.summary$adjr2)\npoints(best.adjr2, reg.summary$adjr2[best.adjr2], col = \"red\", cex = 2, pch = 20)\ntitle(\"Adjusted R^2 (maximize)\")\n\nplot(reg.summary$cp, xlab = \"Number of Variables\", ylab = \"Cp\", type = \"l\")\nbest.cp &lt;- which.min(reg.summary$cp)\npoints(best.cp, reg.summary$cp[best.cp], col = \"red\", cex = 2, pch = 20)\ntitle(\"Mallows Cp (minimize)\")\n\nplot(reg.summary$bic, xlab = \"Number of Variables\", ylab = \"BIC\", type = \"l\")\nbest.bic &lt;- which.min(reg.summary$bic)\npoints(best.bic, reg.summary$bic[best.bic], col = \"red\", cex = 2, pch = 20)\ntitle(\"BIC (minimize)\")\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))\n\ncat(\"Optimal model sizes (by criterion):\\n\")\n\n\nOptimal model sizes (by criterion):\n\n\nCode\ncat(\"  Adjusted R^2:\", best.adjr2, \"variables\\n\")\n\n\n  Adjusted R^2: 12 variables\n\n\nCode\ncat(\"  Cp:          \", best.cp, \"variables\\n\")\n\n\n  Cp:           12 variables\n\n\nCode\ncat(\"  BIC:         \", best.bic, \"variables\\n\\n\")\n\n\n  BIC:          10 variables\n\n\nCode\ncat(\"Note: BIC typically favours more parsimonious models.\\n\")\n\n\nNote: BIC typically favours more parsimonious models.\n\n\nCode\n# Coefficients of the BIC-optimal model (size = best.bic)\nbic_coefs &lt;- coef(regfit.full, best.bic)\nbic_coefs\n\n\n                (Intercept)                        area \n                196289.1411                    252.1058 \n                  bathrooms                     stories \n               1034173.9440                 494356.1424 \n                mainroadyes                 basementyes \n                406815.3534                 459481.4981 \n         hotwaterheatingyes          airconditioningyes \n                859533.5760                 885120.3631 \n                    parking                 prefareayes \n                277731.5878                 667628.2774 \nfurnishingstatusunfurnished \n               -396086.1661 \n\n\nCode\n# For metrics, fit an lm() with the BIC-selected variables\nform_bic &lt;- price ~ area + bathrooms + stories + mainroad + basement + hotwaterheating + airconditioning + parking + prefarea + furnishingstatus\nmodel_bic &lt;- lm(form_bic, data = house_price_data)\n\n# Metrics\nadj_r2  &lt;- summary(model_bic)$adj.r.squared\nbic_val &lt;- BIC(model_bic)\nrmse    &lt;- sqrt(mean(residuals(model_bic)^2))\n\nlist(Adjusted_R2 = adj_r2, BIC = bic_val, RMSE = rmse)\n\n\n$Adjusted_R2\n[1] 0.6706188\n\n$BIC\n[1] 16752.61\n\n$RMSE\n[1] 1061594\n\n\nCode\n# Compact coefficient table\ncoef_table &lt;- data.frame(term = names(coef(model_bic)),\n                         estimate = as.numeric(coef(model_bic)))\ncoef_table\n\n\n                             term     estimate\n1                     (Intercept)  238328.2698\n2                            area     251.7261\n3                       bathrooms 1033284.6065\n4                         stories  493842.4982\n5                     mainroadyes  403925.7767\n6                     basementyes  459016.8636\n7              hotwaterheatingyes  861677.3128\n8              airconditioningyes  880534.6004\n9                         parking  277045.4515\n10                    prefareayes  665796.1037\n11 furnishingstatussemi-furnished  -52738.1140\n12    furnishingstatusunfurnished -430216.1525",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Solution to In-class Sup3</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup3.html#answers-concise",
    "href": "solution_formative_test_sup3.html#answers-concise",
    "title": "Solution to In-class Sup3",
    "section": "Answers (concise)",
    "text": "Answers (concise)\n\n1) Best model equation\nUsing BIC, the selected model is:\n[ ~ + + + + + + + + + ]\nCoefficient snapshot (from subset selection):\n(Intercept) area , 196289.1411 252.1058 , bathrooms stories , 1034173.9440 494356.1424 , mainroadyes basementyes , 406815.3534 459481.4981 , hotwaterheatingyes airconditioningyes , 859533.5760 885120.3631 , parking prefareayes , 277731.5878 667628.2774 , furnishingstatusunfurnished , -396086.1661\n\n\n\n2) Model metrics\nFrom the fitted BIC-selected model:\n\nAdjusted RÂ²: 0.671\n\nBIC: 1.67526^{4}\n\nRMSE: 1.061594^{6}\n\nNote: The maximum Adjusted RÂ² across all sizes occurs around 12 variables (â‰ˆ 0.675), but BIC favours the 10-variable model above for parsimony.\n\n\n\n3) Interpretation of a key variable\nConsider area. Holding other factors constant, the estimated coefficient on area is approximately 252.1. This implies that each additional unit of floor area increases the predicted price by about that amount, reflecting the strong and intuitive role of size in property valuation. The magnitude is economically plausible and aligns with the positive effects seen for amenities (e.g., airconditioning, prefarea).\n\n\n\n4) Economic sense?\nYes. Larger homes (area), additional bathrooms and stories, better amenities (air conditioning, hot-water heating), and desirable location (prefarea, access to mainroad) all increase predicted prices, while being unfurnished reduces priceâ€”precisely what standard housing market theory would anticipate. The model therefore captures realistic drivers of housing values while balancing fit and simplicity via BIC.",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Solution to In-class Sup3</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html",
    "href": "solution_formative_test_sup4.html",
    "title": "Solution to inâ€‘class Sup 4",
    "section": "",
    "text": "Overview\nThis document reproduces the analysis with concise # comments inside each R chunk so students see why each step is taken.\nWork flow: run baseline model 1 &gt; VIF test &gt; fix model if high multicolinearity &gt; Test autocorrelation from previous step &gt; fix the model if high autocorrelation &gt; Test for heteroskedasticity &gt; fix the model if high heteroskedasticity.",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html#packages",
    "href": "solution_formative_test_sup4.html#packages",
    "title": "Solution to inâ€‘class Sup 4",
    "section": "0) Packages",
    "text": "0) Packages\n\n\nCode\n# Minimal package set for this workflow\npkgs &lt;- c(\"PoEdata\",\"car\",\"lmtest\",\"sandwich\",\"dplyr\",\"zoo\")\n\n# Install any missing packages (safe check)\nto_install &lt;- pkgs[!sapply(pkgs, requireNamespace, quietly = TRUE)]\nif (length(to_install)) install.packages(to_install, repos = \"https://cloud.r-project.org\")\n\n# Load libraries\nlibrary(PoEdata)     # provides 'rice' dataset\nlibrary(car)         # VIF for multicollinearity\nlibrary(lmtest)      # dwtest, bgtest, bptest\nlibrary(sandwich)    # robust variance estimators (HC)\nlibrary(dplyr)       # arrange, mutate, lag",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html#data",
    "href": "solution_formative_test_sup4.html#data",
    "title": "Solution to inâ€‘class Sup 4",
    "section": "1) Data",
    "text": "1) Data\n\n\nCode\n# Load and time-order the data\ndata(rice)                                  # bring 'rice' into the workspace\nrice &lt;- rice[order(rice$year), ]            # ensure time order for lags and DW/BG tests\n\n# Baseline production function\nform &lt;- prod ~ area + fert + labor + firm   # 'firm' is an identifier; expect little role",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html#baseline-ols",
    "href": "solution_formative_test_sup4.html#baseline-ols",
    "title": "Solution to inâ€‘class Sup 4",
    "section": "2) Baseline OLS",
    "text": "2) Baseline OLS\n\n\nCode\n# Estimate baseline OLS\nm &lt;- lm(form, data = rice)\n\n# Inspect fit and signs/magnitudes\ncat(\"\\n=== OLS summary ===\\n\"); print(summary(m))\n\n\n\n=== OLS summary ===\n\n\n\nCall:\nlm(formula = form, data = rice)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2718 -0.9266  0.0356  0.9459  8.9075 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.108138   0.318994  -0.339 0.734818    \narea         1.321087   0.209052   6.319 8.06e-10 ***\nfert         0.004583   0.001292   3.547 0.000442 ***\nlabor        0.027476   0.003911   7.026 1.13e-11 ***\nfirm        -0.001137   0.009158  -0.124 0.901232    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.109 on 347 degrees of freedom\nMultiple R-squared:  0.8294,    Adjusted R-squared:  0.8275 \nF-statistic: 421.8 on 4 and 347 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html#multicollinearity-vif",
    "href": "solution_formative_test_sup4.html#multicollinearity-vif",
    "title": "Solution to inâ€‘class Sup 4",
    "section": "3) Multicollinearity (VIF)",
    "text": "3) Multicollinearity (VIF)\n\n\nCode\n# Diagnose collinearity: rule of thumb VIF &gt; 5 (moderate), &gt; 10 (high)\ncat(\"\\n=== Multicollinearity (VIF) ===\\n\"); print(vif(m))\n\n\n\n=== Multicollinearity (VIF) ===\n\n\n    area     fert    labor     firm \n7.266675 3.744241 7.090750 1.070505",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html#reduce-collinearity-via-transformation",
    "href": "solution_formative_test_sup4.html#reduce-collinearity-via-transformation",
    "title": "Solution to inâ€‘class Sup 4",
    "section": "4) Reduce collinearity via transformation",
    "text": "4) Reduce collinearity via transformation\n\n\nCode\n# Construct labor per area to reduce collinearity between labor and area\nrice$labor_density &lt;- rice$labor / rice$area   # density transformation\n\n# Refit a more parsimonious model (drop 'firm' id)\nm2 &lt;- lm(prod ~ area + fert + labor_density, data = rice)\n\n# Check VIFs fall meaningfully\ncat(\"\\n=== VIF after transformation (m2) ===\\n\"); print(vif(m2))\n\n\n\n=== VIF after transformation (m2) ===\n\n\n         area          fert labor_density \n     3.739741      3.606256      1.098055 \n\n\nCode\n# Compare fit (R^2) and significance with baseline\ncat(\"\\n=== OLS summary (m2) ===\\n\"); print(summary(m2))\n\n\n\n=== OLS summary (m2) ===\n\n\n\nCall:\nlm(formula = prod ~ area + fert + labor_density, data = rice)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.7160 -0.9525  0.0347  1.0138  9.2159 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -1.794885   0.500123  -3.589  0.00038 ***\narea           2.581396   0.156507  16.494  &lt; 2e-16 ***\nfert           0.006033   0.001323   4.560 7.10e-06 ***\nlabor_density  0.031629   0.007850   4.029 6.87e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.201 on 348 degrees of freedom\nMultiple R-squared:  0.8137,    Adjusted R-squared:  0.8121 \nF-statistic: 506.6 on 3 and 348 DF,  p-value: &lt; 2.2e-16\n\n\nAnswer (VIF): Using a VIF threshold of 5 (NOte 5â€“10 considered moderate, &gt;10 high), the baseline model reports VIFs of area = 7.27, fert = 3.74, and labor = 7.06, indicating moderate multicollinearity for area and labor. The source is conceptual overlap between cultivated area and the scale of labor. Remedy: I replace labor with labor_density = labor/area to orthogonalise scale and intensity. In the revised model, VIFs fall to area = 3.74, fert = 3.61, labor_density = 1.10, which are comfortably below the threshold.",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html#a-autocorrelation-diagnostics",
    "href": "solution_formative_test_sup4.html#a-autocorrelation-diagnostics",
    "title": "Solution to inâ€‘class Sup 4",
    "section": "4a) Autocorrelation diagnostics",
    "text": "4a) Autocorrelation diagnostics\n\n\nCode\n# Durbinâ€“Watson (focus on AR(1) positive autocorrelation under default alt)\ncat(\"\\n=== Autocorrelation on m2 ===\\n\")\n\n\n\n=== Autocorrelation on m2 ===\n\n\nCode\ncat(\"Durbinâ€“Watson:\\n\"); print(dwtest(m2))\n\n\nDurbinâ€“Watson:\n\n\n\n    Durbin-Watson test\n\ndata:  m2\nDW = 1.7352, p-value = 0.005977\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nCode\n# Breuschâ€“Godfrey (general serial correlation; here AR(1) for comparability)\ncat(\"\\nBreuschâ€“Godfrey (AR1):\\n\"); print(bgtest(m2, order = 1))\n\n\n\nBreuschâ€“Godfrey (AR1):\n\n\n\n    Breusch-Godfrey test for serial correlation of order up to 1\n\ndata:  m2\nLM test = 6.1838, df = 1, p-value = 0.01289\n\n\nCode\n# Note: p &lt; 0.05 indicates evidence of autocorrelation\n\n\nAnswer (autocorrelation): The initial diagnostics on the revised level model show positive serial correlation: DW â‰ˆ 1.74 (p â‰ˆ 0.006) and BG(1) p â‰ˆ 0.013. The ACF/PACF of residuals suggest an AR(1) pattern with some evidence consistent with possible AR(2) dynamics. I therefore consider dynamic specifications and a logâ€“log transformation (which often stabilises variance and improves dynamics).\n\nCreate lag and plot correlograms\n\n\nCode\n# Create lag of dependent variable for dynamic model\nrice &lt;- rice |&gt;\n  arrange(year) |&gt;\n  mutate(prod_lag = dplyr::lag(prod))   # first observation becomes NA by construction\n\n# ACF/PACF of residuals (visual check)\nacf(m2$residuals, main = \"ACF of OLS residuals (m2)\")    # spikes suggest serial correlation\n\n\n\n\n\n\n\n\n\nCode\npacf(m2$residuals, main = \"PACF of OLS residuals (m2)\")\n\n\n\n\n\n\n\n\n\nCode\n# Align sample by dropping rows with NA lag\nrice_lag &lt;- na.omit(rice)               # keep complete cases for dynamic models",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html#b-dynamic-model-and-loglog-refinement",
    "href": "solution_formative_test_sup4.html#b-dynamic-model-and-loglog-refinement",
    "title": "Solution to inâ€‘class Sup 4",
    "section": "4b) Dynamic model and logâ€“log refinement",
    "text": "4b) Dynamic model and logâ€“log refinement\n\n\nCode\n# Dynamic level model: include the lagged dependent variable\nm3 &lt;- lm(prod ~ prod_lag + area + fert + labor_density, data = rice_lag)\ncat(\"\\n=== Dynamic model in levels (m3) ===\\n\"); print(summary(m3))\n\n\n\n=== Dynamic model in levels (m3) ===\n\n\n\nCall:\nlm(formula = prod ~ prod_lag + area + fert + labor_density, data = rice_lag)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5972 -0.9054  0.0746  0.9975  9.1886 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -1.857053   0.504532  -3.681 0.000270 ***\nprod_lag       0.025986   0.024865   1.045 0.296725    \narea           2.536997   0.162464  15.616  &lt; 2e-16 ***\nfert           0.006149   0.001330   4.624 5.33e-06 ***\nlabor_density  0.030997   0.007892   3.928 0.000104 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.204 on 346 degrees of freedom\nMultiple R-squared:  0.8142,    Adjusted R-squared:  0.8121 \nF-statistic: 379.2 on 4 and 346 DF,  p-value: &lt; 2.2e-16\n\n\nCode\ncat(\"\\nDW on m3:\\n\"); print(dwtest(m3))\n\n\n\nDW on m3:\n\n\n\n    Durbin-Watson test\n\ndata:  m3\nDW = 1.7778, p-value = 0.01758\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nCode\ncat(\"\\nBG(AR1) on m3:\\n\"); print(bgtest(m3, order = 1))\n\n\n\nBG(AR1) on m3:\n\n\n\n    Breusch-Godfrey test for serial correlation of order up to 1\n\ndata:  m3\nLM test = 5.1696, df = 1, p-value = 0.02299\n\n\nCode\n# Logâ€“log specification: interpretable elasticities, often stabilizes variance\nbest &lt;- lm(log(prod) ~ log(prod_lag) + log(area) + log(fert) + log(labor_density),\n           data = rice_lag)\n\n# Multicollinearity check in final model\ncat(\"\\n=== VIF (final logâ€“log model) ===\\n\"); print(vif(best))\n\n\n\n=== VIF (final logâ€“log model) ===\n\n\n     log(prod_lag)          log(area)          log(fert) log(labor_density) \n          1.115383           4.698116           4.274338           1.221126 \n\n\nCode\n# Final model summary: coefficients are elasticities\ncat(\"\\n=== Final model (logâ€“log) summary ===\\n\"); print(summary(best))\n\n\n\n=== Final model (logâ€“log) summary ===\n\n\n\nCall:\nlm(formula = log(prod) ~ log(prod_lag) + log(area) + log(fert) + \n    log(labor_density), data = rice_lag)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.74604 -0.19002  0.05003  0.22904  1.40457 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        -1.47819    0.25611  -5.772 1.74e-08 ***\nlog(prod_lag)      -0.05305    0.02197  -2.414   0.0163 *  \nlog(area)           0.82324    0.04947  16.640  &lt; 2e-16 ***\nlog(fert)           0.20008    0.03826   5.230 2.94e-07 ***\nlog(labor_density)  0.44431    0.06676   6.655 1.11e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3396 on 346 degrees of freedom\nMultiple R-squared:  0.8502,    Adjusted R-squared:  0.8485 \nF-statistic: 491.1 on 4 and 346 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Autocorrelation checks on final model (expect no serial correlation)\ncat(\"\\nDW on final model:\\n\"); print(dwtest(best))\n\n\n\nDW on final model:\n\n\n\n    Durbin-Watson test\n\ndata:  best\nDW = 2.0162, p-value = 0.5545\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nCode\ncat(\"\\nBG(AR1) on final model:\\n\"); print(bgtest(best, order = 1))\n\n\n\nBG(AR1) on final model:\n\n\n\n    Breusch-Godfrey test for serial correlation of order up to 1\n\ndata:  best\nLM test = 0.048045, df = 1, p-value = 0.8265\n\n\nAnswer (autocorrelation â€“ final model): In the preferred logâ€“log model with log(prod_lag), both tests indicate no residual serial correlation: DW â‰ˆ 2.02 (p â‰ˆ 0.55) and BG(1) p â‰ˆ 0.83. This, together with acceptable VIFs (â‰¤~4.7), supports using the logâ€“log model for inference and interpretation as elasticities.",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html#heteroskedasticity-tests",
    "href": "solution_formative_test_sup4.html#heteroskedasticity-tests",
    "title": "Solution to inâ€‘class Sup 4",
    "section": "5) Heteroskedasticity tests",
    "text": "5) Heteroskedasticity tests\n\n\nCode\n# Breuschâ€“Pagan: H0 = homoskedasticity\ncat(\"\\n=== Heteroskedasticity ===\\n\")\n\n\n\n=== Heteroskedasticity ===\n\n\nCode\ncat(\"Breuschâ€“Pagan:\\n\"); print(bptest(best))\n\n\nBreuschâ€“Pagan:\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  best\nBP = 20.065, df = 4, p-value = 0.0004848\n\n\nCode\n# White-type (auxiliary regression on fitted and fitted^2)\ncat(\"\\nWhite-type (fitted & fitted^2):\\n\")\n\n\n\nWhite-type (fitted & fitted^2):\n\n\nCode\nprint(bptest(best, ~ fitted(best) + I(fitted(best)^2), data = rice_lag))\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  best\nBP = 24.196, df = 2, p-value = 5.572e-06",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html#robust-inference-hc1",
    "href": "solution_formative_test_sup4.html#robust-inference-hc1",
    "title": "Solution to inâ€‘class Sup 4",
    "section": "6) Robust inference (HC1)",
    "text": "6) Robust inference (HC1)\n\n\nCode\n# Use heteroskedasticity-consistent (HC1) standard errors for inference\n# HAC is not needed because DW/BG suggest no serial correlation in the final model.\ncat(\"\\n=== Coefficients with robust (HC1) SE ===\\n\")\n\n\n\n=== Coefficients with robust (HC1) SE ===\n\n\nCode\nprint(lmtest::coeftest(best, vcov = sandwich::vcovHC(best, type = \"HC1\")))\n\n\n\nt test of coefficients:\n\n                    Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)        -1.478192   0.351465 -4.2058 3.317e-05 ***\nlog(prod_lag)      -0.053045   0.023119 -2.2944  0.022363 *  \nlog(area)           0.823241   0.079032 10.4166 &lt; 2.2e-16 ***\nlog(fert)           0.200076   0.066501  3.0086  0.002817 ** \nlog(labor_density)  0.444306   0.089841  4.9455 1.187e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\ncat(\"\\n=== Coefficients with HAC (Neweyâ€“West, lag = 1) ===\\n\")\n\n\n\n=== Coefficients with HAC (Neweyâ€“West, lag = 1) ===\n\n\nCode\nprint(lmtest::coeftest(best, vcov. = sandwich::NeweyWest(best, lag = 1, prewhite = FALSE)))\n\n\n\nt test of coefficients:\n\n                    Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)        -1.478192   0.331960 -4.4529 1.145e-05 ***\nlog(prod_lag)      -0.053045   0.024527 -2.1627  0.031249 *  \nlog(area)           0.823241   0.077409 10.6350 &lt; 2.2e-16 ***\nlog(fert)           0.200076   0.065352  3.0615  0.002375 ** \nlog(labor_density)  0.444306   0.085679  5.1857 3.670e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnswer (heteroskedasticity & inference): Both BP and White-type tests reject homoskedasticity, so I report HC1 and also HAC (Neweyâ€“West) inference. Comparing pâ€‘values across classical vs HC1 vs HAC, the significance pattern is robust: at the 5% level, log(area), log(fert), and log(labor_density) remain significant; log(prod_lag) also remains significant at â‰ˆ5% (small negative carryover). Hence, substantive conclusions are unchanged by robust/HAC corrections.",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html#short-interpretation-notes-for-students",
    "href": "solution_formative_test_sup4.html#short-interpretation-notes-for-students",
    "title": "Solution to inâ€‘class Sup 4",
    "section": "Short interpretation notes (for students)",
    "text": "Short interpretation notes (for students)\n\nCollinearity: Replacing labor with labor_density reduced VIFs (improves stability of estimates).\nAutocorrelation: Final logâ€“log model (best) passes DW/BG (no serial correlation).\nHeteroskedasticity: BP/White indicate non-constant variance â†’ report robust SEs.\nElasticities: In the logâ€“log model, coefficients read as % changes (e.g., 0.82 on log(area) â‰ˆ 1% â†‘ area â†’ 0.82% â†‘ output, ceteris paribus).\n\nFinal report: The preferred specification is a logâ€“log production function with a dynamic term: \\(log(\\text{prod}_t) = \\alpha + \\beta1 \\log(\\text{prod}{t-1}) + \\beta_2 \\log(\\text{area}_t) + \\beta_3 \\log(\\text{fert}_t) +\\) \\(\\beta_4 \\log(\\text{labor/area}t) + u_t\\)\nDiagnostics follow textbook order. Multicollinearity: baseline VIFs were moderate (area â‰ˆ 7.27, labor â‰ˆ 7.06), so I replaced labor with labor density, reducing VIFs to â‰¤~3.74. Autocorrelation: the revised level model exhibited positive AR(1) serial correlation (DW â‰ˆ 1.74; BG p â‰ˆ 0.013; ACF consistent with AR(1) and possible AR(2)), which motivated adding a lagged dependent variable and moving to a logâ€“log form. In the final model, DW â‰ˆ 2.02 and BG(1) p â‰ˆ 0.83 indicate no remaining autocorrelation. Heteroskedasticity: both BP and White-type tests reject homoskedasticity, so I report HC1 and HAC (Neweyâ€“West) standard errors; inference is unchanged. Interpretation: Coefficients are elasticities. Output elasticity w.r.t. cultivated area is â‰ˆ0.82, fertiliser â‰ˆ0.20, and labor density â‰ˆ0.44: a 1% rise in each raises output by roughly 0.82%, 0.20%, and 0.44%, respectively. The small negative coefficient on (\\(\\log(\\text{prod}{t-1}\\))) suggests modest mean reversion after shocks. Overall, the transformed specification is wellâ€‘behaved and economically interpretable, with robust inference to heteroskedasticity and HAC corrections.",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_task_sup5.html",
    "href": "solution_formative_task_sup5.html",
    "title": "Solution to inâ€‘class Sup 5",
    "section": "",
    "text": "ğŸ¯ Purpose\nThis handout explains what diagnostics matter in hedonic pricing models, why we use HC1 robust standard errors, and why autocorrelation/HAC are not relevant for our cross-sectional housing dataset. It also gives you a clear workflow and short code snippets you can reuse.",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 5</span>"
    ]
  },
  {
    "objectID": "solution_formative_task_sup5.html#workflow-for-crosssectional-hedonic-models",
    "href": "solution_formative_task_sup5.html#workflow-for-crosssectional-hedonic-models",
    "title": "Solution to inâ€‘class Sup 5",
    "section": "ğŸ§­ Workflow for Crossâ€‘Sectional Hedonic Models",
    "text": "ğŸ§­ Workflow for Crossâ€‘Sectional Hedonic Models\n\n\n  Import & explore the data (summary, histograms, missing values).\n  Select variables using economic reasoning (see Supervision 3).\n  Estimate the model with lm() (log-price is standard).\n  Diagnose assumptions:\n    \n      Multicollinearity â†’ VIF\n      Heteroskedasticity â†’ White test (Breuschâ€“Pagan)\n      Optional (time/panel only): Autocorrelation (BG test)\n    \n  \n  Correct inference with HC1 robust SEs (crossâ€‘section default).\n  Visual checks: residuals vs fitted, normal Qâ€“Q.\n  Interpret coefficients and tell the economic story.\n\n\n\nKey takeaway: Hedonic models here are crossâ€‘sectional. We use HC1 robust SEs. HAC/Neweyâ€“West and autocorrelation tests are for time series or panel data, not for this dataset.",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 5</span>"
    ]
  },
  {
    "objectID": "solution_formative_task_sup5.html#diagnostics-that-matter-crosssection",
    "href": "solution_formative_task_sup5.html#diagnostics-that-matter-crosssection",
    "title": "Solution to inâ€‘class Sup 5",
    "section": "ğŸ” Diagnostics That Matter (Crossâ€‘Section)",
    "text": "ğŸ” Diagnostics That Matter (Crossâ€‘Section)\n\nA) Multicollinearity â€” VIF\nMany house attributes are correlated (area, bedrooms, baths).\nRules of thumb: VIF &lt; 5 usually fine; 5â€“10 investigate; &gt;10 consider removing/reâ€‘coding or combining variables.\n\n\nB) Heteroskedasticity â€” White/Breuschâ€“Pagan\nCross-sectional housing price data almost always violates homoskedasticity.",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 5</span>"
    ]
  },
  {
    "objectID": "solution_formative_task_sup5.html#correct-inference-hc1-robust-standard-errors",
    "href": "solution_formative_task_sup5.html#correct-inference-hc1-robust-standard-errors",
    "title": "Solution to inâ€‘class Sup 5",
    "section": "ğŸ§® Correct Inference â€” HC1 Robust Standard Errors",
    "text": "ğŸ§® Correct Inference â€” HC1 Robust Standard Errors\nHC1 is consistent for cross-sectional data. HAC is consistent for time series.\n\nWe do not use HAC/Neweyâ€“West in this exercise because there is no time ordering in crossâ€‘sectional data.",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 5</span>"
    ]
  },
  {
    "objectID": "solution_formative_task_sup5.html#visual-diagnostics-good-academic-practice",
    "href": "solution_formative_task_sup5.html#visual-diagnostics-good-academic-practice",
    "title": "Solution to inâ€‘class Sup 5",
    "section": "ğŸ‘€ Visual Diagnostics (Good Academic Practice)",
    "text": "ğŸ‘€ Visual Diagnostics (Good Academic Practice)\npar(mfrow = c(1, 2))\nplot(m0, which = 1)  # Residuals vs Fitted (linearity & variance)\nplot(m0, which = 2)  # Normal Qâ€“Q (error distribution)\nInterpretation hints: - Left plot: no curve or funnel â†’ linearity & constant variance ok; patterns â†’ consider transforms or interactions. - Right plot: points near line â†’ residuals roughly normal; big tails â†’ consider log transforms or outlier checks.",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 5</span>"
    ]
  },
  {
    "objectID": "solution_formative_task_sup5.html#optional-variable-selection-link-to-supervision-3",
    "href": "solution_formative_task_sup5.html#optional-variable-selection-link-to-supervision-3",
    "title": "Solution to inâ€‘class Sup 5",
    "section": "(Optional) Variable Selection (Link to Supervision 3)",
    "text": "(Optional) Variable Selection (Link to Supervision 3)\nUse only if multicollinearity is high or the model is unwieldy.\n\nAlways justify inclusion/exclusion with economic reasoning, not just AIC.",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 5</span>"
    ]
  },
  {
    "objectID": "solution_formative_task_sup5.html#optional-for-timepanel-data-autocorrelation",
    "href": "solution_formative_task_sup5.html#optional-for-timepanel-data-autocorrelation",
    "title": "Solution to inâ€‘class Sup 5",
    "section": "(Optional, for Time/Panel Data) Autocorrelation",
    "text": "(Optional, for Time/Panel Data) Autocorrelation",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 5</span>"
    ]
  },
  {
    "objectID": "solution_formative_task_sup5.html#not-used-for-this-crosssection.-shown-only-for-comparison-with-timeseries-workflows.",
    "href": "solution_formative_task_sup5.html#not-used-for-this-crosssection.-shown-only-for-comparison-with-timeseries-workflows.",
    "title": "Solution to inâ€‘class Sup 5",
    "section": "Not used for this crossâ€‘section. Shown only for comparison with timeâ€‘series workflows.",
    "text": "Not used for this crossâ€‘section. Shown only for comparison with timeâ€‘series workflows.",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 5</span>"
    ]
  },
  {
    "objectID": "solution_formative_task_sup5.html#key-takeaways",
    "href": "solution_formative_task_sup5.html#key-takeaways",
    "title": "Solution to inâ€‘class Sup 5",
    "section": "âœ… Key Takeaways",
    "text": "âœ… Key Takeaways\n\nThis dataset is crossâ€‘sectional â†’ HC1 robust SEs are appropriate.\nFocus diagnostics on VIF and White test, plus residual visuals.\nAutocorrelation tests and HAC are for time/panel data and are not used here.\nCombine theory and evidence (Sup 3 + Sup 4) to justify your final model.",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 5</span>"
    ]
  },
  {
    "objectID": "solution_formative_task_sup5.html#r-script",
    "href": "solution_formative_task_sup5.html#r-script",
    "title": "Solution to inâ€‘class Sup 5",
    "section": "R script",
    "text": "R script\n\n# ---------------------------------------------------------\n# SUPERVISION 5 â€” INDICATIVE SOLUTION (OLS ONLY)\n# Hedonic Pricing Model for Ames Housing Dataset\n# ---------------------------------------------------------\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(car)\nlibrary(lmtest)\nlibrary(sandwich)\nlibrary(leaps)\nlibrary(modelsummary)\n\n# ---------------------------------------------------------\n# 1. Load & Prepare Data\n# ---------------------------------------------------------\n\ndf &lt;- read_csv(\"hedonic_ames.csv\") %&gt;%\n  mutate(\n    ln_price  = log(sale_price),\n    ln_area   = log(gr_liv_area),\n    baths_tot = full_bath + 0.5 * half_bath,\n    overall_qual = factor(overall_qual, ordered = TRUE),\n    overall_cond = factor(overall_cond, ordered = TRUE),\n    exter_qual   = factor(exter_qual, ordered = TRUE),\n    kitchen_qual = factor(kitchen_qual, ordered = TRUE),\n    fireplace_qu = factor(fireplace_qu, ordered = TRUE),\n    neighborhood = factor(neighborhood)\n  )\n\n# ---------------------------------------------------------\n# 2. Baseline OLS Model (structural attributes)\n# ---------------------------------------------------------\n\nm1 &lt;- lm(\n  ln_price ~ ln_area + bedroom_abv_gr + baths_tot +\n    total_bsmt_sf + garage_cars,\n  data = df\n)\n\n# ---------------------------------------------------------\n# Multicollinearity: VIF for baseline model\n# ---------------------------------------------------------\n\nlibrary(car)\n\nvif_m1 &lt;- vif(m1)\nvif_m1\n\n       ln_area bedroom_abv_gr      baths_tot  total_bsmt_sf    garage_cars \n      3.155534       1.530198       2.333348       1.399979       1.629202 \n\nmean(vif_m1)  # average VIF\n\n[1] 2.009652\n\nmax(vif_m1)   # worst VIF\n\n[1] 3.155534\n\n# ---------------------------------------------------------\n# 3. Add quality variables (OLS with factors)\n# ---------------------------------------------------------\n\nm2 &lt;- lm(\n  ln_price ~ ln_area + bedroom_abv_gr + baths_tot +\n    total_bsmt_sf + garage_cars +\n    overall_qual + exter_qual + kitchen_qual + fireplace_qu,\n  data = df\n)\n\n#We computed VIFs for the main structural variables.\n#Common rules-of-thumb treat VIF values above 5â€“10 as indicating \n#serious multicollinearity.\n#In our case, VIFs are moderate, so multicollinearity does not appear\n#to be a major threat to inference, although some correlation between\n#size-related variables (e.g. floor area, basement area, \n#garage capacity) is expected.\n\n# ---------------------------------------------------------\n# 4. Add neighborhood dummies (still OLS)\n# ---------------------------------------------------------\n\nm3 &lt;- lm(\n  ln_price ~ ln_area + bedroom_abv_gr + baths_tot +\n    total_bsmt_sf + garage_cars +\n    overall_qual + exter_qual + kitchen_qual + fireplace_qu +\n    neighborhood,\n  data = df\n)\n\n# ---------------------------------------------------------\n# 5. Best Subset Selection using BIC (numeric variables only)\n# ---------------------------------------------------------\n\nnum_df &lt;- df %&gt;%\n  select(ln_price, ln_area, bedroom_abv_gr, baths_tot,\n         total_bsmt_sf, garage_cars, age, remod_age)\n\nbest &lt;- regsubsets(\n  ln_price ~ ., data = num_df,\n  nvmax = ncol(num_df)-1\n)\n\nbest_summary &lt;- summary(best)\nbic_best &lt;- which.min(best_summary$bic)\n\nselected_vars &lt;- names(coef(best, bic_best))[-1]\nformula_best &lt;- as.formula(\n  paste(\"ln_price ~\", paste(selected_vars, collapse = \" + \"))\n)\n\nm4 &lt;- lm(formula_best, data = num_df)\n\n\n\n# ---------------------------------------------------------\n# 6. Robust SEs (standard OLS topic in this course)\n# 6. Tests for heteroskedasticity (H0: constant variance)\n# ---------------------------------------------------------\n\n# Breuschâ€“Pagan / White-type tests (same style as Supervision 4)\nbp_m1 &lt;- bptest(m1, varformula = ~ fitted(m1) + I(fitted(m1)^2))\nbp_m2 &lt;- bptest(m2, varformula = ~ fitted(m2) + I(fitted(m2)^2))\nbp_m3 &lt;- bptest(m3, varformula = ~ fitted(m3) + I(fitted(m3)^2))\nbp_m4 &lt;- bptest(m4, varformula = ~ fitted(m4) + I(fitted(m4)^2))\n\nbp_m1\n\n\n    studentized Breusch-Pagan test\n\ndata:  m1\nBP = 909.24, df = 2, p-value &lt; 2.2e-16\n\nbp_m2\n\n\n    studentized Breusch-Pagan test\n\ndata:  m2\nBP = 235.86, df = 2, p-value &lt; 2.2e-16\n\nbp_m3\n\n\n    studentized Breusch-Pagan test\n\ndata:  m3\nBP = 143.6, df = 2, p-value &lt; 2.2e-16\n\nbp_m4\n\n\n    studentized Breusch-Pagan test\n\ndata:  m4\nBP = 738.91, df = 2, p-value &lt; 2.2e-16\n\nrob_m1 &lt;- coeftest(m1, vcovHC(m1, type = \"HC1\"))\nrob_m2 &lt;- coeftest(m2, vcovHC(m2, type = \"HC1\"))\n#There is one very high-leverage observation (almost perfectly fitted).\n#This makes the HC1 robust covariance matrix nearly singular, \n#so robust SEs for rob_m2 model are not trustworthy\nrob_m3 &lt;- coeftest(m3, vcovHC(m3, type = \"HC1\"))\nrob_m4 &lt;- coeftest(m4, vcovHC(m4, type = \"HC1\"))\n\n# ---------------------------------------------------------\n# 7. Compare Models (shorten long coefficient names)\n# ---------------------------------------------------------\n\n\nnice_names &lt;- c(\n  \"neighborhoodSouth_and_West_of_Iowa_State_University\" = \"Nbh: ISU SW\",\n  \"neighborhoodIowa_DOT_and_Rail_Road\"                  = \"Nbh: Iowa DOT/RR\"\n)\n\n# edit variable names for a better results table\n# we shorten the \"neighborhood\" prefix globally for \"Nbh:\"\nmodelsummary(\n  list(\n    \"Structural Only\"         = m1,\n    \"Add Quality\"             = m2,\n    \"Add Neighbourhood\"       = m3,\n    \"Best BIC (Numeric Only)\" = m4\n  ),\n  statistic = \"({std.error})\",\n  gof_omit  = \"AIC|Log.Lik|RMSE\",\n  coef_rename = function(x) {\n    x &lt;- gsub(\"^neighborhood\", \"Nbh:\", x)\n    x &lt;- gsub(\"South_and_West_of_Iowa_State_University\", \"ISU SW\", x)\n    x &lt;- gsub(\"Iowa_DOT_and_Rail_Road\", \"Iowa DOT/RR\", x)\n    x\n  }\n)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Structural Only\n                Add Quality\n                Add Neighbourhood\n                Best BIC (Numeric Only)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  7.758\n                  9.387\n                  9.157\n                  7.450\n                \n                \n                  \n                  (0.128)\n                  (0.121)\n                  (0.119)\n                  (0.115)\n                \n                \n                  ln_area\n                  0.518\n                  0.303\n                  0.341\n                  0.632\n                \n                \n                  \n                  (0.021)\n                  (0.018)\n                  (0.018)\n                  (0.019)\n                \n                \n                  bedroom_abv_gr\n                  -0.071\n                  -0.012\n                  -0.013\n                  -0.037\n                \n                \n                  \n                  (0.006)\n                  (0.005)\n                  (0.005)\n                  (0.005)\n                \n                \n                  baths_tot\n                  0.097\n                  0.038\n                  0.018\n                  -0.041\n                \n                \n                  \n                  (0.009)\n                  (0.008)\n                  (0.008)\n                  (0.009)\n                \n                \n                  total_bsmt_sf\n                  0.000\n                  0.000\n                  0.000\n                  0.000\n                \n                \n                  \n                  (0.000)\n                  (0.000)\n                  (0.000)\n                  (0.000)\n                \n                \n                  garage_cars\n                  0.148\n                  0.084\n                  0.064\n                  0.096\n                \n                \n                  \n                  (0.006)\n                  (0.005)\n                  (0.005)\n                  (0.006)\n                \n                \n                  overall_qual.L\n                  \n                  -0.115\n                  -0.160\n                  \n                \n                \n                  \n                  \n                  (0.049)\n                  (0.045)\n                  \n                \n                \n                  overall_qual.Q\n                  \n                  -0.011\n                  -0.079\n                  \n                \n                \n                  \n                  \n                  (0.051)\n                  (0.047)\n                  \n                \n                \n                  overall_qual.C\n                  \n                  -0.129\n                  -0.178\n                  \n                \n                \n                  \n                  \n                  (0.048)\n                  (0.044)\n                  \n                \n                \n                  overall_qual^4\n                  \n                  -0.283\n                  -0.297\n                  \n                \n                \n                  \n                  \n                  (0.037)\n                  (0.034)\n                  \n                \n                \n                  overall_qual^5\n                  \n                  -0.401\n                  -0.383\n                  \n                \n                \n                  \n                  \n                  (0.032)\n                  (0.030)\n                  \n                \n                \n                  overall_qual^6\n                  \n                  -0.078\n                  -0.061\n                  \n                \n                \n                  \n                  \n                  (0.026)\n                  (0.024)\n                  \n                \n                \n                  overall_qual^7\n                  \n                  0.156\n                  0.146\n                  \n                \n                \n                  \n                  \n                  (0.028)\n                  (0.026)\n                  \n                \n                \n                  overall_qual^8\n                  \n                  0.015\n                  0.037\n                  \n                \n                \n                  \n                  \n                  (0.030)\n                  (0.027)\n                  \n                \n                \n                  overall_qual^9\n                  \n                  0.591\n                  0.520\n                  \n                \n                \n                  \n                  \n                  (0.030)\n                  (0.029)\n                  \n                \n                \n                  exter_qual.L\n                  \n                  -0.028\n                  -0.023\n                  \n                \n                \n                  \n                  \n                  (0.018)\n                  (0.016)\n                  \n                \n                \n                  exter_qual.Q\n                  \n                  0.103\n                  0.079\n                  \n                \n                \n                  \n                  \n                  (0.019)\n                  (0.017)\n                  \n                \n                \n                  exter_qual.C\n                  \n                  -0.147\n                  -0.090\n                  \n                \n                \n                  \n                  \n                  (0.022)\n                  (0.021)\n                  \n                \n                \n                  kitchen_qual.L\n                  \n                  -0.115\n                  -0.170\n                  \n                \n                \n                  \n                  \n                  (0.052)\n                  (0.048)\n                  \n                \n                \n                  kitchen_qual.Q\n                  \n                  0.077\n                  0.125\n                  \n                \n                \n                  \n                  \n                  (0.044)\n                  (0.040)\n                  \n                \n                \n                  kitchen_qual.C\n                  \n                  -0.010\n                  0.104\n                  \n                \n                \n                  \n                  \n                  (0.101)\n                  (0.093)\n                  \n                \n                \n                  kitchen_qual^4\n                  \n                  0.154\n                  0.223\n                  \n                \n                \n                  \n                  \n                  (0.077)\n                  (0.071)\n                  \n                \n                \n                  fireplace_qu.L\n                  \n                  -0.040\n                  -0.041\n                  \n                \n                \n                  \n                  \n                  (0.019)\n                  (0.018)\n                  \n                \n                \n                  fireplace_qu.Q\n                  \n                  0.064\n                  0.047\n                  \n                \n                \n                  \n                  \n                  (0.015)\n                  (0.014)\n                  \n                \n                \n                  fireplace_qu.C\n                  \n                  0.015\n                  0.006\n                  \n                \n                \n                  \n                  \n                  (0.018)\n                  (0.017)\n                  \n                \n                \n                  fireplace_qu^4\n                  \n                  -0.022\n                  -0.024\n                  \n                \n                \n                  \n                  \n                  (0.018)\n                  (0.017)\n                  \n                \n                \n                  fireplace_qu^5\n                  \n                  -0.041\n                  -0.034\n                  \n                \n                \n                  \n                  \n                  (0.011)\n                  (0.010)\n                  \n                \n                \n                  Nbh:Blueste\n                  \n                  \n                  -0.075\n                  \n                \n                \n                  \n                  \n                  \n                  (0.054)\n                  \n                \n                \n                  Nbh:Briardale\n                  \n                  \n                  -0.167\n                  \n                \n                \n                  \n                  \n                  \n                  (0.040)\n                  \n                \n                \n                  Nbh:Brookside\n                  \n                  \n                  -0.038\n                  \n                \n                \n                  \n                  \n                  \n                  (0.033)\n                  \n                \n                \n                  Nbh:Clear_Creek\n                  \n                  \n                  0.157\n                  \n                \n                \n                  \n                  \n                  \n                  (0.036)\n                  \n                \n                \n                  Nbh:College_Creek\n                  \n                  \n                  0.102\n                  \n                \n                \n                  \n                  \n                  \n                  (0.029)\n                  \n                \n                \n                  Nbh:Crawford\n                  \n                  \n                  0.144\n                  \n                \n                \n                  \n                  \n                  \n                  (0.032)\n                  \n                \n                \n                  Nbh:Edwards\n                  \n                  \n                  -0.038\n                  \n                \n                \n                  \n                  \n                  \n                  (0.031)\n                  \n                \n                \n                  Nbh:Gilbert\n                  \n                  \n                  0.070\n                  \n                \n                \n                  \n                  \n                  \n                  (0.030)\n                  \n                \n                \n                  Nbh:Green_Hills\n                  \n                  \n                  0.527\n                  \n                \n                \n                  \n                  \n                  \n                  (0.106)\n                  \n                \n                \n                  Nbh:Greens\n                  \n                  \n                  0.031\n                  \n                \n                \n                  \n                  \n                  \n                  (0.059)\n                  \n                \n                \n                  Nbh:Iowa DOT/RR\n                  \n                  \n                  -0.142\n                  \n                \n                \n                  \n                  \n                  \n                  (0.034)\n                  \n                \n                \n                  Nbh:Landmark\n                  \n                  \n                  -0.032\n                  \n                \n                \n                  \n                  \n                  \n                  (0.147)\n                  \n                \n                \n                  Nbh:Meadow_Village\n                  \n                  \n                  -0.127\n                  \n                \n                \n                  \n                  \n                  \n                  (0.039)\n                  \n                \n                \n                  Nbh:Mitchell\n                  \n                  \n                  0.081\n                  \n                \n                \n                  \n                  \n                  \n                  (0.032)\n                  \n                \n                \n                  Nbh:North_Ames\n                  \n                  \n                  0.035\n                  \n                \n                \n                  \n                  \n                  \n                  (0.030)\n                  \n                \n                \n                  Nbh:Northpark_Villa\n                  \n                  \n                  -0.059\n                  \n                \n                \n                  \n                  \n                  \n                  (0.042)\n                  \n                \n                \n                  Nbh:Northridge\n                  \n                  \n                  0.189\n                  \n                \n                \n                  \n                  \n                  \n                  (0.034)\n                  \n                \n                \n                  Nbh:Northridge_Heights\n                  \n                  \n                  0.142\n                  \n                \n                \n                  \n                  \n                  \n                  (0.031)\n                  \n                \n                \n                  Nbh:Northwest_Ames\n                  \n                  \n                  0.051\n                  \n                \n                \n                  \n                  \n                  \n                  (0.031)\n                  \n                \n                \n                  Nbh:Old_Town\n                  \n                  \n                  -0.113\n                  \n                \n                \n                  \n                  \n                  \n                  (0.031)\n                  \n                \n                \n                  Nbh:Sawyer\n                  \n                  \n                  0.043\n                  \n                \n                \n                  \n                  \n                  \n                  (0.032)\n                  \n                \n                \n                  Nbh:Sawyer_West\n                  \n                  \n                  0.043\n                  \n                \n                \n                  \n                  \n                  \n                  (0.031)\n                  \n                \n                \n                  Nbh:Somerset\n                  \n                  \n                  0.106\n                  \n                \n                \n                  \n                  \n                  \n                  (0.030)\n                  \n                \n                \n                  Nbh:ISU SW\n                  \n                  \n                  -0.050\n                  \n                \n                \n                  \n                  \n                  \n                  (0.037)\n                  \n                \n                \n                  Nbh:Stone_Brook\n                  \n                  \n                  0.182\n                  \n                \n                \n                  \n                  \n                  \n                  (0.035)\n                  \n                \n                \n                  Nbh:Timberland\n                  \n                  \n                  0.122\n                  \n                \n                \n                  \n                  \n                  \n                  (0.033)\n                  \n                \n                \n                  Nbh:Veenker\n                  \n                  \n                  0.120\n                  \n                \n                \n                  \n                  \n                  \n                  (0.041)\n                  \n                \n                \n                  age\n                  \n                  \n                  \n                  -0.003\n                \n                \n                  \n                  \n                  \n                  \n                  (0.000)\n                \n                \n                  remod_age\n                  \n                  \n                  \n                  -0.003\n                \n                \n                  \n                  \n                  \n                  \n                  (0.000)\n                \n                \n                  Num.Obs.\n                  2918\n                  2918\n                  2918\n                  2918\n                \n                \n                  R2\n                  0.744\n                  0.843\n                  0.871\n                  0.808\n                \n                \n                  R2 Adj.\n                  0.744\n                  0.842\n                  0.869\n                  0.807\n                \n                \n                  BIC\n                  -1028.2\n                  -2284.2\n                  -2648.8\n                  -1845.9\n                \n                \n                  F\n                  1694.970\n                  597.010\n                  365.830\n                  1747.383\n                \n        \n      \n    \n\n\n#results table(...)\nmodelsummary(\n  list(\n    \"Structural Only\"         = m1,\n    \"Add Quality\"             = m2,\n    \"Add Neighbourhood\"       = m3,\n    \"Best BIC (Numeric Only)\" = m4\n  ),\n  statistic  = \"({std.error})\",\n  gof_omit   = \"AIC|Log.Lik|RMSE\",\n  coef_rename = nice_names\n)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Structural Only\n                Add Quality\n                Add Neighbourhood\n                Best BIC (Numeric Only)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  7.758\n                  9.387\n                  9.157\n                  7.450\n                \n                \n                  \n                  (0.128)\n                  (0.121)\n                  (0.119)\n                  (0.115)\n                \n                \n                  ln_area\n                  0.518\n                  0.303\n                  0.341\n                  0.632\n                \n                \n                  \n                  (0.021)\n                  (0.018)\n                  (0.018)\n                  (0.019)\n                \n                \n                  bedroom_abv_gr\n                  -0.071\n                  -0.012\n                  -0.013\n                  -0.037\n                \n                \n                  \n                  (0.006)\n                  (0.005)\n                  (0.005)\n                  (0.005)\n                \n                \n                  baths_tot\n                  0.097\n                  0.038\n                  0.018\n                  -0.041\n                \n                \n                  \n                  (0.009)\n                  (0.008)\n                  (0.008)\n                  (0.009)\n                \n                \n                  total_bsmt_sf\n                  0.000\n                  0.000\n                  0.000\n                  0.000\n                \n                \n                  \n                  (0.000)\n                  (0.000)\n                  (0.000)\n                  (0.000)\n                \n                \n                  garage_cars\n                  0.148\n                  0.084\n                  0.064\n                  0.096\n                \n                \n                  \n                  (0.006)\n                  (0.005)\n                  (0.005)\n                  (0.006)\n                \n                \n                  overall_qual.L\n                  \n                  -0.115\n                  -0.160\n                  \n                \n                \n                  \n                  \n                  (0.049)\n                  (0.045)\n                  \n                \n                \n                  overall_qual.Q\n                  \n                  -0.011\n                  -0.079\n                  \n                \n                \n                  \n                  \n                  (0.051)\n                  (0.047)\n                  \n                \n                \n                  overall_qual.C\n                  \n                  -0.129\n                  -0.178\n                  \n                \n                \n                  \n                  \n                  (0.048)\n                  (0.044)\n                  \n                \n                \n                  overall_qual^4\n                  \n                  -0.283\n                  -0.297\n                  \n                \n                \n                  \n                  \n                  (0.037)\n                  (0.034)\n                  \n                \n                \n                  overall_qual^5\n                  \n                  -0.401\n                  -0.383\n                  \n                \n                \n                  \n                  \n                  (0.032)\n                  (0.030)\n                  \n                \n                \n                  overall_qual^6\n                  \n                  -0.078\n                  -0.061\n                  \n                \n                \n                  \n                  \n                  (0.026)\n                  (0.024)\n                  \n                \n                \n                  overall_qual^7\n                  \n                  0.156\n                  0.146\n                  \n                \n                \n                  \n                  \n                  (0.028)\n                  (0.026)\n                  \n                \n                \n                  overall_qual^8\n                  \n                  0.015\n                  0.037\n                  \n                \n                \n                  \n                  \n                  (0.030)\n                  (0.027)\n                  \n                \n                \n                  overall_qual^9\n                  \n                  0.591\n                  0.520\n                  \n                \n                \n                  \n                  \n                  (0.030)\n                  (0.029)\n                  \n                \n                \n                  exter_qual.L\n                  \n                  -0.028\n                  -0.023\n                  \n                \n                \n                  \n                  \n                  (0.018)\n                  (0.016)\n                  \n                \n                \n                  exter_qual.Q\n                  \n                  0.103\n                  0.079\n                  \n                \n                \n                  \n                  \n                  (0.019)\n                  (0.017)\n                  \n                \n                \n                  exter_qual.C\n                  \n                  -0.147\n                  -0.090\n                  \n                \n                \n                  \n                  \n                  (0.022)\n                  (0.021)\n                  \n                \n                \n                  kitchen_qual.L\n                  \n                  -0.115\n                  -0.170\n                  \n                \n                \n                  \n                  \n                  (0.052)\n                  (0.048)\n                  \n                \n                \n                  kitchen_qual.Q\n                  \n                  0.077\n                  0.125\n                  \n                \n                \n                  \n                  \n                  (0.044)\n                  (0.040)\n                  \n                \n                \n                  kitchen_qual.C\n                  \n                  -0.010\n                  0.104\n                  \n                \n                \n                  \n                  \n                  (0.101)\n                  (0.093)\n                  \n                \n                \n                  kitchen_qual^4\n                  \n                  0.154\n                  0.223\n                  \n                \n                \n                  \n                  \n                  (0.077)\n                  (0.071)\n                  \n                \n                \n                  fireplace_qu.L\n                  \n                  -0.040\n                  -0.041\n                  \n                \n                \n                  \n                  \n                  (0.019)\n                  (0.018)\n                  \n                \n                \n                  fireplace_qu.Q\n                  \n                  0.064\n                  0.047\n                  \n                \n                \n                  \n                  \n                  (0.015)\n                  (0.014)\n                  \n                \n                \n                  fireplace_qu.C\n                  \n                  0.015\n                  0.006\n                  \n                \n                \n                  \n                  \n                  (0.018)\n                  (0.017)\n                  \n                \n                \n                  fireplace_qu^4\n                  \n                  -0.022\n                  -0.024\n                  \n                \n                \n                  \n                  \n                  (0.018)\n                  (0.017)\n                  \n                \n                \n                  fireplace_qu^5\n                  \n                  -0.041\n                  -0.034\n                  \n                \n                \n                  \n                  \n                  (0.011)\n                  (0.010)\n                  \n                \n                \n                  neighborhoodBlueste\n                  \n                  \n                  -0.075\n                  \n                \n                \n                  \n                  \n                  \n                  (0.054)\n                  \n                \n                \n                  neighborhoodBriardale\n                  \n                  \n                  -0.167\n                  \n                \n                \n                  \n                  \n                  \n                  (0.040)\n                  \n                \n                \n                  neighborhoodBrookside\n                  \n                  \n                  -0.038\n                  \n                \n                \n                  \n                  \n                  \n                  (0.033)\n                  \n                \n                \n                  neighborhoodClear_Creek\n                  \n                  \n                  0.157\n                  \n                \n                \n                  \n                  \n                  \n                  (0.036)\n                  \n                \n                \n                  neighborhoodCollege_Creek\n                  \n                  \n                  0.102\n                  \n                \n                \n                  \n                  \n                  \n                  (0.029)\n                  \n                \n                \n                  neighborhoodCrawford\n                  \n                  \n                  0.144\n                  \n                \n                \n                  \n                  \n                  \n                  (0.032)\n                  \n                \n                \n                  neighborhoodEdwards\n                  \n                  \n                  -0.038\n                  \n                \n                \n                  \n                  \n                  \n                  (0.031)\n                  \n                \n                \n                  neighborhoodGilbert\n                  \n                  \n                  0.070\n                  \n                \n                \n                  \n                  \n                  \n                  (0.030)\n                  \n                \n                \n                  neighborhoodGreen_Hills\n                  \n                  \n                  0.527\n                  \n                \n                \n                  \n                  \n                  \n                  (0.106)\n                  \n                \n                \n                  neighborhoodGreens\n                  \n                  \n                  0.031\n                  \n                \n                \n                  \n                  \n                  \n                  (0.059)\n                  \n                \n                \n                  Nbh: Iowa DOT/RR\n                  \n                  \n                  -0.142\n                  \n                \n                \n                  \n                  \n                  \n                  (0.034)\n                  \n                \n                \n                  neighborhoodLandmark\n                  \n                  \n                  -0.032\n                  \n                \n                \n                  \n                  \n                  \n                  (0.147)\n                  \n                \n                \n                  neighborhoodMeadow_Village\n                  \n                  \n                  -0.127\n                  \n                \n                \n                  \n                  \n                  \n                  (0.039)\n                  \n                \n                \n                  neighborhoodMitchell\n                  \n                  \n                  0.081\n                  \n                \n                \n                  \n                  \n                  \n                  (0.032)\n                  \n                \n                \n                  neighborhoodNorth_Ames\n                  \n                  \n                  0.035\n                  \n                \n                \n                  \n                  \n                  \n                  (0.030)\n                  \n                \n                \n                  neighborhoodNorthpark_Villa\n                  \n                  \n                  -0.059\n                  \n                \n                \n                  \n                  \n                  \n                  (0.042)\n                  \n                \n                \n                  neighborhoodNorthridge\n                  \n                  \n                  0.189\n                  \n                \n                \n                  \n                  \n                  \n                  (0.034)\n                  \n                \n                \n                  neighborhoodNorthridge_Heights\n                  \n                  \n                  0.142\n                  \n                \n                \n                  \n                  \n                  \n                  (0.031)\n                  \n                \n                \n                  neighborhoodNorthwest_Ames\n                  \n                  \n                  0.051\n                  \n                \n                \n                  \n                  \n                  \n                  (0.031)\n                  \n                \n                \n                  neighborhoodOld_Town\n                  \n                  \n                  -0.113\n                  \n                \n                \n                  \n                  \n                  \n                  (0.031)\n                  \n                \n                \n                  neighborhoodSawyer\n                  \n                  \n                  0.043\n                  \n                \n                \n                  \n                  \n                  \n                  (0.032)\n                  \n                \n                \n                  neighborhoodSawyer_West\n                  \n                  \n                  0.043\n                  \n                \n                \n                  \n                  \n                  \n                  (0.031)\n                  \n                \n                \n                  neighborhoodSomerset\n                  \n                  \n                  0.106\n                  \n                \n                \n                  \n                  \n                  \n                  (0.030)\n                  \n                \n                \n                  Nbh: ISU SW\n                  \n                  \n                  -0.050\n                  \n                \n                \n                  \n                  \n                  \n                  (0.037)\n                  \n                \n                \n                  neighborhoodStone_Brook\n                  \n                  \n                  0.182\n                  \n                \n                \n                  \n                  \n                  \n                  (0.035)\n                  \n                \n                \n                  neighborhoodTimberland\n                  \n                  \n                  0.122\n                  \n                \n                \n                  \n                  \n                  \n                  (0.033)\n                  \n                \n                \n                  neighborhoodVeenker\n                  \n                  \n                  0.120\n                  \n                \n                \n                  \n                  \n                  \n                  (0.041)\n                  \n                \n                \n                  age\n                  \n                  \n                  \n                  -0.003\n                \n                \n                  \n                  \n                  \n                  \n                  (0.000)\n                \n                \n                  remod_age\n                  \n                  \n                  \n                  -0.003\n                \n                \n                  \n                  \n                  \n                  \n                  (0.000)\n                \n                \n                  Num.Obs.\n                  2918\n                  2918\n                  2918\n                  2918\n                \n                \n                  R2\n                  0.744\n                  0.843\n                  0.871\n                  0.808\n                \n                \n                  R2 Adj.\n                  0.744\n                  0.842\n                  0.869\n                  0.807\n                \n                \n                  BIC\n                  -1028.2\n                  -2284.2\n                  -2648.8\n                  -1845.9\n                \n                \n                  F\n                  1694.970\n                  597.010\n                  365.830\n                  1747.383",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Solution to inâ€‘class Sup 5</span>"
    ]
  }
]