[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Methods for Land Economy",
    "section": "",
    "text": "Welcome\nWelcome to these interactive laboratory sessions for Research Methods in Land Economy. This digital workbook provides hands-on experience with R programming, quantitative analysis, and data visualisation techniques essential for contemporary research in land economy, finance, planning, and environmental housing studies.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome and Supervision Overview</span>"
    ]
  },
  {
    "objectID": "index.html#supervisions-overview",
    "href": "index.html#supervisions-overview",
    "title": "Research Methods for Land Economy",
    "section": "Supervisions Overview",
    "text": "Supervisions Overview\nThese supervised laboratory sessions are designed for MPhil students beginning their quantitative research journey. Over several progressive supervision, you will develop from complete programming novice to confident data analysis skills, working with appropriate real-world land economy datasets from institutional sources and applying cutting-edge analytical techniques to substantive research questions in your field.\n\nAcademic Integration\nEach lab connects technical skill development with substantive research applications, ensuring that programming competency serves your broader academic and professional objectives. You’ll work with datasets relevant to housing policy, regional development, and planning analysis while developing reproducible research workflows used in leading academic institutions and policy organisations.\n\n\n\nHow to Use This Interactive Workbook\nThis workbook is designed as a practical, hands-on learning experience that requires active engagement with both the content and your RStudio environment. Success depends on following the structured approach outlined below.\n\n\nEssential Setup\n\nTechnical Requirements\nBefore beginning, ensure you have: - R (version ≥ 4.3) installed from r-project.org - RStudio Desktop (latest version) from posit.co - A stable internet connection for package installation and data downloads - Sufficient disk space (≥ 2GB) for datasets and outputs\n\n\n💾 Install R and RStudio on your laptops/gadgets\nRStudio is a wrapper for R; we are using RStudio because it makes looks R more organised and straightforward. Follow the following instructions:\n\ngo to https://www.r-project.org\nclick on ‘CRAN mirror’\nchoose one of the two UK mirrors - https://www.stats.bris.ac.uk/R/ or https://cran.ma.imperial.ac.uk/\nClick on ‘Download R for Windows’ or ‘Download R for macOS’\nClick on ’install R for the first time\nClick on ‘Download R-… for Windows’, the current version is R-4.5.1. Or alternatively, download the executable file for macOS.\ndouble-click the downloaded file, then click on ‘Run’, ‘language:English’, ‘OK’, and read and follow the instructions of installation until you click on ‘Next’ for installing and ‘Finish’\n\nThen proceed to Installing RStudio.\n\nGo to rstudio.com\nclick on ‘download’\ndownload the Free version, download RStudio for Windows or macOS\nClick on ‘Next’ if you agree and on ‘Install’ and ‘Finish’ to complete the process.\n\n\n\nProject Organisation\nCreate a dedicated folder structure for the course:\nRM01_Labs/\n├── data/           # Raw datasets and imports\n├── scripts/        # Your R code files  \n├── outputs/        # Generated plots, tables, reports\n├── certificates/   # Professional development documentation\n└── reflections/    # Written responses to critical questions\n\n\n\nActive Learning Protocol\n\n1. Read → Code → Reflect\nEach lab section follows this essential pattern: - Read the contextual material and instructions carefully - Code by replicating all examples in your RStudio session - Reflect by answering critical thinking questions in writing\n\n\n2. Hands-On Replication\nCritical: Do not simply read the code examples. You must: - Type (don’t copy-paste) all code into RStudio scripts - Execute each code block and verify the outputs - Experiment with variations and modifications - Debug any errors before proceeding\n\n\n3. Documentation Standards\nMaintain professional documentation practices: - Save each lab as a separate R script (e.g., lab01_certification.R) - Comment your code extensively using # explanatory text - Include session information - Date and version your files systematically\n\n\n\nDeliverables\nEach lab includes specific deliverables: - Technical: Working R scripts with documented code - Visual: Publication-quality plots and analytical outputs\n- Written: Responses to all critical reflection questions - Professional: Certificates, portfolios, and documentation\n\n\nLearning Support\n\nGetting Help\nWhen you encounter difficulties: 1. Review the relevant section carefully for missed details or typos 2. Check R documentation using ?function_name or help(function_name) 3. Examine error messages systematically—they often provide clear guidance and AI can often provide answers 4. Consult peers through appropriate academic channels 5. Attend supervision sessions prepared with specific questions\n\n\nCommon Pitfalls to Avoid\n\nPassive learning: You must complete the materials before attending the face-to-face supervisions.\nPassive reading: You must actively code to learn programming\nCopy-paste coding: Type examples yourself to build muscle memory\nIgnoring errors: Always resolve issues before proceeding\nSuperficial reflections: Engage deeply with methodological questions\nPoor organisation: Maintain systematic file and folder structures\n\n\n\nIndustry Applications\nR programming competency is increasingly valued in: - Policy Analysis: Government departments and think tanks - Consultancy: Urban planning, housing, and development firms - Research Organisations: Academic institutions and policy institutes - Data Science: Emerging opportunities across multiple sectors\n\n\n\n\nTechnical Notes\nLabs will introduce packages progressively. Install required packages as indicated in each session, and maintain a record of your technical environment using:\n# Document your session for reproducibility\nsessionInfo()\n\n\n\nGetting Started\nNavigate to Supervision 1 using the link to begin your journey into R programming for land economy research. Remember: this is an interactive learning experience that requires preparation before attending supervisions, your active participation, critical thinking, and professional engagement.\nSuccess in these labs depends not just on following instructions, but on developing the analytical thinking skills that will serve your research skills.\n\nFor technical support or academic guidance, consult your teaching team or attend designated office hours with specific questions prepared.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome and Supervision Overview</span>"
    ]
  },
  {
    "objectID": "supervision_1.html",
    "href": "supervision_1.html",
    "title": "Supervision 1",
    "section": "",
    "text": "🧪 Lab 1 — R Quickstart",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervision 1</span>"
    ]
  },
  {
    "objectID": "supervision_1.html#lab-1-r-quickstart",
    "href": "supervision_1.html#lab-1-r-quickstart",
    "title": "Supervision 1",
    "section": "",
    "text": "🎯 Learning outcomes (you will be able to)\n\nOpen RStudio and recognise the Console, Script, Environment/History, Files/Plots/Help panes.\nCreate and run a short R script (comments, simple operations).\nSet your working directory to a course folder.\nCreate and inspect basic objects (a scalar and a small matrix).\nSave your script for the next supervision.\n\n\n\n🧰 Prerequisites\n\nRead “Welcome & Supervision Overview” section.\nR and RStudio installed.\nA course folder and subfolders on your machine where you will save scripts and other files:\n\nRM01_labs/\n├── data/           # For raw datasets and imports\n├── scripts/        # Your R code files  \n├── outputs/        # Generated plots, tables, reports\n├── certificates/   # Professional development documentation\n└── reflections/    # Written responses to critical questions\n\n\n📘 Introduction to R\nAfter downloading and installing open RStudio, we will cover now some tutorials.\n👨‍💻 Long Tutorial\nWe would cover a brief introduction on this labs, students can use a longer tutorial for detailed learning at https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf\n\n\nA. Get oriented in RStudio\nYou need to have installed R and RStudio to continue with this lab.\n\nOpen Rstudio in your computer.\n\n\n\n\nOn this picture, there are three panels ‘the Console’, ‘The Environment’, and ‘The Plots, Help and Files’ panel.\n\n\n\nLet’s choose a better appeareance for our RStudio. Click on Tools&gt;Global Options.\n\n\n\n\nIn RStudio, click on ‘Tools’ and ‘Global Options’\n\n\n\nAfterwards, in the window “Options”, click on “Appearance”, RStudio theme: “Modern”, Editor font: “Courier”, Editor theme: Cobalt. Finally, click on “Apply”\n\n\n\n\nYou are here by clicking on Tools&gt;Global Options..&gt;Appereance&gt;Modern&gt;Courier&gt;Cobalt&gt;Apply\n\n\n\n\n\nB. Your first script\n\nOpen a file to save the code for this session. Click on File → New File → R Script; or simply press Ctrl+Shift+N (command+shift+enter in Mac); this will open a new panel called ‘Script’ where we are going to write the commands for R. Change the name of the script to “lab_1”, saving the script in the folder RM01_Labs/scripts/ File → Save As → “lab_1”\n\n\n\n\nClick on File&gt;New File&gt;R Script\n\n\n\nAdd a comment (starts with #.) On the script panel, type the caracter # and then write 'LAB ONE AND TODAY'S DATE'. R will ignore any line of command starting with #. You can use # to add comments, titles, reminders within your R code/ R Script.\n\nYour R Script now shall look similar to this:\n\n\n\nComments are crucial to learn R\n\n\nThe Rstudio screen has four windows or panels: 1. Console. 2. Environment and history. 3. Files, plots, packages and help. 4. The R script(s) and data view. The R script is where you keep a record of your work or commands - in a line by line basis.\n\nOn your script, type getwd() in line number ‘2’ and hit the keys ‘ctrl+enter’ in your keyboard (The + indicates ‘do it at the same time’), you dont need to press the key ‘+’. After ‘running’ line 2, you will see in the console panel the working directory (or folder) that R is using to download and upload files.\n\n\n\n\nThis image shows how your R Script should look\n\n\n\n\n\nThe outcome in the console -bottom panel after getwd()\n\n\n\n\n\nC. Set your working directory\nChoose your course file folder 📁. Recall we called the working folder folder ‘RM01_labs’. In RStudio’ go to ‘Session’, ‘Set Working Directory’, ‘Choose Directory’, and select this folder.\n\n\n\nCreate your folder and then click on Session&gt;Set Working Directory&gt;Choose Directory\n\n\nThe ‘Console’ panel renders setwd(and a path/directory to your chosen folder in your computer). Copy and paste this output in line 3 from your Console to your R Script. Setting the directory with code and not ‘clicks’ will save you time.\n\n\n\nThis is the code you shall copy and paste in your R Script, ignore ‘&gt;’\n\n\n\n\n\nsetwd() in your R Script\n\n\nSo far, we have created the following script:\n# LAB ONE 25/09/2025\ngetwd()\nsetwd(\"C:/Users/YOUR DIRECTORY/RM01_labs\")\nand this is the Console output:\n&gt; # LAB ONE 25/09/2025\n&gt; getwd()\n[1] \"C:/Users/YOUR DIRECTORY/Documents\"\n&gt; setwd(\"C:/Users/YOUT DIRECTORY/RM01_labs\")\n\n\nD. First calculations & a matrix (3–4 min)\n\nIn your script, run: 5 * 5 (result should be 25 in the Console).\n\n\n&gt; 5*5\n[1] 25\n\n\nCreate a small matrix and run it. Use the following code, explanatory comments are written with #:\n# A is an object, \n# &lt;- is similar to equal\n# 1:8 are the numbers from 1 to 8\n# nrow is the matrix number of rows = 4\n# ncol is the matrix number of columns = 2 \nA &lt;- matrix(1:8, nrow = 4, ncol = 2)\n# The object A now has a matrix. Run the object A\nA\nThe outcome in the console shall be as follows:\n\n\n&gt; # A is an object, \n&gt;    # &lt;- is similar to equal\n&gt;    # 1:8 are the numbers from 1 to 8\n&gt;    # nrow is the matrix number of rows = 4\n&gt;    # ncol is the matrix number of columns = 2 \n&gt;    A &lt;- matrix(1:8, nrow = 4, ncol = 2)\n&gt;    # The object A now has a matrix. Run the object A\n&gt;    A\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\n\nIn brief, you created a matrix called ‘A’, with numbers from 1 to 8, with four rows and two columns.\n\n\n\n\nE. Inspect objects & view data\n\nCheck the Environment tab to see objects you’ve created.\nThe tab History shows a list of commands used do far.\n\n\n\n\nEnvironment Panel\n\n\n\nTo see the matrix A in a new window, type the command View(A) in your script and hit the keys ‘ctrl+enter’ to run that line , this will send you to a different tab. Afterwards, click on your R script to come back.\n\n# LAB ONE 25/09/2025\ngetwd()\nsetwd(\"C:/Users/Cam/OneDrive - University of Cambridge/Cambridge 2025-26/RM01_labs\")\n5*5\n# A is an object, \n# &lt;- is similar to equal\n# 1:8 are the numbers from 1 to 8\n# nrow is the matrix number of rows = 4\n# ncol is the matrix number of columns = 2 \nA &lt;- matrix(1:8, nrow = 4, ncol = 2)\n# The object A now has a matrix. Run the object A\nA\nView(A)\n\n\nF. (Optional) Install and load a package\nThe tab ‘Packages’ shows the list of add-ons included in the installation of RStudio. Click on the tab ‘Packages’, if you check a box next to a package, that package is loaded into R, if not, any command related to that package won’t work, you will need select it. You can also install other add-ons by clicking on the ‘Install’ icon.\nAnother way to install add-ons is to type the function install.packages(\"name of the package\"), and then you will be able to open the library of commands of that package.\n\nIn line 8, to install the package forecast, type install.packages(“forecast”) and hit ctrl+enter. If a window opens, you can hit yes to restart RStudio. See the picture below, make sure you do not get errors in the console. The sign ‘&gt;’ will show when R finishes running a line.\n\n# the function to install packages in R\n# we use \"forecast\" as example\ninstall.packages(\"forecast\") # to install the package\nlibrary(forecast) # to activate the package 'forecast'\nRemember to activate the package by typing ‘library(the package)’, for example, library(forecast) and ctrl+enter. This will activate all functions within the ‘forecast’ package.\nMake sure there are no error messages in the installation, and if so, make sure to solve the issue before continuing.\nThe package forecast is used to analyse and predict time series (e.g. yearly house prices during the last 20 years).\nScript/Data View Window:\n• Begin scripts with a comment for title and description using the hash character (#). Anything after the hash on the same line is considered a comment and is ignored by R. • Code can continue to the next line without a special character, but only if the previous line ends in a way that suggests continuation (e.g., a comma or unclosed brackets). • To run a line of code, position the cursor on the line and press Ctrl+Enter. For multiple lines, select them and press Ctrl+Enter.\nConsole/Output Window:\n• It’s recommended to work in script mode for reusability. • Commands outside the script context can be typed at the bottom of the console, indicated by the “&gt;” sign. • Press Enter to execute a command. • Use the up and down arrows to revisit and edit older lines of code previously typed into the console.\n\n\n\n💾 Save your script\nFile → Save (e.g., lab_1) into your ‘script’ folder. Remember to save your R Scripts after each supervision, this will save you a lot time!\n\n\n✅ Check‑off\n\nI can run code from a script (Ctrl+Enter).\nMy working directory is set via setwd() at the top of the script.\nI created A &lt;- matrix(1:8, 4, 2) and can see it in Environment.\nScript saved.\n\n\n🏁 End of Lab 1 🛑 Remember to save your script 💾",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervision 1</span>"
    ]
  },
  {
    "objectID": "supervision_1.html#lab-2-professional-development-r-certification-via-linkedin-learning",
    "href": "supervision_1.html#lab-2-professional-development-r-certification-via-linkedin-learning",
    "title": "Supervision 1",
    "section": "🧪 Lab 2 — Professional Development: R Certification via LinkedIn Learning",
    "text": "🧪 Lab 2 — Professional Development: R Certification via LinkedIn Learning\nProfessional certifications complement academic learning by providing industry-recognised credentials. For land economy graduates leading industries and entering consultancy, government, or research roles, demonstrating quantitative competency through multiple channels strengthens your professional profile and evidences commitment to continuous learning.\n\n\nLearning outcomes\nBy the end of this lab you will be able to:\n\nAccess institutional resources / LinkedIn Learning for continuous professional development\nComplete a certified R programming course that demonstrates foundational competency\nDocument your learning through professional certification pathways\nPrepare your technical environment for advanced coursework\nReflect critically on skill acquisition in academic and professional contexts\n\n\n\n\nPrerequisites\n\nUniversity credentials for LinkedIn Learning access.\n\nR and RStudio installed (you’ll run a small code check).\n\n\n\n\n🧪 Part A — First‑time access to LinkedIn Learning (20 minutes)\nEstablish access to University-provided LinkedIn Learning resources.\n\nFollow the UIS guidance: help.uis.cam.ac.uk/service/support/training/linkedin-learning-info (opens the official instructions).\n\n\n\nChoose Login with your University account (SSO). If prompted, connect to an existing LinkedIn profile (optional, but helpful for saving certificates).\n\n\n\n\n🧪 Part B — Course Selection and Academic Alignment\n\nIn LinkedIn Learning, search for “R Programming” or “R for Data Analysis”.\n\nPick a Beginner course or a Learning Path that issues a certificate upon completion.\n\nSkim the syllabus; confirm it covers: R basics (e.g. objects, vectors), data frames, importing data, and simple plots.\n\n\nShort beginner certificates often take only a few hours. Pick one you can finish this week.\n\n💭 Reflect: a) Provide a certificate/enrolment URL or a screenshot proving enrolment. What challenges do you anticipate in acquiring programming skills alongside your substantive coursework? b) List three new R commands and three packages you learned and one common error you corrected.\n\n\n\n🧪 Part C — Add to your LinkedIn profile (optional but recommended)\n\nOpen your LinkedIn profile → Add profile section → Recommended → Licenses & Certifications.\n\nName: Course title. Issuing organization: LinkedIn Learning. Credential URL: paste the certificate share link (if available).\n\nSave.\n\n\n\n\n🧪 Part D — RStudio tech‑check for upcoming labs\nRun the following lines in RStudio to ensure you’re ready for the following Labs.\n\nRecall you can run lines of code in R by hitting the keys ctrl+enter (commad+enter for Mac)\n\n\nR.version.string\n\n[1] \"R version 4.4.2 (2024-10-31 ucrt)\"\n\nsessionInfo()$running\n\n[1] \"Windows 10 x64 (build 19045)\"\n\n\nInstall key packages (run once):\n\n# install all packages at once\ninstall.packages(c(\"learnr\", \"r4ds.tutorials\", \"readr\", \"readxl\", \"ggplot2\"))\n\nVerify they load and basic plotting works:\n\n# Create a small data frame with x = 1,...,10 and y = sqrt(x).\n# We'll use this to do a quick \"tech check\" with a base R plot.\ndf &lt;- data.frame(x = 1:10, y = (1:10)^0.5)\n\n# Base R line plot:\n# - type = \"l\" draws a line (instead of points).\n# - main/xlab/ylab set the title and axis labels.\n# This confirms base graphics are working and shows a concave curve of sqrt(x).\nplot(df$x, df$y, type = \"l\",\n     main = \"Tech check: base plot\",\n     xlab = \"x\",\n     ylab = \"sqrt(x)\")\n\n\n\n\n\n\n\n\n\n# -----------------------------------------------\n# Test basic plotting capability with ggplot2\n# -----------------------------------------------\n\n# Load ggplot2 (grammar of graphics) for layered, customizable plots.\nlibrary(ggplot2)\n\n# Create a toy dataset: yearly \"average house prices\" (hypothetical numbers).\n# This lets us test lines, points, labels, and themes in ggplot2.\ntest_data &lt;- data.frame(\n  year = 2010:2020,\n  house_prices = c(250000, 260000, 275000, 285000, 290000, \n                   310000, 325000, 340000, 355000, 370000, 385000)\n)\n\n# Start a ggplot:\n# - aes(x = year, y = house_prices) maps variables to axes.\n# - geom_line() draws the trend line; linewidth controls thickness.\n# - geom_point() adds markers at each year.\n# - scale_y_continuous(labels = scales::comma_format()) formats y-axis with commas (e.g., 310,000).\n# - labs() sets a clear title, subtitle, axis labels, and a caption.\n# - theme_minimal() gives a clean, publication-style look.\nggplot(test_data, aes(x = year, y = house_prices)) +\n  geom_line(colour = \"steelblue\", linewidth = 1.2) +   # trend line\n  geom_point(colour = \"darkred\", size = 2) +           # points on each year\n  scale_y_continuous(labels = scales::comma_format()) +# pretty y-axis labels\n  labs(\n    title = \"Hypothetical House Price Trends\",         # main title\n    subtitle = \"System functionality test\",            # context/subtitle\n    x = \"Year\",                                        # x-axis label\n    y = \"Average House Price (£)\",                     # y-axis label\n    caption = \"Test data for R/RStudio verification\"   # caption under the plot\n  ) +\n  theme_minimal()                                      # clean theme\n\n\n\n\n\n\n\n\n💭 Reflect: Did everything install and run without warnings? Capture any errors so we can fix them in class.\n\n\n\nBest practices\n\nPractice while you watch—replicate examples in RStudio, don’t just read the labs and watch videos.\n\nKeep a snippets file (your personal cheat‑sheet) with commands you’ll reuse.\n\nStore all artifacts (notes, PDFs) in your course project folder for quick reference.\n\n\n🏁 End of Lab 2 🛑 Remember to save your script 💾",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervision 1</span>"
    ]
  },
  {
    "objectID": "supervision_1.html#lab-3-interactive-r-learning-building-programming-competency",
    "href": "supervision_1.html#lab-3-interactive-r-learning-building-programming-competency",
    "title": "Supervision 1",
    "section": "🧪 Lab 3 — Interactive R Learning: Building Programming Competency",
    "text": "🧪 Lab 3 — Interactive R Learning: Building Programming Competency\n\n\nLearning outcomes\nThis lab exposes you to different pedagogical approaches in quantitative methods education while building the practical skills necessary for data-driven research in land economy.\nBy the end of this lab you will be able to:\n\nNavigate multiple R learning platforms with different pedagogical approaches\nComplete structured programming exercises with immediate feedback\nDevelop troubleshooting skills through guided problem-solving\nAcquire multiple ways of self-development for your own study plan.\n\n\n\n\nPrerequisites\n\nR (≥ 4.1) and RStudio.\nPackages (install once):\n\n\ninstall.packages(c(\"learnr\", \"r4ds.tutorials\", \"swirl\"))\n\n\nIf tutorials don’t appear in RStudio’s Tutorial tab after installation, restart R.\n\n\n\n\nPart A — r4ds.tutorials: 01-data-visualization\nThis runs as an interactive tutorial in RStudio’s Tutorial pane.\n\n# Option 1: via code\nlearnr::run_tutorial(\"01-data-visualization\", package = \"r4ds.tutorials\")\n\n# Option 2: discover tutorials installed on your system\nlearnr::available_tutorials(\"r4ds.tutorials\")\n\nDo this: 1. Start and complete the tutorial on data visualization. Use the Show in New Window icon if the pane is small. 2. When you reach the Submit page, follow its instructions and save the html (we won’t collect submissions in this lab, just finish to the end).\n💭 Reflect: - What topic would benefit from additional practice or explanation? e.g. aesthetics (e.g., aes(x, y, color)) and geoms (e.g., geom_point(), geom_line())\n\n\n\nPart B — learnr: ex-data-basics\nThis is a short core tutorial bundled with learnr.\n\n# open interactive tutorial in the package 'learnr'\nlearnr::run_tutorial(\"ex-data-basics\", package = \"learnr\")\n\n# Explore other tutorials\nlearnr::run_tutorial(, package = \"learnr\")\n\nDo this: Complete all exercises for the tutorial ‘ex-data-basics’..\n💭 Reflect: - What is the difference between a tibble and a data frame? - Why might tibbles be preferred in modern R workflows?\n\n\n\nPart C — swirl (console‑based)\nswirl runs entirely in the Console and saves your progress.\n\n# Install/run swirl\ninstall.packages(\"swirl\")\nlibrary(swirl)\n\n# Start swirl\nswirl()\n# (enter your name when prompted)\n\n# Install and start a course (first time only)\ninstall_from_swirl(\"R Programming\")   # or \"R_Programming\"\n\n# At any time, leave swirl with\n# bye()\n\n# Check progress later\nswirl::progress()\n\nDo this: - Install R Programming and complete the first lesson. - Exit with bye() and verify progress with swirl::progress().\n💭 Reflect: - How does working in the Console (swirl) feel compared with the GUI tutorials?\n\n\n\nPart D — Mini practice\nCreate a tiny vector and compute a statistic, then plot a quick line:\n\n# create variable x\nx &lt;- 1:10\n#calculate the mean and asign it to mean_x\nmean_x &lt;- mean(x)\n# review mean_x\nmean_x\n\n[1] 5.5\n\n# quick plotting function\nplot(x, type = \"l\", main = \"Quick line\", ylab = \"x\")\n# abline adds horizontal (h) or vertical (v) lines.\nabline(h = mean_x, lty = 2)\n\n\n\n\n\n\n\n\n💭 Reflect: - Which approach (r4ds.tutorials, learnr, swirl) best prepared you to learn R coding independently?\n\n\n\nBest practices\n\nUse learnr/r4ds.tutorials for guided practice with hints, auto‑checks, and saved state.\nUse swirl when you prefer keyboard‑only Console practice or have limited UI.\nRestart RStudio if tutorials don’t show; list what’s available with learnr::available_tutorials().\nKeep your scripts open alongside tutorials to copy refined solutions into your own notes.\n\n\n🏁 End of Lab 3 🛑 Remember to save your script 💾",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervision 1</span>"
    ]
  },
  {
    "objectID": "supervision_1.html#lab-4-uploading-excel-csv-files-to-r-pipr",
    "href": "supervision_1.html#lab-4-uploading-excel-csv-files-to-r-pipr",
    "title": "Supervision 1",
    "section": "🧪 Lab 4 — Uploading Excel & CSV Files to R (PIPR)",
    "text": "🧪 Lab 4 — Uploading Excel & CSV Files to R (PIPR)\n\n\nLearning outcomes\nBy the end of this lab you will be able to: 1. Download the Private rents annual inflation to August 2024 monthly workbook and save it in a tidy project structure. 2. Import Excel (.xlsx/.xls) and CSV files using RStudio’s Import Dataset UI and reproducible R code. 3. Deal with metadata/header rows via skip, set col_names, and verify column types. 4. Plot time series (index and annual % change) with base R and add reference lines. 5. Use help pages (e.g., ?plot) and annotate charts for a policy audience.\n\n\n\nPrerequisites\n\nR (≥ 4.0) and RStudio.\nProject folder (e.g., RM01_labs/) with subfolder data/.\nPackages (install once):\n\n\n# Make sure to install the packages.\n# install.packages(c(\"readxl\", \"readr\"))\n\n\n💡 Tip: Prefer file names without spaces and lower-cases, e.g. pipr_monthly.xls rather than pipr monthly.xls.\n\n\n\n\nPart A — Get PIPR monthly from ONS (manual steps)\n\nGo to ONS.gov.uk and search Private rents annual inflation, UK countries, January 2016 to August 2024. This lab is using ONS data on Figure 4: Rent annual inflation slowed across the UK, Private rents annual inflation, across the UK, January 2016 to August 2025.\n\n\n\n\nThis is the website / data source.\n\n\n\n\n\nThis is the the data we used in the tutorial.\n\n\n\nOn the dataset page, choose Data → Download monthly workbook. Save the file as Excel and CSV Comma delimited.\nSave as data/pipr_monthly.xls inside your project folder (e.g., RM01_labs/) with subfolder data/.\n(Optional) Open the file and identify the sheet that contains the UK and country/regions.\n\n💭 Reflect: Why does PIPR (rents) provide a more direct lens on housing affordability and regional pressures than GDP for planning and regeneration?\n\n\n\nPart B — Import Excel via code (reproducible)\nUse readxl::read_excel() and inspect column names to choose the UK series.\n\nlibrary(readxl)\n# Adjust 'sheet' and 'skip' depending on the workbook structure you download.\n# we use skip = 7 because there are headings in the first 7 rows of the dataset.\npipr_raw &lt;- read_excel(\"data/pipr_monthly.xls\", sheet = 1, skip = 7)\n\n# Inspect names to locate the date/month, UK Index, and UK annual % change columns\nnames(pipr_raw)\n\n[1] \"Date\"             \"UK\"               \"England\"          \"Wales\"           \n[5] \"Scotland\"         \"Northern Ireland\"\n\n# A good practice is to keep a copy of the raw data and create a new version of the data for manipulation.\n\npipr&lt;-pipr_raw\n\n# rename column names for easiness. \nnames(pipr) &lt;- c(\"month\", \"uk\", \"england\", \"wales\", \"scotland\", \"n_ireland\")\n\nstr(pipr)\n\ntibble [116 × 6] (S3: tbl_df/tbl/data.frame)\n $ month    : POSIXct[1:116], format: \"2016-01-01\" \"2016-02-01\" ...\n $ uk       : num [1:116] 3.3 3.3 3.4 3.3 3.2 3.3 3.4 3.3 3.2 3.3 ...\n $ england  : num [1:116] 3.5 3.5 3.6 3.5 3.3 3.5 3.7 3.6 3.5 3.6 ...\n $ wales    : num [1:116] 1.2 0.8 0.8 0.6 0.4 0.3 0.3 0.9 1.4 1.3 ...\n $ scotland : num [1:116] 2.1 1.8 1.6 1.5 1.4 1.1 1.2 0.9 1.1 1.1 ...\n $ n_ireland: num [1:116] 1.1 1.3 1.4 1.6 1.6 1.6 1.5 1.5 1 0.9 ...\n\nhead(pipr, 6)\n\n# A tibble: 6 × 6\n  month                  uk england wales scotland n_ireland\n  &lt;dttm&gt;              &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 2016-01-01 00:00:00   3.3     3.5   1.2      2.1       1.1\n2 2016-02-01 00:00:00   3.3     3.5   0.8      1.8       1.3\n3 2016-03-01 00:00:00   3.4     3.6   0.8      1.6       1.4\n4 2016-04-01 00:00:00   3.3     3.5   0.6      1.5       1.6\n5 2016-05-01 00:00:00   3.2     3.3   0.4      1.4       1.6\n6 2016-06-01 00:00:00   3.3     3.5   0.3      1.1       1.6\n\n\n\n\n\nPart C — Import via RStudio UI (reference)\n\nGo to RStudio → Import Dataset → From Excel → select data/pipr_monthly.xls.\n\n\n\n\nimporting from Rstudio with ‘import dataset’\n\n\n\nChoose the relevant sheet and, if needed, set Skip for metadata rows (recall we used skip = 7 in Part B).\n\n\n\n\nImport Ecvel data window\n\n\n\nSet Name to pipr_monthly and verify column names (e.g., month, uk_index, uk_yoy).\nClick Copy to grab the generated R code, then Import.\n\nPaste the generated code in your R script so your workflow is reproducible.\n\n\n\nPart D — Basic time‑series line plot (annual % change)\nThe following code plots PIPR annual % change for the UK as a line. Label the x‑axis sparsely to keep it readable.\n\n# Create a line plot of UK private rent annual % changes\nplot(pipr$uk, type = \"l\",  # type = \"l\" means line plot\n     xlab = \"\",  # empty x-axis label (we'll customize it below)\n     ylab = \"Private rents — annual % change\",  # y-axis label\n     main = \"UK private rents (PIPR): annual % change\",  # chart title\n     xaxt = \"n\")  # suppress default x-axis (we'll add custom labels)\n\n# Add sparse x-axis labels (every 6th data point to avoid overcrowding)\nlabs_every &lt;- 6  # show a label every 6 months\nat_idx &lt;- seq(1, nrow(pipr), by = labs_every)  # create sequence of index positions\naxis(1,  # 1 means x-axis (bottom)\n     at = at_idx,  # positions where labels appear\n     labels = pipr$month[at_idx],  # the actual month labels\n     las = 2,  # rotate labels perpendicular to axis\n     cex.axis = 0.7)  # make label font smaller (70% of default)\n\n# Add subtitle showing data source directly under the title\ntitle(sub = \"Source: ONS - Index of Private Housing Rental Prices (PIPR)\", \n      cex.sub = 0.8,  # 80% of normal text size\n      font.sub = 3)  # 3 means italic font\n\n# Add horizontal reference line at 2% (Bank of England inflation target)\nabline(h = 2,  # horizontal line at y = 2\n       lty = 2,  # line type 2 = dashed\n       col = \"blue\")  # blue color\n\n# Add text label for the reference line\ntext(x = 40,  # position at start of time series\n     y = 2,  # at the 2% level\n     labels = \"Inflation target (2%)\",  # the text to display\n     pos = 3,  # position 3 = above the point\n     col = \"blue\",  # match the line color\n     cex = 0.8)  # slightly smaller text (80% of default)\n\n\n\n\n\n\n\n\n💭 Reflect: Identify periods of fastest rent inflation and periods of slowdown. What macro factors might line up with these shifts?\n\n\n\nPart E — Import PIPR as CSV (alternative)\nRecall we downloaded the CSV version from the PIPR page. Import the file with readr::read_csv().\n\n# Open the library to read csv comma delimited files.\nlibrary(readr)\n# Create the object pipr_csv to store the data\npipr_csv &lt;- read_csv(\"data/pipr_monthly.csv\", skip =7)  # adjust file name\n\nRows: 116 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Date\ndbl (5): UK, England, Wales, Scotland, Northern Ireland\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# colums/variable names\nnames(pipr_csv)\n\n[1] \"Date\"             \"UK\"               \"England\"          \"Wales\"           \n[5] \"Scotland\"         \"Northern Ireland\"\n\n# quick overview of the data (tibble)\nhead(pipr_csv)\n\n# A tibble: 6 × 6\n  Date      UK England Wales Scotland `Northern Ireland`\n  &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;              &lt;dbl&gt;\n1 Jan-16   3.3     3.5   1.2      2.1                1.1\n2 Feb-16   3.3     3.5   0.8      1.8                1.3\n3 Mar-16   3.4     3.6   0.8      1.6                1.4\n4 Apr-16   3.3     3.5   0.6      1.5                1.6\n5 May-16   3.2     3.3   0.4      1.4                1.6\n6 Jun-16   3.3     3.5   0.3      1.1                1.6\n\n\n\n\n\nPart G — Compare UK with a region\nThe workbook includes regions (England, Wales, Scotland, Northern Ireland), select one annual % change column and plot together with the UK.\n\nplot(\n    pipr$uk, type = \"l\", lwd = 2,\n    xlab = \"Month\", ylab = \"Annual % change\",\n    main = \"PIPR annual % change: UK vs UK countries to Aug 2025\",\n    ylim = c(0,12)\n  )\n# Use the function 'lines' to add the time series to the initial plot.\nlines(pipr$england, lwd= 1, col=\"red\")\nlines(pipr$scotland, lwd= 1, col=\"blue\")\nlines(pipr$wales, lwd= 1, col=\"green\")\nlines(pipr$n_ireland, lwd= 1, col=\"grey\")\n\nlegend(\n  \"topleft\",\n  legend = c(\"UK\", \"England\", \"Scotland\", \"Wales\", \"N. Ireland\"),\n  col    = c(\"black\", \"red\", \"blue\", \"green\", \"grey\"),\n  lwd    = c(2, 1, 1, 1, 1),\n  lty    = 1,\n  bty    = \"n\"       # no box around legend (optional)\n  # inset = 0.02     # nudge inward if needed\n)\n\n\n\n\n\n\n\n\n💭 Reflect: Did the countries move broadly with the UK or diverge materially? What local factors could explain divergence?\n\n\n\nPart H — Help pages and plotting extras\nLet’s explore ?plot in the console. This will launch a help page in the help tab.\nWe also explore curve() and abline().\n\n# ?plot   # open during interactive session\ncurve(sin(x), from = 0, to = 6.28, xlab = \"x\", ylab = \"y = sin(x)\")\nabline(h = 0, v = 5, lty = 2)\n\n\n\n\n\n\n\n\n💭 Reflect: Which plot() arguments improved readability most? Explain the use of las, xaxt, lwd, and xlim.\n\n\n\nPart I — Access the data of ‘Principles of Econometrics’\nYou can also explore textbook datasets with PoEdata for practice with scatter plots and abline(lm()).\n\n#install.packages(c(\"remotes\", \"pkgbuild\"))   # helper packages\n#pkgbuild::has_build_tools(debug = TRUE)       # should say TRUE on Windows\n#remotes::install_github(\"ccolonescu/PoEdata\")\nlibrary(PoEdata)\ndata()\ndata(\"andy\")\nhead(andy)\n\n  sales price advert\n1  73.2  5.69    1.3\n2  71.8  6.49    2.9\n3  62.4  5.63    0.8\n4  67.4  6.22    0.7\n5  89.3  5.02    1.5\n6  70.3  6.41    1.3\n\n\n\n\n\nBest practices\n\nReproducibility: Prefer code over manual spreadsheet edits; paste RStudio’s generated import code into your script.\nPaths & naming: Use a data/ subfolder; avoid spaces; use forward slashes.\nTypes & missing values: Check with str(), summary(), anyNA(); coerce explicitly when needed.\nAxis labelling: Sparse, rotated labels help on monthly series.\nVersioning: Save dated copies of raw downloads (e.g., pipr_monthly_2025-09-19.xlsx).\n\n\n🏁 End of Lab 4\n🛑 Remember to save your script 💾",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervision 1</span>"
    ]
  },
  {
    "objectID": "supervision_1.html#lab-5-data-visualisation",
    "href": "supervision_1.html#lab-5-data-visualisation",
    "title": "Supervision 1",
    "section": "🧪 Lab 5 — Data Visualisation",
    "text": "🧪 Lab 5 — Data Visualisation\nAdapted from: A Guide to Data Visualisation in R for Beginners\n\n\nLearning outcomes\nBy the end of this lab you will be able to:\n\nExplore the state.x77 dataset in R.\nGenerate basic descriptive statistics.\nCreate simple plots using base R graphics.\nAdd labels, titles, and colours to plots.\nCompare different chart types (scatter, bar, histogram, boxplot).\nUse multi-panel displays to visualise multiple plots at once.\n\n\n\n\nPrerequisites\n\nR (≥ 4.0) and RStudio.\nNo additional packages required beyond base R (optional: ggplot2 for comparison).\n\n\n\n\nPart A — Load dataset\n\nlibrary(datasets)\nstate_data &lt;- as.data.frame(datasets::state.x77)\nView(state_data)\n\n\n\n\nPart B — Data exploration\n\nstr(state_data)\n\n'data.frame':   50 obs. of  8 variables:\n $ Population: num  3615 365 2212 2110 21198 ...\n $ Income    : num  3624 6315 4530 3378 5114 ...\n $ Illiteracy: num  2.1 1.5 1.8 1.9 1.1 0.7 1.1 0.9 1.3 2 ...\n $ Life Exp  : num  69 69.3 70.5 70.7 71.7 ...\n $ Murder    : num  15.1 11.3 7.8 10.1 10.3 6.8 3.1 6.2 10.7 13.9 ...\n $ HS Grad   : num  41.3 66.7 58.1 39.9 62.6 63.9 56 54.6 52.6 40.6 ...\n $ Frost     : num  20 152 15 65 20 166 139 103 11 60 ...\n $ Area      : num  50708 566432 113417 51945 156361 ...\n\n\n💭 Reflect: Explore possible research aims from the state.x77 dataset. What variables are present in state.x77? How many states (rows) and variables (columns) are there? Which are continuous, and which categorical (if any)?\n\n\n\nPart C — Beginning and end of the dataset\n\nhead(state_data, 7)\n\n            Population Income Illiteracy Life Exp Murder HS Grad Frost   Area\nAlabama           3615   3624        2.1    69.05   15.1    41.3    20  50708\nAlaska             365   6315        1.5    69.31   11.3    66.7   152 566432\nArizona           2212   4530        1.8    70.55    7.8    58.1    15 113417\nArkansas          2110   3378        1.9    70.66   10.1    39.9    65  51945\nCalifornia       21198   5114        1.1    71.71   10.3    62.6    20 156361\nColorado          2541   4884        0.7    72.06    6.8    63.9   166 103766\nConnecticut       3100   5348        1.1    72.48    3.1    56.0   139   4862\n\ntail(state_data, 5)\n\n              Population Income Illiteracy Life Exp Murder HS Grad Frost  Area\nVirginia            4981   4701        1.4    70.08    9.5    47.8    85 39780\nWashington          3559   4864        0.6    71.72    4.3    63.5    32 66570\nWest Virginia       1799   3617        1.4    69.48    6.7    41.6   100 24070\nWisconsin           4589   4468        0.7    72.48    3.0    54.5   149 54464\nWyoming              376   4566        0.6    70.29    6.9    62.9   173 97203\n\n\n\n\n\nPart D — Descriptive statistics\nThere are several functions to obtain descriptive statistics, summary, psych, skimr, etc.\nHere is an example with summary.\n\nsummary(state_data)\n\n   Population        Income       Illiteracy       Life Exp    \n Min.   :  365   Min.   :3098   Min.   :0.500   Min.   :67.96  \n 1st Qu.: 1080   1st Qu.:3993   1st Qu.:0.625   1st Qu.:70.12  \n Median : 2838   Median :4519   Median :0.950   Median :70.67  \n Mean   : 4246   Mean   :4436   Mean   :1.170   Mean   :70.88  \n 3rd Qu.: 4968   3rd Qu.:4814   3rd Qu.:1.575   3rd Qu.:71.89  \n Max.   :21198   Max.   :6315   Max.   :2.800   Max.   :73.60  \n     Murder          HS Grad          Frost             Area       \n Min.   : 1.400   Min.   :37.80   Min.   :  0.00   Min.   :  1049  \n 1st Qu.: 4.350   1st Qu.:48.05   1st Qu.: 66.25   1st Qu.: 36985  \n Median : 6.850   Median :53.25   Median :114.50   Median : 54277  \n Mean   : 7.378   Mean   :53.11   Mean   :104.46   Mean   : 70736  \n 3rd Qu.:10.675   3rd Qu.:59.15   3rd Qu.:139.75   3rd Qu.: 81163  \n Max.   :15.100   Max.   :67.30   Max.   :188.00   Max.   :566432  \n\n\nAnd, another example with the package psych.\n\n#install.packages(\"psych\")   # if not already installed\nlibrary(psych)\n\ndescribe(state_data)\n\n           vars  n     mean       sd   median  trimmed      mad     min\nPopulation    1 50  4246.42  4464.49  2838.50  3384.28  2890.33  365.00\nIncome        2 50  4435.80   614.47  4519.00  4430.08   581.18 3098.00\nIlliteracy    3 50     1.17     0.61     0.95     1.10     0.52    0.50\nLife Exp      4 50    70.88     1.34    70.67    70.92     1.54   67.96\nMurder        5 50     7.38     3.69     6.85     7.30     5.19    1.40\nHS Grad       6 50    53.11     8.08    53.25    53.34     8.60   37.80\nFrost         7 50   104.46    51.98   114.50   106.80    53.37    0.00\nArea          8 50 70735.88 85327.30 54277.00 56575.72 35144.29 1049.00\n                max     range  skew kurtosis       se\nPopulation  21198.0  20833.00  1.92     3.75   631.37\nIncome       6315.0   3217.00  0.20     0.24    86.90\nIlliteracy      2.8      2.30  0.82    -0.47     0.09\nLife Exp       73.6      5.64 -0.15    -0.67     0.19\nMurder         15.1     13.70  0.13    -1.21     0.52\nHS Grad        67.3     29.50 -0.32    -0.88     1.14\nFrost         188.0    188.00 -0.37    -0.94     7.35\nArea       566432.0 565383.00  4.10    20.39 12067.10\n\n\nHowever, some users writing research might want more elaborated outputs.\n\nlibrary(dplyr)\nlibrary(gtsummary)\nlibrary(flextable)\n\n# Recreate data inside this chunk (Quarto runs in a clean session)\nstate_data &lt;- as.data.frame(datasets::state.x77)\nstate_data$region &lt;- factor(datasets::state.region)  # add stratifier\nnames(state_data) &lt;- make.names(names(state_data))   # Life.Exp, HS.Grad, etc.\n\ntbl &lt;- state_data |&gt;\n  gtsummary::tbl_summary(\n    by = region,\n    type = gtsummary::all_continuous() ~ \"continuous2\",\n    statistic = gtsummary::all_continuous() ~ c(\"{mean} ({sd})\", \"{median} [{p25}, {p75}]\"),\n    digits = gtsummary::all_continuous() ~ 1,\n    missing = \"ifany\",\n    label = list(\n      Income ~ \"Per-capita Income\",\n      Illiteracy ~ \"Illiteracy (%)\",\n      Life.Exp ~ \"Life Expectancy\",\n      Murder ~ \"Murder Rate\",\n      HS.Grad ~ \"High-School Grad (%)\",\n      Frost ~ \"Frost Days\",\n      Area ~ \"Area (sq mi)\"\n    )\n  ) |&gt;\n  gtsummary::add_overall(last = TRUE) |&gt;\n  gtsummary::add_p() |&gt;\n  gtsummary::bold_labels() |&gt;\n  gtsummary::modify_caption(\"**Table: Descriptive statistics by region**\")\n\ntbl\n\n\n\nTable 2.1: Table: Descriptive statistics by region\n\n\n\n\n\n\n\nTable: Descriptive statistics by region\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nNortheast\nN = 9\nSouth\nN = 16\nNorth Central\nN = 12\nWest\nN = 13\nOverall\nN = 50\np-value1\n\n\n\n\nPopulation\n\n\n\n\n\n\n\n\n\n\n0.054\n\n\n    Mean (SD)\n5,495.1 (6,079.6)\n4,208.1 (2,779.5)\n4,803.0 (3,702.8)\n2,915.3 (5,578.6)\n4,246.4 (4,464.5)\n\n\n\n\n    Median [Q1, Q3]\n3,100.0 [931.0, 7,333.0]\n3,710.5 [2,528.0, 4,956.0]\n4,255.0 [1,912.0, 7,212.0]\n1,144.0 [746.0, 2,284.0]\n2,838.5 [1,058.0, 4,981.0]\n\n\n\n\nPer-capita Income\n\n\n\n\n\n\n\n\n\n\n0.019\n\n\n    Mean (SD)\n4,570.2 (559.1)\n4,011.9 (605.5)\n4,611.1 (283.1)\n4,702.6 (663.9)\n4,435.8 (614.5)\n\n\n\n\n    Median [Q1, Q3]\n4,558.0 [4,281.0, 4,903.0]\n3,848.0 [3,620.5, 4,444.5]\n4,594.5 [4,463.0, 4,713.0]\n4,660.0 [4,347.0, 4,963.0]\n4,519.0 [3,983.0, 4,815.0]\n\n\n\n\nIlliteracy (%)\n\n\n\n\n\n\n\n\n\n\n&lt;0.001\n\n\n    Mean (SD)\n1.0 (0.3)\n1.7 (0.6)\n0.7 (0.1)\n1.0 (0.6)\n1.2 (0.6)\n\n\n\n\n    Median [Q1, Q3]\n1.1 [0.7, 1.1]\n1.8 [1.4, 2.2]\n0.7 [0.6, 0.8]\n0.6 [0.6, 1.5]\n1.0 [0.6, 1.6]\n\n\n\n\nLife Expectancy\n\n\n\n\n\n\n\n\n\n\n&lt;0.001\n\n\n    Mean (SD)\n71.3 (0.7)\n69.7 (1.0)\n71.8 (1.0)\n71.2 (1.4)\n70.9 (1.3)\n\n\n\n\n    Median [Q1, Q3]\n71.2 [70.6, 71.8]\n70.1 [68.9, 70.4]\n72.3 [70.8, 72.6]\n71.7 [70.3, 72.1]\n70.7 [70.1, 71.9]\n\n\n\n\nMurder Rate\n\n\n\n\n\n\n\n\n\n\n&lt;0.001\n\n\n    Mean (SD)\n4.7 (2.7)\n10.6 (2.6)\n5.3 (3.6)\n7.2 (2.7)\n7.4 (3.7)\n\n\n\n\n    Median [Q1, Q3]\n3.3 [3.1, 5.5]\n10.9 [9.0, 12.4]\n3.8 [2.3, 8.4]\n6.8 [5.0, 9.7]\n6.9 [4.3, 10.7]\n\n\n\n\nHigh-School Grad (%)\n\n\n\n\n\n\n\n\n\n\n&lt;0.001\n\n\n    Mean (SD)\n54.0 (3.9)\n44.3 (5.7)\n54.5 (3.6)\n62.0 (3.5)\n53.1 (8.1)\n\n\n\n\n    Median [Q1, Q3]\n54.7 [52.5, 57.1]\n41.7 [40.3, 49.7]\n53.3 [52.7, 58.3]\n62.6 [59.5, 63.9]\n53.3 [47.8, 59.2]\n\n\n\n\nFrost Days\n\n\n\n\n\n\n\n\n\n\n&lt;0.001\n\n\n    Mean (SD)\n132.8 (30.9)\n64.6 (31.3)\n138.8 (23.9)\n102.2 (68.9)\n104.5 (52.0)\n\n\n\n\n    Median [Q1, Q3]\n127.0 [115.0, 161.0]\n67.5 [42.5, 90.0]\n133.0 [123.0, 154.5]\n126.0 [32.0, 155.0]\n114.5 [65.0, 140.0]\n\n\n\n\nArea (sq mi)\n\n\n\n\n\n\n\n\n\n\n&lt;0.001\n\n\n    Mean (SD)\n18,141.0 (18,075.7)\n54,605.1 (57,965.3)\n62,652.0 (14,967.0)\n134,463.0 (134,981.7)\n70,735.9 (85,327.3)\n\n\n\n\n    Median [Q1, Q3]\n9,027.0 [7,521.0, 30,920.0]\n46,113.0 [34,937.5, 53,017.5]\n62,906.0 [55,106.0, 76,219.0]\n103,766.0 [82,677.0, 121,412.0]\n54,277.0 [36,097.0, 81,787.0]\n\n\n\n\n\n1 Kruskal-Wallis rank sum test\n\n\n\n\n\n\n\n\n\n\n\n💭 Reflect: Which socio-economic indicators show the widest spread?\n\n\n\nPart E — Graphics package\nReview other type of graphics or plotting functions that can be useful for land economy research. The folloging code opens a new tab with more details.\n\nlibrary(help = \"graphics\")\n\n\n\n\nPart F — Scatterplots\n\nplot(state_data$Income, state_data$Illiteracy)\n\n\n\n\n\n\n\n\n💭 Reflect: Do states with higher income tend to have lower illiteracy rates?\n\n\n\nPart G — Entire dataset plot\n\nplot(state_data)\n\n\n\n\n\n\n\n\n💭 Reflect: Which pairs of socio-economic indicators appear strongly related?\n\n\n\nPart H — Points and lines\n\nplot(state_data$Life.Exp, type = \"b\")\n\n\n\n\n\n\n\nplot(state_data$Life.Exp, type = \"h\")\n\n\n\n\n\n\n\n\n💭 Reflect: Which representation communicates variation in life expectancy more clearly?\n\n\n\nPart J — Help\n\n?plot\n\nstarting httpd help server ... done\n\n\n💭 Reflect: Identify one new argument from the help page that could improve your plot.\n\n\n\nPart K — Labels and titles\n\nplot(state_data$Income,\n     xlab = \"State Index\",\n     ylab = \"Per Capita Income\",\n     main = \"Income levels across US states\",\n     col = \"blue\")\n\n\n\n\n\n\n\n\n💭 Reflect: Do the labels improve clarity? Suggest improvements if needed.\n\n\n\nPart L — Horizontal bar plot\n\n# Take the Murder column as a named vector (names = state names)\nm &lt;- state_data$Murder\nnames(m) &lt;- rownames(state_data)\n\n# Pick the top 10 states by murder rate (highest first)…\nm_top &lt;- sort(m, decreasing = TRUE)[1:10]\n# …then re-sort ascending so the biggest bar appears at the TOP in a horizontal plot\nm_top &lt;- sort(m_top)\n\n# Make the plot: horizontal bars, tidy labels\nbp &lt;- barplot(\n  m_top,                                # bar heights (uses names(m_top) as labels)\n  horiz = TRUE,                         # horizontal bars\n  las = 1,                              # horizontal axis labels\n  cex.names = 0.8,                      # slightly smaller labels\n  space = 0.02,                         # small gaps between bars\n  col = rainbow(length(m_top)),         # one colour per bar (top 10 only)\n  main = \"Top 10 States by Murder Rate\",\n  xlab = \"Murder arrests per 100,000\",\n  xlim = c(0, max(m_top) * 1.1)         # a bit of right padding\n)\n\n\n\n\n\n\n\n\n💭 Reflect: - Which states stand out with particularly high murder rates? - Can you adapt the script to barplot the 10 states with lower murder rates?\n\n\n\nPart M — Vertical bar plot\nWe could use the previous script to barplot vertically by changing the option from horiz = TRUE to horiz = FALSEas in the following routine/code.\n\nbarplot(state_data$Illiteracy,\n        main = \"Illiteracy Rate by State\",\n        xlab = \"% Illiteracy\",\n        col = \"orange\",\n        horiz = FALSE)\n\n\n\n\n\n\n\n\n💭 Reflect: How might this plot inform discussions on educational policy?\n\n\n\nPart N — Histograms\n\nhist(state_data$Income,\n     main = \"Distribution of State Incomes\",\n     xlab = \"Per Capita Income\",\n     col = \"green\")\n\n\n\n\n\n\n\n\n💭 Reflect: Is the income distribution symmetric, skewed, or multi-modal?\n\n\n\nPart O — Boxplots\n\nboxplot(state_data$Life.Exp)\n\n\n\n\n\n\n\n\n💭 Reflect: What does the boxplot reveal about state-level life expectancy?\n\n\n\nPart P — Multiple boxplots\n\nboxplot(state_data[, c(\"Life.Exp\", \"Murder\", \"HS.Grad\", \"Frost\")],\n        main = \"Socio-economic Indicators across States\")\n\n\n\n\n\n\n\n\n💭 Reflect: Which variable shows the most variability? The least?\n\n\n\nPart Q — Grid of charts\nWe can create a grid of charts with the function par; (3,3) means a grid made of 3 rows x 3 columns.\n\npar(mfrow = c(3,3), mar = c(2,5,2,1), las = 1, bty = \"n\")\n\nplot(state_data$Population)\nplot(state_data$Income, state_data$Illiteracy)\nplot(state_data$Life.Exp, type = \"c\")\nplot(state_data$Life.Exp, type = \"s\")\nplot(state_data$Life.Exp, type = \"h\")\n\nbarplot(state_data$Income, main = \"Income levels\", col = \"blue\", horiz = TRUE)\nhist(state_data$Murder)\nboxplot(state_data$HS.Grad)\nboxplot(state_data[, c(\"Population\", \"Income\", \"Illiteracy\", \n\"Life.Exp\")], main = \"Multiple Box plots\")\n\n\n\n\n\n\n\n\n\n\n\nBest practices\n\nAlways check the structure of socio-economic data before plotting.\nMatch plot type to variable type.\nAdd descriptive titles and labels.\nAvoid misleading graphics (bar plots for continuous data should be used cautiously).\nVisualise relationships before modelling.\n\n\n\n\nFurther visualisation packages\n\nlattice: kernel density plots\nggplot2: flexible grammar of graphics\nplotly: interactive plots\nmaps: plot country maps\n\nMore resources: towardsdatascience.com article\n\n🏁 End of Lab 5 🛑 Remember to save your script 💾",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervision 1</span>"
    ]
  },
  {
    "objectID": "supervision_1.html#lab-6-using-github-copilot-in-rstudio",
    "href": "supervision_1.html#lab-6-using-github-copilot-in-rstudio",
    "title": "Supervision 1",
    "section": "🧪 Lab 6 — Using GitHub Copilot in RStudio",
    "text": "🧪 Lab 6 — Using GitHub Copilot in RStudio\n\nGoal\nLearn to use GitHub Copilot inside RStudio to speed up routine coding, while staying in control of quality, privacy, and reproducibility.\n\n\n\nLearning outcomes\nBy the end you will be able to:\n\nEnable and sign into GitHub Copilot in RStudio.\nAccept, reject, or modify Copilot autocomplete suggestions effectively.\nUse comment-first prompting to steer suggestions.\nDiagnose and fix typical Copilot mistakes (wrong column names, redundant args, etc.).\nApply safe-use practices (privacy, determinism, avoiding “vibe coding”).\n\n\n\n\nPrerequisites\n\nR (≥ 4.3 recommended) and RStudio (a recent version with Copilot support).\nA GitHub account with Copilot access (student/educator plans are available).\nPackages: tidyverse, palmerpenguins, readxl, ggplot2.\n\ninstall.packages(c(\"tidyverse\", \"palmerpenguins\", \"readxl\", \"ggplot2\"))\n\nNote: Column names for the penguins dataset may differ depending on source. Treat this as a feature to practice Copilot-aware debugging.\n\n\n\n\nPart A — Set up Copilot in RStudio\n\nOpen RStudio → Tools → Global Options → Copilot.\nEnable GitHub Copilot and sign in with GitHub.\nCreate a new R Script (File → New File → R Script).\nType some characters to see ghost text suggestions.\n\nTab to accept, Esc to dismiss, or keep typing to ignore.\n\n\n\n\nReflection A\nExplain in one sentence the difference between ghost text and code you’ve actually inserted.\n\n\n\nPart B — First suggestions with ggplot2 (20 min)\nWe’ll use penguins to practice.\n\nAdd a guiding comment:\n\n# Task: scatter plot of bill length (x) vs body mass (y), colour by species\n\nStart a plot:\n\nlibrary(ggplot2)\n# library(palmerpenguins)\n# penguins &lt;- palmerpenguins::penguins\n\np &lt;- ggplot(penguins, aes(x = bill_length_mm, y = body_mass_g, colour = species)) +\n  geom_point()\n\nRun and, if errors occur, inspect:\n\nglimpse(penguins)\nnames(penguins)\n\nFix names if needed (bill_length_mm → bill_len).\n\n\n\nMini-exercises\n\nAdd title, caption, and axis labels:\n\n# Add a title, caption, and nicer axis labels\n\nAdd smoother:\n\n# Add a loess smoother without confidence band\n\n\nReflection B\nWhen is Copilot faster? When slower?\n\n\n\nPart C — Comment-first prompting for histograms\n\nAdd a guiding comment:\n\n# Histogram of body mass by species, overlapping with transparency and an accessible palette\n\nInspect Copilot’s suggestion, remove redundant arguments, and adjust to:\n\ngeom_histogram(position = \"identity\", alpha = 0.5)\n\nCommit the cleaned version.\n\n\n\nReflection C\nReplace colour with fill, adjust legend, apply minimal theme.\n\n\n\nPart D — Loading local data safely\nCopilot does not know your file system.\n\nSave Scooby.xlsx in your data/ folder.\n\nDownload Scooby-doo.csv\n\nLoad explicitly:\n\nlibrary(readxl)\nscooby &lt;- read_xlsx(\"data/Scooby.xlsx\")\n\nAdd prompt:\n\n# Quick EDA: glimpse, summary, and count episodes by network\n\nEdit Copilot’s code before running.\n\n\n\nReflection D\nWhy be explicit with data loading and library imports?\n\n\n\nPart E — Boxplots\nVisualise IMDb ratings by network.\n\nPrompt:\n\n# Boxplot of IMDb (y) by network (x); tilt x labels; use fill instead of colour\n\nEdit column names if needed.\nAdd improvements:\n\n# Make it horizontal, tidy legend, and add labs\n\n\nExtension E\nOrder networks by median rating.\n\n\n\nPart F — Best practices\n\nPrivacy: Never expose credentials/confidential data.\nNon-determinism: Review suggestions before accepting.\nNo vibe coding: Don’t accept what you don’t understand.\nContext helps: Use clean code and comments.\nReproducibility: Remove redundant args, consider renv.\n\ninstall.packages(\"renv\")\nrenv::init()\n\n\n\nPart G — Check-off & submission\n\nCopilot enabled and signed in.\nOne scatter plot (corrected column names).\nOne histogram (transparency + accessible palette).\nOne boxplot (labels/theme improved).\nReflection paragraph on Copilot.\n\n\n\n\nTroubleshooting\n\nSuggestions not appearing? Check Global Options → Copilot.\nWrong dataset/columns? Run names()/glimpse().\nOverconfident code? Scale back, test small steps.\n\n\n\n\nFurther practice\n\nRewrite prompts as comments for grouped summaries, joins, faceted plots.\nCompare Copilot vs manual solutions.\n\n\n🏁 End of lab 6 🛑 Remember to save your script 💾",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervision 1</span>"
    ]
  },
  {
    "objectID": "supervision_2.html",
    "href": "supervision_2.html",
    "title": "Supervision 2",
    "section": "",
    "text": "🧪 Lab 7 — The Simple Linear Regression Model",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervision 2</span>"
    ]
  },
  {
    "objectID": "supervision_2.html#lab-7-the-simple-linear-regression-model",
    "href": "supervision_2.html#lab-7-the-simple-linear-regression-model",
    "title": "Supervision 2",
    "section": "",
    "text": "🎯 Learning outcomes\nBy the end of this lab you will be able to:\n\nUnderstand the mathematical formulation of the simple linear regression model\nEstimate regression coefficients using lm() in R\nInterpret the intercept and slope parameters in an applied context\nExtract and examine regression output using summary() and related functions\nVisualize the fitted regression line on a scatter plot\nAssess the quality of model fit using residuals and R-squared\n\n\n\n\n🧰 Prerequisites\n\nR (≥ 4.0) and RStudio installed\nCompleted Labs 1-6\nPackages (install once):\n\ninstall.packages(c(\"remotes\", \"PoEdata\"))\nremotes::install_github(\"ccolonescu/PoEdata\")\n\nSummary. In simple linear regression, we use one independent variable to predict a dependent variable. The relationship is represented by a straight line with an intercept and a slope.\nIntroduction. We explore the fundamentals of the Simple Linear Regression Model, a method to understand the relationship between two variables. This model is foundational for more advanced analyses.\nKey terms. The dependent variable \\((y)\\) is what we aim to predict; the independent variable, \\(x\\), explains \\(y\\).\n\n\nThe General Model\nAssume a linear relationship between the conditional expectation of (y) and (x):\n\\[\ny_i = \\beta_1 + \\beta_2 x_i + e_i \\tag{1}\n\\]\n\n\\(\\beta_1\\): intercept\n\\(\\beta_2\\): slope\n\n\\(e_i\\): error term with variance \\(\\sigma^2\\)\n\n\\(i=1,\\dots,N\\): observation index\n\nThe predicted (estimated) value of \\(y\\) given \\(x\\) is:\n\\[\n\\hat{y} = b_1 + b_2 x \\tag{2}\n\\]\nAssumptions:\n\nnon-random (x);\nconstant error variance (homoskedasticity);\nerrors uncorrelated across observations;\n\\(E[e_i \\mid x_i]=0\\).\n\n\nPART A. Example: Food Expenditure vs Income\n# Recall to install the package PoEdata once if needed:\ninstall.packages(\"remotes\")\nremotes::install_github(\"ccolonescu/PoEdata\")\n\nlibrary(PoEdata)\ndata(\"cps_small\")\nplot(cps_small$educ, cps_small$wage, xlab=\"Education\", ylab=\"Wage\")\n\n\n\n\n\n\n\n# Food data\ndata(\"food\")\nhead(food)\n\n  food_exp income\n1   115.22   3.69\n2   135.98   4.39\n3   119.34   4.75\n4   114.96   6.03\n5   187.05  12.47\n6   243.92  12.98\n\nplot(food$income, food$food_exp,\n     ylim=c(0, max(food$food_exp)),\n     xlim=c(0, max(food$income)),\n     xlab=\"weekly income in $100\", ylab=\"weekly food expenditure in $\",\n     type=\"p\")\n\n\n\n\n\n\n\n\n\n\nPART B. Estimating a Linear Regression\nFor the food data the model is\n\\[\n\\text{food\\_exp}_i = \\beta_1 + \\beta_2\\,\\text{income}_i + e_i.\\tag{3}\n\\]\n\nlibrary(PoEdata)\nmod1 &lt;- lm(food_exp ~ income, data = food)\nb1 &lt;- coef(mod1)[[1]]\nb2 &lt;- coef(mod1)[[2]]\nsmod1 &lt;- summary(mod1)\nsmod1\n\n\nCall:\nlm(formula = food_exp ~ income, data = food)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-223.025  -50.816   -6.324   67.879  212.044 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   83.416     43.410   1.922   0.0622 .  \nincome        10.210      2.093   4.877 1.95e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 89.52 on 38 degrees of freedom\nMultiple R-squared:  0.385, Adjusted R-squared:  0.3688 \nF-statistic: 23.79 on 1 and 38 DF,  p-value: 1.946e-05\n\n\nAdd the regression line to the scatter plot:\n\nplot(food$income, food$food_exp,\n     ylim=c(0, max(food$food_exp)),\n     xlim=c(0, max(food$income)),\n     xlab=\"weekly income in $100\", ylab=\"weekly food expenditure in $\",\n     type = \"p\")\nabline(b1, b2)\n\n\n\n\n\n\n\n\nRetrieve common results:\n\nnames(mod1)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\nnames(smod1)\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\nmod1$coefficients\n\n(Intercept)      income \n   83.41600    10.20964 \n\nsmod1$coefficients\n\n            Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 83.41600  43.410163 1.921578 6.218242e-02\nincome      10.20964   2.093264 4.877381 1.945862e-05\n\n\n\n\nPART C. MCQs\n\n\n\n\n1) What relationship does simple linear regression assume between y and x?\n\n\n Quadratic Logarithmic Linear Exponential\n\n\n\n\nHint\n\nThink straight line: one slope + one intercept.\n\nβ\n\n\n2) In the model y = β₀ + β₁ x + ε, what does β₁ represent?\n\n\n The average of y The error variance Expected change in y for a one-unit increase in x The intercept\n\n\n\n\nHint\n\nΔy for +1 in x.\n\n\n\n3) In the model y = β₀ + β₁ x + ε, what does β₀ represent?\n\n\n The slope of x The expected value of y when x = 0 The variance of ε Correlation between x and y\n\n\n\n\nHint\n\nValue of y at x = 0.\n\n\n\n4) A residual is defined as:\n\n\n Predicted y minus observed y Observed x minus predicted x Observed y minus predicted y The fitted value\n\n\n\n\nHint\n\nActual − fitted.\n\n\n\n5) Ordinary Least Squares (OLS) chooses coefficients to:\n\n\n Maximize R² Minimize the sum of absolute residuals Minimize the sum of squared residuals Maximize the likelihood under normal errors only\n\n\n\n\nHint\n\nSquares penalize big errors more.\n\n\n\n6) With an intercept in the model, OLS residuals:\n\n\n Sum to a positive number Sum to a negative number Sum to zero Sum to the sample mean of y\n\n\n\n\nHint\n\n∑eᵢ = 0 and Xᵀe = 0 when an intercept is included.\n\n\n\n7) In simple OLS with an intercept, the fitted line passes through…\n\n\n The origin (0,0) The point (x̄, ȳ) The median of x and y The maximum of y\n\n\n\n\nHint\n\nIt goes through the means.\n\n\n\n8) Which R function fits a simple linear regression model?\n\n\n plot() cor() lm() predict()\n\n\n\n\n9) In summary(mod), the p-value for the slope tests:\n\n\n H₀: β₁ ≠ 0 H₀: β₁ = 0 H₀: β₀ = 0 H₀: errors are normal\n\n\n\n\n10) The p-value is best described as:\n\n\n The probability H₁ is true The probability the estimate is correct The probability (under H₀) of a result at least as extreme as observed The Type II error rate\n\n\n\n\n11) A 95% confidence interval for β₁ is generally:\n\n\n Estimate ± 1.96 × σ of y Estimate ± critical t × SE(β₁) Estimate ± 2 × residual SD SE(β₁) ± estimate\n\n\n\n\n12) R² measures:\n\n\n The correlation between x and y Proportion of variance in y explained by x The average residual size The probability the model is correct\n\n\n\n\n13) Adjusted R² differs from R² because it:\n\n\n Ignores sample size Is always larger than R² Penalizes adding predictors relative to sample size Equals 1 − R²\n\n\n\n\n14) To make a prediction at x = x₀, you use:\n\n\n β₁ × x₀ β₀ ÷ x₀ ŷ = β₀ + β₁ × x₀ SE(β₁) × x₀\n\n\n\n\nHint\n\nPlug x₀ into the fitted line.\n\n\n\n15) For OLS to be unbiased, a key assumption is:\n\n\n x is normally distributed Errors have zero variance E[ε | x] = 0 (errors have zero mean conditional on x) y is standardized\n\n\n\n\n16) Homoskedasticity means:\n\n\n Errors are perfectly correlated Variance of errors is constant across x Mean of errors is zero only at x̄ x has constant variance\n\n\n\n\n17) In summary(mod), the t value for β₁ equals:\n\n\n β₁ SE(β₁) Estimate(β₁) / SE(β₁) 1 − p-value\n\n\n\n\nHint\n\nSignal ÷ noise for the slope estimate.\n\n\n\n18) The units of β₁ are best described as:\n\n\n Units of y Units of y per unit of x Unitless Units of x per unit of y\n\n\n\n\n19) Rescaling x from dollars to hundreds of dollars will:\n\n\n Leave β₁ unchanged Rescale β₁ numerically but leave R² and the t-test for β₁ unchanged Change the data but not the model Invalidate OLS\n\n\n\n\n20) After fitting mod &lt;- lm(y ~ x, d), the fitted value at x₀ is:\n\n\n residuals(mod)[x==x₀] predict(mod, newdata = data.frame(x = x₀)) coef(mod)['x'] fitted(mod)[1] always\n\n\n\n\nHint\n\nUse predict() with a small data frame for x₀.\n\n\n\n\n🏁 End of lab 7 🛑 Remember to save your script 💾",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervision 2</span>"
    ]
  },
  {
    "objectID": "supervision_2.html#lab-8-prediction-with-the-linear-regression-model",
    "href": "supervision_2.html#lab-8-prediction-with-the-linear-regression-model",
    "title": "Supervision 2",
    "section": "🧪 Lab 8 — Prediction with the Linear Regression Model",
    "text": "🧪 Lab 8 — Prediction with the Linear Regression Model\n\n🎯 Learning outcomes\nBy the end of this lab you will be able to:\n\nGenerate point predictions and quantify their uncertainty using R’s prediction framework\nDistinguish between confidence intervals for average outcomes and prediction intervals for individual cases, and select the appropriate interval for applied research contexts\nAssess the reliability of coefficient estimates through their variance-covariance structure\nRecognize when predictions involve extrapolation beyond observed data and understand the associated risks\nHandle non-linear relationships through quadratic and log-linear specifications\n\n\n\n\n🧰 Prerequisites\nKnowledge: - Understanding of simple linear regression (Lab 7) - Familiarity with residuals, fitted values, and R² - Basic probability concepts (sampling distributions, standard errors)\nTechnical: - R (≥ 4.0) and RStudio installed - Completed Labs 1-7 - Required packages (install if needed):\ninstall.packages(c(\"remotes\", \"PoEdata\")) \nremotes::install_github(\"ccolonescu/PoEdata\")\nlibrary(PoEdata)\nDatasets:\nfood - household food expenditure and income (40 observations) br - Baton Rouge house prices and characteristics (1,080 observations) —\n\n\n\n\n\n\nNote\n\n\n\nNotation reminder: We use Greek letters (β1,β2​) for true population parameters and Latin letters (b1,b2​) for their sample estimates.\nŷ: predicted y value\n\\(\\hat{\\beta}\\) or \\(\\hat{b}\\) are sometimes used interchangeably with b for estimates\n\n\nIn Lab 7, we learned how to estimate relationships, But estimation is only half the story. In applied research, we often need to make predictions. In applied land economics research, prediction is crucial—whether forecasting property values, estimating rental yields, or projecting land use changes. This lab equips you with the tools to make robust predictions and quantify their uncertainty. Let’s begin with the basic prediction workflow.\nOnce we have estimated our regression coefficients \\(b_1\\) (intercept) and \\(b_2\\) (slope), we can use them to predict food expenditure for any given income level using the fitted regression equation (Eq. 2).\n\\[\n\\hat{y} = b_1 + b_2 x \\tag{2}\n\\]\n⚠️Unit conversion: In the following R script “income = $2000” is “income = 20” (the data is in hundreds of dollars).\n\n#### Step 1: Fit the model (recap from Lab 7)\n\nlibrary(PoEdata)\ndata(\"food\")\n\n# Estimate the food expenditure model\nmod1 &lt;- lm(food_exp ~ income, data = food)\n\n# Review estimates\nsummary(mod1)\n\n\nCall:\nlm(formula = food_exp ~ income, data = food)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-223.025  -50.816   -6.324   67.879  212.044 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   83.416     43.410   1.922   0.0622 .  \nincome        10.210      2.093   4.877 1.95e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 89.52 on 38 degrees of freedom\nMultiple R-squared:  0.385, Adjusted R-squared:  0.3688 \nF-statistic: 23.79 on 1 and 38 DF,  p-value: 1.946e-05\n\n#### Step 2: Create a data frame with target income values\n\n# Scenario: Predict food expenditure for three household types\n# Note: income is in $100s, so divide actual income by 100\nnewx &lt;- data.frame(income = c(20, 25, 27))\n\n#### Step 3: Generate predictions\n\n# The predict() function takes two arguments:\n#   1. A fitted model object (mod1)\n#   2. A data frame with new x-values (must have same column names as original data)\nyhat &lt;- predict(mod1, newx)\n\n# Give friendly names to each prediction so the output is   easy to read\nnames(yhat) &lt;- c(\"Low income = $2000\", \"Median income = $2500\", \"High income = $2700\")\n\n# Show the predictions\nyhat\n\n   Low income = $2000 Median income = $2500   High income = $2700 \n             287.6089              338.6571              359.0764 \n\n#### Step 4: Interpret the results\n\n# A household earning $2,000/week is predicted to spend $287.60 on food per week.\n# This represents about 14.4% of their income (287.6/2000).\n\n# For the median household ($2,500/week), predicted food expenditure is $338.70,\n# or about 13.5% of income.\n\n# Observation: The proportion of income spent on food decreases as income rises—\n# this is consistent with Engel's Law from economics.\n\nWhat’s Missing? Uncertainty!\nThese are point predictions—our single best guess at each income level. But how confident are we?\nConsider the household earning $2,000/week: - We predict they’ll spend $287.60 on food - But there’s sampling uncertainty (our \\(b_1\\) and \\(b_2\\) are estimates) - And individual variation (not all $2,000/week households spend exactly $287.60)\nTwo types of intervals address these concerns:\n\nConfidence intervals answer: “What’s the average food expenditure for all households at this income level?”\nPrediction intervals answer: “What might one specific household at this income level spend?”\n\n\n\n\nUnderstanding Prediction Uncertainty Through Sampling Variability\nOur predictions depend entirely on our estimated coefficients (\\(b_1, b_2\\)). But these are sample estimates—if we collected a different dataset, we’d get different values and therefore different predictions.\nBefore making predictions, it’s important to understand that our coefficient estimates vary across samples. This matters for land economics applications: if we’re advising on property valuations, we need to know how sensitive our predictions are to sampling variation.\n\n\n\n\n\n\nWhat is “sampling with replacement”?\n\n\n\nImagine our 40 observations are numbered balls in an urn. We draw a ball, record its data, put it back, then draw again. Some observations may appear multiple times in a bootstrap sample; others not at all. This mimics the randomness of drawing a new sample from the population.\n\n\nThe following bootstrap exercise (simulating sampling variability) demonstrates this variability by repeatedly resampling our data and re-estimating the model:\n\nN &lt;- nrow(food)   # observations\nC &lt;- 50           # repeats\nS &lt;- 38           # subsample size\n\nsumb2 &lt;- 0        # sum of slopes\nfor (i in 1:C){\n  set.seed(3*i)   # reproducible\n  # Draw a bootstrap sample (with replacement)\n  subsample &lt;- food[sample(1:N, S, replace = TRUE), ]  # bootstrap draw\n  # Fit the model on this bootstrap sample\n  mod2 &lt;- lm(food_exp ~ income, data = subsample)      \n  sumb2 &lt;- sumb2 + coef(mod2)[2]                       # store slope β2\n}\n\nprint(sumb2 / C, digits = 3)   # average slope\n\nincome \n  9.89 \n\n\nCompare this bootsrap average (repeated samples) witht the original OLS regression output (from mod1) for \\(b_2\\). They should be similar, confirming that OLS is unbiased.\n\n\nEstimated Variances and Covariance of Coefficients\nThe variance-covariance matrix tells us two crucial things: 1. Variances (diagonal): How much each coefficient estimate varies 2. Covariances (off-diagonal): How the estimates move together\nThis matters for prediction because uncertainty in \\(\\hat{y}\\) depends on uncertainty in both coefficients and their correlation. For land valuation models, ignoring this covariance can lead to overconfident predictions.\n\n\n\n\n\n\nWarning\n\n\n\nCommon Error in Applied Work Many analysts incorrectly calculate prediction variance as just \\(\\text{Var}(b_1) + x_0^2 \\cdot \\text{Var}(b_2)\\), omitting the covariance term. This can make predictions appear less precise than they actually are.\n\n\nThe following R script extracts estimated variances and covariances from the object mod 1.\n\n# coef variance–covariance matrix.\n# This is a 2×2 symmetric matrix:\n# - Top-left (1884.44): Var(b₁), variance of intercept\n# - Bottom-right (4.38): Var(b₂), variance of slope  \n# - Off-diagonals (-87.78): Cov(b₁,b₂), covariance between coefficients\nvcov(mod1)                       \n\n            (Intercept)     income\n(Intercept)  1884.44226 -85.903157\nincome        -85.90316   4.381752\n\n#### Extract individual components\n(varb1   &lt;- vcov(mod1)[1, 1])        # Var(B1), intercept variance\n\n[1] 1884.442\n\n(varb2   &lt;- vcov(mod1)[2, 2])        # Var(B2), slope variance\n\n[1] 4.381752\n\n(covb1b2 &lt;- vcov(mod1)[1, 2])        # Cov(B1, B2), covariance\n\n[1] -85.90316\n\n\nHaving established how to assess coefficient stability, we now turn to cases where linear relationships are insufficient to capture real-world patterns in land markets.\n\n\nNon-Linear Relationships - When Straight Lines Don’t Fit\nIn practice, many land economic relationships are non-linear: - Property prices don’t increase linearly with size (diminishing returns to scale) - Land values may have threshold effects near transport nodes - Agricultural productivity often follows diminishing returns\nWe’ll explore two common approaches Quadratic Models and Log-Linear Models.\n\nPART A. Quadratic model - for U-shaped or inverted-U relationships\n\\[\ny_i = \\beta_1 + \\beta_2 x_i^2 + e_i.\\tag{5}\n\\]\n\nlibrary(PoEdata)            # load package\ndata(br)                    # dataset\n\nmod3 &lt;- lm(price ~ I(sqft^2), data = br)  # fit model\nb1 &lt;- coef(mod3)[1]        # intercept\nb2 &lt;- coef(mod3)[2]        # coeff on sqft^2\n\nsqftx &lt;- c(2000, 4000, 6000)              # evaluation points\npricex &lt;- b1 + b2 * sqftx^2               # predicted price\nDpriceDsqft &lt;- 2 * b2 * sqftx             # marginal effect d(price)/d(sqft)\nelasticity &lt;- (DpriceDsqft * sqftx) / pricex  # elasticity\n\nb1; b2; DpriceDsqft; elasticity           # output\n\n(Intercept) \n   55776.57 \n\n\nI(sqft^2) \n0.0154213 \n\n\n[1]  61.68521 123.37041 185.05562\n\n\n[1] 1.050303 1.631251 1.817408\n\n\nPlotting two alternatives for the quadratic fit:\n\nmod31 &lt;- lm(price ~ I(sqft^2), data = br)                # fit\nplot(br$sqft, br$price, col = \"grey\",\n     xlab = \"Total square feet\", ylab = \"Sale price, $\") # scatter\n\nb &lt;- coef(mod31)                                         # [intercept], sqft^2\ncurve(b[1] + b[2]*x^2,\n      from = min(br$sqft), to = max(br$sqft),            # range\n      add = TRUE, lwd = 2)                               # fitted curve\n\n\n\n\n\n\n\n\n\nordat &lt;- br[order(br$sqft), ]                             # sort by sqft\nmod31 &lt;- lm(price ~ I(sqft^2), data = ordat)              # fit model on sorted data\n\nplot(br$sqft, br$price, col = \"grey\",                     # scatter\n     main = \"Dataset ordered by 'sqft'\",\n     xlab = \"Total square feet\", ylab = \"Sale price, $\")\n\nlines(fitted(mod31) ~ ordat$sqft)                         # add fitted curve\n\n\n\n\n\n\n\n\n\n\nPART B. Log-Linear Models - for proportional (percentage) relationships\n\\[\n\\log(y_i) = \\beta_1 + \\beta_2 x_i + e_i.\\tag{6}\n\\]\n\nhist(br$price)                # price distribution\n\n\n\n\n\n\n\nhist(log(br$price))           # log-price distribution\n\n\n\n\n\n\n\nmod4 &lt;- lm(log(price) ~ sqft, data = br)  # log-linear fit\nb1 &lt;- coef(mod4)[1]          # intercept\nb2 &lt;- coef(mod4)[2]          # slope\n\n# Back-transform fitted curve to price scale\nordat &lt;- br[order(br$sqft), ]                     # sort by sqft\nmod4  &lt;- lm(log(price) ~ sqft, data = ordat)      # refit (ordered)\nplot(br$sqft, br$price, col = \"grey\",\n     xlab = \"Total square feet\", ylab = \"Sale price, $\")  # scatter\nlines(exp(fitted(mod4)) ~ ordat$sqft, lwd = 2)    # exp of fitted log-price\n\n\n\n\n\n\n\n# ⚠️ Important: Simply exponentiating log predictions (exp(fitted)) gives the median, not the mean. For mean predictions, apply a smearing correction\n\nElasticity and marginal effect at the median price:\n\npricex &lt;- median(br$price)                                   # target price (median)\nsqftx  &lt;- (log(pricex) - coef(mod4)[1]) / coef(mod4)[2]      # back out sqft from log model\n(DyDx &lt;- pricex * coef(mod4)[2])                              # marginal effect d(price)/d(sqft) = b2 * price\n\n    sqft \n53.46495 \n\n(elasticity &lt;- sqftx * coef(mod4)[2])                         # elasticity = (dP/dx)*(x/P) = b2 * sqft\n\n(Intercept) \n  0.9366934 \n\n\nMultiple points:\n\nb1 &lt;- coef(mod4)[1]                     # intercept of log(price) ~ sqft\nb2 &lt;- coef(mod4)[2]                     # slope wrt sqft\n\nsqftx  &lt;- c(2000, 3000, 4000)           # sqft points\npricex &lt;- c(100000, exp(b1 + b2*sqftx)) # prices: first fixed at 100k; others from model\n\nsqftx  &lt;- (log(pricex) - b1) / b2       # implied sqft from each price (now length = length(pricex))\n(elasticities &lt;- b2 * sqftx)            # elasticity = (dP/dx)*(x/P) = b2 * sqft\n\n[1] 0.6743291 0.8225377 1.2338066 1.6450754\n\n\n💡 Land Economics Application In rental valuation, we rarely observe rental income for vacant plots. Prediction intervals help quantify the uncertainty in forecasted rents, which is crucial for development feasibility analysis.\n⚠️ Common Mistake Students often confuse confidence intervals (for the average) with prediction intervals (for individuals). In property valuation, this distinction matters: are you estimating the average price for houses of this type, or the likely sale price of one specific house?\n\n\nPART C. MCQs\n\n\n1) Using B₀ and B₁ primarily helps to:\n\n\n Test normality of residuals Estimate σ² Predict E[y|x] Standardize x and y\n\n\n\n\nHint\n\nPlug x into the fitted line.\n\n\n\n2) The function lm() is used to:\n\n\n Simulate data Draw histograms Estimate a linear model Compute correlation only\n\n\n\n\nHint\n\nIt returns coefficients and a model object.\n\n\n\n3) The function predict() mainly:\n\n\n Fits a model Computes residuals only Estimates y-hat for new data Sorts a data frame\n\n\n\n\nHint\n\nYou pass newdata.\n\n\n\n4) Coefficients are random because they:\n\n\n Depend only on fixed formulas Are set by the user Depend on the sample Don’t change across samples\n\n\n\n\n5) Random subsamples help to:\n\n\n Reduce file size Guarantee higher R² Evaluate stability/variability Eliminate outliers always\n\n\n\n\n6) vcov(model) returns:\n\n\n Residuals Fitted values Variances and covariances of coefficients Confidence intervals for y-hat\n\n\n\n\n7) Using data.frame() with predict() is to:\n\n\n Provide new x values with correct column names Shuffle the rows Change variable types to character Compute R²\n\n\n\n\n8) To request standard errors from predict.lm you set:\n\n\n stderr = TRUE se = TRUE se.fit = TRUE ci = TRUE\n\n\n\n\nHint\n\nIt returns fit and se.fit.\n\n\n\n9) For a confidence interval for the mean response at x₀, use:\n\n\n interval = 'pi' bands = 'mean' interval = 'confidence' type = 'mean'\n\n\n\n\nHint\n\nCI for E[y|x₀].\n\n\n\n10) For an interval predicting an individual future y at x₀, use:\n\n\n interval = 'confidence' interval = 'prediction' level = 'future' type = 'response'\n\n\n\n\nHint\n\nIncludes error variance.\n\n\n\n11) Prediction intervals are typically:\n\n\n Narrower than confidence intervals Wider than confidence intervals Identical to confidence intervals Unrelated to residual variance\n\n\n\n\n12) confint(model) returns:\n\n\n Intervals for y-hat Confidence intervals for coefficients Prediction intervals for new y Variance of residuals\n\n\n\n\n13) For models with multiple predictors, newdata must:\n\n\n Contain columns matching model terms and types Be a vector in any order Include only the response Be sorted by the response\n\n\n\n\nHint\n\nNames must match the formula’s variables.\n\n\n\n14) In a log-linear model log(y) ~ x, to back-transform the mean prediction you should:\n\n\n Use only exp(eta) Multiply exp(eta) by a smearing factor Square the fitted values Add residual SD then exponentiate\n\n\n\n\nHint\n\nDuan’s correction: mean(exp(ε̂)).\n\n\n\n15) A bootstrap (or resampling with replacement) in R uses:\n\n\n sample(x, replace = FALSE) sample(x, replace = TRUE) sort(x) order(x)\n\n\n\n\n16) set.seed(123) is used to:\n\n\n Speed up lm() Normalize variables Make random subsamples reproducible Center predictors\n\n\n\n\n17) For out-of-sample performance, a good metric is:\n\n\n Training R² Test RMSE Number of predictors Intercept size\n\n\n\n\nHint\n\nEvaluate on held-out data.\n\n\n\n18) Extrapolation risk means:\n\n\n Predictions are always unbiased x₀ lies outside the observed x range so predictions can be unreliable R² increases automatically CI and PI become identical\n\n\n\n\n19) Variance of the predicted mean at x₀ depends on:\n\n\n Only σ² Only the intercept x₀ᵀ Var(B̂) x₀ Only sample size\n\n\n\n\nHint\n\nUse the variance–covariance of coefficients.\n\n\n\n20) When predicting with factors, newdata must:\n\n\n Use levels seen in the training data Introduce new unseen levels freely Coerce factors to numeric Omit the factor columns\n\n\n\n\nHint\n\nUnseen levels cause errors in predict().\n\n\n\n\n🏁 End of lab 8 🛑 Remember to save your script 💾",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervision 2</span>"
    ]
  },
  {
    "objectID": "supervision_2.html#lab-9-hypothesis-test-pvalue-testing-linear-combinations",
    "href": "supervision_2.html#lab-9-hypothesis-test-pvalue-testing-linear-combinations",
    "title": "Supervision 2",
    "section": "🧪 Lab 9 — Hypothesis Test, p‑Value & Testing Linear Combinations",
    "text": "🧪 Lab 9 — Hypothesis Test, p‑Value & Testing Linear Combinations\n\n🎯 Learning Outcomes\nBy the end of this lab you will be able to:\n\nFormulate null and alternative hypotheses for regression coefficients\nCompute test statistics and compare them to critical values\nCalculate and interpret p-values for one-tailed and two-tailed tests\nUnderstand the relationship between confidence intervals and hypothesis tests\nTest linear combinations of parameters (e.g., predictions at specific x values)\nApply hypothesis testing to real-world land economics questions\n\n\n\nHypothesis Tests\n🧰 Prerequisites Knowledge:\nUnderstanding of simple linear regression (Lab 7) Familiarity with coefficient estimates and standard errors (Lab 8) Basic understanding of statistical inference (confidence intervals, significance levels)\nTechnical:\nR (≥ 4.0) and RStudio installed Completed Labs 1-8 Required packages:\ninstall.packages(c(\"remotes\", \"PoEdata\")) \nremotes::install_github(\"ccolonescu/PoEdata\")\nlibrary(PoEdata)\n\n\n\nIntroduction: Why Hypothesis Testing Matters\nIn Lab 7, we estimated relationships. In Lab 8, we made predictions. But how do we know if our findings are statistically meaningful? Consider our food expenditure model:\nWe estimated that food spending increases by $10.21 for every $100 increase in income But is this effect real, or could it be due to random chance? Could the true effect actually be zero (no relationship)?\nHypothesis testing provides a formal framework to answer these questions.\n💡 Land Economics Application When valuing agricultural land, we might find that proximity to roads increases land value by £500 per meter. But before advising clients, we need to know: Is this effect statistically significant, or could it be sampling noise?\n\nPART A - Hypothesis Tests for Individual Coefficients\nWe start with two competing claims: \\[\nH_0: \\beta_k = c, \\qquad H_A: \\beta_k \\ne c.\n\\]\nTest statistic:\nUnder the null hypothesis, the test statistic follows a t-distribution: \\[\nt = \\frac{b_k - c}{\\operatorname{se}(b_k)},\\quad t \\sim t_{N-2}.\\tag{6}\n\\]\nwhere:\n\\(b_k\\)​ is our estimated coefficient\n\\(c\\) is the hypothesized value (zero in our case)\n\\(\\operatorname{se}(b_k)\\) is the standard error of \\(b_k\\)\n\\(N−K\\) is degrees of freedom (N observations, K parameters)\nIntuition: This ratio measures how many standard errors our estimate is away from the hypothesized value.\nExample 1: Two-Tailed Test (Is there any effect?) Question: Does income affect food expenditure? (i.e., is \\(β2≠0\\)?)\n\n# Set significance level\nalpha &lt;- 0.05\n\n# Load data and estimate model\nlibrary(PoEdata); library(xtable); library(knitr)\n\nWarning: package 'xtable' was built under R version 4.4.3\n\n\nWarning: package 'knitr' was built under R version 4.4.3\n\ndata(\"food\")\nmod1 &lt;- lm(food_exp ~ income, data=food)\nsmod1 &lt;- summary(mod1)\n\n# Regression output\ntable &lt;- data.frame(xtable(mod1))\nkable(table, caption=\"Regression output showing the coefficients\")\n\n\nRegression output showing the coefficients\n\n\n\nEstimate\nStd..Error\nt.value\nPr…t..\n\n\n\n\n(Intercept)\n83.41600\n43.410163\n1.921578\n0.0621824\n\n\nincome\n10.20964\n2.093263\n4.877381\n0.0000195\n\n\n\n\n# Extract coefficient and standard error\nb2 &lt;- coef(mod1)[[\"income\"]]\nseb2 &lt;- sqrt(vcov(mod1)[2,2])\ndf  &lt;- df.residual(mod1)\n\n# Compute test statistic\nt   &lt;- b2/seb2\n\n# Find critical value for two-tailed test\ntcr &lt;- qt(1-alpha/2, df)\n\nt; tcr\n\n[1] 4.877381\n\n\n[1] 2.024394\n\n# Display results\ncat(\"Test statistic:\", round(t, 3), \"\\n\")\n\nTest statistic: 4.877 \n\ncat(\"Critical value (±):\", round(tcr, 3), \"\\n\")\n\nCritical value (±): 2.024 \n\ncat(\"Decision:\", ifelse(abs(t) &gt; tcr, \"Reject H0\", \"Fail to reject H0\"), \"\\n\")\n\nDecision: Reject H0 \n\n\nInterpretation:\n\nOur test statistic is r round(t, 2), which is much larger than the critical value ±r round(tcr, 2)\nWe reject the null hypothesis\nThere is strong evidence that income affects food expenditure\nRight‑tail and left‑tail versions:\n\nOptional: 📊 Visualizing the Test:\n\n# Create visualization of t-distribution and test\ncurve(dt(x, df), from = -5, to = 5, \n      main = \"Two-Tailed Hypothesis Test\",\n      xlab = \"t-value\", ylab = \"Density\",\n      lwd = 2)\n\n# Shade rejection regions\npolygon(c(-4, seq(-4, -tcr, 0.01), -tcr),\n        c(0, dt(seq(-4, -tcr, 0.01), df), 0),\n        col = \"red\", border = NA, density = 20)\npolygon(c(tcr, seq(tcr, 4, 0.01), 4),\n        c(0, dt(seq(tcr, 4, 0.01), df), 0),\n        col = \"red\", border = NA, density = 20)\n\n# Mark critical values and test statistic\nabline(v = c(-tcr, tcr), col = \"red\", lty = 2, lwd = 2)\n\nabline(v = t, col = \"blue\", lwd = 2)\n\nlegend(\"topright\", \n       legend = c(\"Rejection region (α/2 each)\", \n                  \"Critical values\", \n                  \"Test statistic\"),\n       col = c(\"red\", \"red\", \"blue\"),\n       lty = c(1, 2, 1), lwd = 2, bty = \"n\")\n\n\n\n\n\n\n\n\nExample 2: One-Tailed Tests Sometimes we have a directional hypothesis based on economic theory. Right-Tailed Test: Is the effect of income on food expenditure greater than $5.50?\n\\[\nH_0: \\beta_2 ≤ 5.5, \\qquad H_A: \\beta_2 &gt; 5.5.\n\\]\n\n# Hypothesized value\nc &lt;- 5.5\n\n# Compute test statistic\nt_right &lt;- (b2 - c)/seb2\n\n# Critical value (right tail only)\ntcr_right &lt;- qt(1-alpha, df)\n\ncat(\"Test statistic:\", round(t_right, 3), \"\\n\")\n\nTest statistic: 2.25 \n\ncat(\"Critical value:\", round(tcr_right, 3), \"\\n\")\n\nCritical value: 1.686 \n\ncat(\"Decision:\", ifelse(t_right &gt; tcr_right, \"Reject H0\", \"Fail to reject H0\"), \"\\n\")\n\nDecision: Reject H0 \n\n\nLeft-Tailed Test: Is the effect of income on food expenditure less than $15?\n\\[\nH_0: \\beta_2 ≥ 15, \\qquad H_A: \\beta_2 &lt; 15.\n\\]\n\n# Left-tail: H0: beta2 &gt;= 15; HA: beta2 &lt; 15\nc &lt;- 15\nt_left &lt;- (b2 - c)/seb2\ntcr_left &lt;- qt(alpha, df)\n\nc(t_right=t_right, tcr_right=tcr_right, t_left=t_left, tcr_left=tcr_left)\n\n  t_right tcr_right    t_left  tcr_left \n 2.249904  1.685954 -2.288463 -1.685954 \n\ncat(\"Test statistic:\", round(t_left, 3), \"\\n\")\n\nTest statistic: -2.288 \n\ncat(\"Critical value:\", round(tcr_left, 3), \"\\n\")\n\nCritical value: -1.686 \n\ncat(\"Decision:\", ifelse(t_left &lt; tcr_left, \"Reject H0\", \"Fail to reject H0\"), \"\\n\")\n\nDecision: Reject H0 \n\n\n⚠️ Common Mistake: Students often confuse the direction of the inequality in \\(H_0\\)​ and \\(H_A\\). Remember: the alternative hypothesis represents what you’re trying to find evidence for.\n\n\n\nPART B - The p‑Value Approach\nThe p-value is the probability of observing a test statistic at least as extreme as the one computed, assuming the null hypothesis is true.\nDecision rule: Reject \\(H_0\\)​ if \\(p&lt;αp\\)\nComputing p-Values: \\(F_t\\) stands for the cumulative distribution function (CDF) of the t-distribution.\nRight‑tail: \\(p = 1 - F_t(t)\\)\nLeft‑tail: \\(p = F_t(t)\\)\nTwo‑tail: \\(p = 2 * (1-F_t(|t|)\\)\n\n# Right-tail test (H0: β2 ≤ 5.5)\nc &lt;- 5.5\nt &lt;- (b2-c)/seb2\np_right &lt;- 1-pt(t, df)\n\ncat(\"Right-tail test:\\n\")\n\nRight-tail test:\n\ncat(\"  H0: β2 ≤\", c, \"\\n\")\n\n  H0: β2 ≤ 5.5 \n\ncat(\"  Test statistic:\", round(t, 3), \"\\n\")\n\n  Test statistic: 2.25 \n\ncat(\"  p-value:\", round(p_right, 4), \"\\n\")\n\n  p-value: 0.0152 \n\ncat(\"  Decision at α=0.05:\", ifelse(p_right &lt; 0.05, \"Reject H0\", \"Fail to reject H0\"), \"\\n\\n\")\n\n  Decision at α=0.05: Reject H0 \n\n# Left-tail test (H0: β2 ≥ 15)\nc &lt;- 15\nt &lt;- (b2-c)/seb2\np_left &lt;- pt(t, df)\n\ncat(\"Left-tail test:\\n\")\n\nLeft-tail test:\n\ncat(\"  H0: β2 ≥\", c, \"\\n\")\n\n  H0: β2 ≥ 15 \n\ncat(\"  Test statistic:\", round(t, 3), \"\\n\")\n\n  Test statistic: -2.288 \n\ncat(\"  p-value:\", round(p_left, 4), \"\\n\")\n\n  p-value: 0.0139 \n\ncat(\"  Decision at α=0.05:\", ifelse(p_left &lt; 0.05, \"Reject H0\", \"Fail to reject H0\"), \"\\n\\n\")\n\n  Decision at α=0.05: Reject H0 \n\n# Two-tail test (H0: β2 = 0)\nc &lt;- 0  \nt &lt;- (b2-c)/seb2\np_two &lt;- 2*(1-pt(abs(t), df))\n\nc(p_right=p_right, p_left=p_left, p_two=p_two)\n\n     p_right       p_left        p_two \n1.516329e-02 1.388071e-02 1.945862e-05 \n\ncat(\"Two-tail test:\\n\")\n\nTwo-tail test:\n\ncat(\"  H0: β2 =\", c, \"\\n\")\n\n  H0: β2 = 0 \n\ncat(\"  Test statistic:\", round(t, 3), \"\\n\")\n\n  Test statistic: 4.877 \n\ncat(\"  p-value:\", format.pval(p_two, digits = 4), \"\\n\")\n\n  p-value: 1.946e-05 \n\ncat(\"  Decision at α=0.05:\", ifelse(p_two &lt; 0.05, \"Reject H0\", \"Fail to reject H0\"), \"\\n\")\n\n  Decision at α=0.05: Reject H0 \n\n\n💡 Interpretation Guide:\n\\(p &lt; 0.01\\): Very strong evidence against \\(H_0\\)\n​ \\(0.01 ≤ p &lt; 0.05\\): Strong evidence against \\(H_0\\)\n​ \\(0.05 ≤ p &lt; 0.10\\): Weak evidence against \\(H_0\\)\n​ \\(p ≥ 0.10\\): Insufficient evidence against \\(H_0\\)\n\n\nPART C - Testing Linear Combinations of Parameters\nWe want to test hypotheses about combinations of parameters, not individual coefficients.Example: What is the expected food expenditure for a household earning $2,000/week (income = 20 in our units)?\n\\[\nL = E(\\text{food\\_exp}\\mid \\text{income}=20) = \\beta_1 + 20\\,\\beta_2\n\\]\nVariance identities:\n\\[\n\\operatorname{var}(aX+bY) = a^2\\,\\operatorname{var}(X) + b^2\\,\\operatorname{var}(Y) + 2ab\\,\\operatorname{cov}(X,Y)\n\\]\n\\[\n\\operatorname{var}(b_1+20b_2) = \\operatorname{var}(b_1) + 20^2\\operatorname{var}(b_2) + 2\\cdot20\\operatorname{cov}(b_1,b_2)\n\\]\n⚠️ Critical Point: Many students forget the covariance term! Omitting it leads to incorrect standard errors.\n\n# Significance level and x-value of interest\nalpha &lt;- 0.05\nx &lt;- 20\n\n# the model\nm1 &lt;- lm(food_exp ~ income, data=food)\n\n\n# Extract coefficients and variance-covariance matrix\nb1 &lt;- m1$coef[1]\nb2 &lt;- m1$coef[2]\nvarb1 &lt;- vcov(m1)[1,1]\nvarb2 &lt;- vcov(m1)[2,2]\ncovb1b2 &lt;- vcov(m1)[1,2]\n\n# Compute the linear combination and its variance\nL &lt;- b1 + b2*x\nvarL &lt;- varb1 + x^2 * varb2 + 2*x*covb1b2\nseL &lt;- sqrt(varL)\n\n# Confidence interval for L\ndf &lt;- df.residual(m1)\ntcr &lt;- qt(1-alpha/2, df)\nlowbL &lt;- L - tcr*seL; upbL &lt;- L + tcr*seL\nc(L=L, seL=seL, low=lowbL, up=upbL)\n\n  L.(Intercept)             seL low.(Intercept)  up.(Intercept) \n      287.60886        14.17804       258.90692       316.31081 \n\ncat(\"Expected food expenditure at income = $2000:\\n\")\n\nExpected food expenditure at income = $2000:\n\ncat(\"  Point estimate:\", round(L, 2), \"\\n\")\n\n  Point estimate: 287.61 \n\ncat(\"  Standard error:\", round(seL, 2), \"\\n\")\n\n  Standard error: 14.18 \n\ncat(\"  95% CI: [\", round(lowbL, 2), \",\", round(upbL, 2), \"]\\n\\n\")\n\n  95% CI: [ 258.91 , 316.31 ]\n\n\n\n\nHypothesis Test for Linear Combination\nQuestion: Is expected food expenditure significantly different from $250 at this income level?\n\\[\nH_0: L = 250, \\qquad H_A: L \\ne 250.\n\\]\n\n# Hypothesized value\nc &lt;- 250\n\n# Test statistic\nt &lt;- (L - c)/seL\n\n# p-value (two-tailed)\np_value &lt;- 2*(1-pt(abs(t), df))\nc(t=t, p_value=p_value)\n\n      t.(Intercept) p_value.(Intercept) \n         2.65261316          0.01159078 \n\ncat(\"Hypothesis test: H0: L = 250\\n\")\n\nHypothesis test: H0: L = 250\n\ncat(\"  Test statistic:\", round(t, 3), \"\\n\")\n\n  Test statistic: 2.653 \n\ncat(\"  p-value:\", round(p_value, 4), \"\\n\")\n\n  p-value: 0.0116 \n\ncat(\"  Decision at α=0.05:\", ifelse(p_value &lt; 0.05, \"Reject H0\", \"Fail to reject H0\"), \"\\n\")\n\n  Decision at α=0.05: Reject H0 \n\n\nInterpretation: We fail to reject the null hypothesis. There is insufficient evidence that expected food expenditure differs from $250 for households earning $2,000/week.\n\n\nPART D - MCQs\n\n\n1) In hypothesis testing, H₀ represents:\n\n\n The research hypothesis What we hope to prove The status quo or claim being tested The alternative explanation\n\n\n\n\nHint\n\nThink of it as the “innocent until proven guilty” claim.\n\n\n\n2) The test statistic t = (b - c) / se(b) measures:\n\n\n The absolute error The confidence level How many standard errors b is from c The sample size\n\n\n\n\nHint\n\nIt’s a standardized distance.\n\n\n\n3) For a two-tailed test at α = 0.05 with df = 38, the critical value is approximately:\n\n\n 1.645 1.686 ±2.024 ±1.96\n\n\n\n\nHint\n\nUse qt(0.975, 38) for the upper tail.\n\n\n\n4) A p-value of 0.03 means:\n\n\n H₀ has 3% chance of being true There's a 3% chance the result is wrong If H₀ were true, there's a 3% chance of results this extreme The effect size is 3%\n\n\n\n\n5) We reject H₀ when:\n\n\n p-value &lt; α p-value &gt; α t-statistic &lt; 0 Confidence interval includes zero\n\n\n\n\n6) For a right-tailed test, the p-value is calculated as:\n\n\n pt(t, df) 2 * pt(abs(t), df) 1 - pt(t, df) pt(-t, df)\n\n\n\n\nHint\n\nWe want the upper tail probability.\n\n\n\n7) A 95% confidence interval for β₂ is [3, 8]. In a two-tailed test at α = 0.05, we would:\n\n\n Reject H₀: β₂ = 5 Fail to reject H₀: β₂ = 5 Reject H₀: β₂ = 0 Need more information\n\n\n\n\nHint\n\n5 is inside the interval.\n\n\n\n8) The same confidence interval [3, 8] tells us we would:\n\n\n Fail to reject H₀: β₂ = 0 Reject H₀: β₂ = 0 Accept H₀: β₂ = 10 Reject H₀: β₂ = 6\n\n\n\n\nHint\n\n0 is outside the interval.\n\n\n\n9) To test L = β₁ + 20β₂, we need:\n\n\n Only var(β₁) and var(β₂) Only the covariance Var(β₁), var(β₂), and cov(β₁, β₂) Just the point estimate\n\n\n\n\n10) The variance of L = a·X + b·Y includes the term:\n\n\n ab · var(X) ab · var(Y) 2ab · cov(X,Y) (a+b)² · var(X)\n\n\n\n\nHint\n\nDon’t forget the “2” and the product ab.\n\n\n\n11) vcov(model) returns:\n\n\n Only variances Only covariances A matrix with variances on diagonal and covariances off-diagonal Confidence intervals\n\n\n\n\n12) A Type I error occurs when:\n\n\n We fail to reject a false H₀ We reject a true H₀ Our sample size is too small The p-value is large\n\n\n\n\nHint\n\nIt’s controlled by α.\n\n\n\n13) The significance level α represents:\n\n\n The p-value The probability H₀ is true The probability of Type I error The power of the test\n\n\n\n\n14) For H₀: β₂ = 0, if the 95% CI is [2.5, 7.5], then:\n\n\n p-value &gt; 0.05 p-value &lt; 0.05 We cannot reject H₀ The estimate is unbiased\n\n\n\n\nHint\n\n0 is not in the interval, so we reject.\n\n\n\n15) In R, to get the p-value for a right-tailed test with t = 2.5 and df = 30:\n\n\n pt(2.5, 30) 2*pt(2.5, 30) 1 - pt(2.5, 30) pt(2.5, 30, lower.tail=TRUE)\n\n\n\n\n16) A larger sample size generally:\n\n\n Increases standard errors Decreases standard errors Doesn't affect hypothesis tests Increases the critical value\n\n\n\n\nHint\n\nMore data = more precision.\n\n\n\n17) The degrees of freedom for a simple regression (N=40) is:\n\n\n 40 39 38 37\n\n\n\n\nHint\n\ndf = N - K where K=2 (intercept + slope).\n\n\n\n18) If |t| &lt; t_critical, we:\n\n\n Reject H₀ Fail to reject H₀ Accept H₁ Recalculate the test\n\n\n\n\n19) A one-tailed test is appropriate when:\n\n\n We don't know the direction We want more power Theory predicts a specific direction The sample size is small\n\n\n\n\n20) The t-distribution approaches the normal distribution as:\n\n\n α decreases α increases n decreases n increases\n\n\n\n\nHint\n\nWith large df, t ≈ z.\n\n\n\n\n\n\nSummary\n\nHypothesis testing provides a formal framework for evaluating claims about parameters\nTest statistics measure how far estimates are from hypothesized values in standard error units\np-values quantify the strength of evidence against H₀\nConfidence intervals and hypothesis tests are mathematically equivalent\nLinear combinations require accounting for covariances in variance calculations\n\n\n🏁 End of lab 9 🛑 Remember to save your script 💾\n\n\n\n\nReferences\nAcemoglu, D., Laibson, D., & List, J. A. (2015). Microeconomics (2nd ed.). Pearson. pp. 512–530. (For conceptual understanding of regression interpretation and causal inference basics.)\nColonescu, C. (2022). Principles of Econometrics with R (Version 2.0). Open Textbook Library. Chapters 7–9. (Used directly in labs, matches the PoEdata package and R examples.)\nGujarati, D. N., Porter, D. C., & Gunasekar, S. (2017). Basic Econometrics (5th ed.). McGraw-Hill Education. pp. 126–150, 175–195. (For theory and assumptions behind simple and multiple linear regression.)\nStock, J. H., & Watson, M. W. (2020). Introduction to Econometrics (4th ed.). Pearson. pp. 97–121, 145–165. (For hypothesis testing, p-values, and model diagnostics.)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervision 2</span>"
    ]
  },
  {
    "objectID": "supervision_3.html",
    "href": "supervision_3.html",
    "title": "Supervision 3",
    "section": "",
    "text": "Lab 10 — Data Prep and Best Variable Selection",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Supervision 3</span>"
    ]
  },
  {
    "objectID": "supervision_3.html#lab-10-data-prep-and-best-variable-selection",
    "href": "supervision_3.html#lab-10-data-prep-and-best-variable-selection",
    "title": "Supervision 3",
    "section": "",
    "text": "🎯 Learning outcomes\n\nLoad the Hitters data and inspect dimensions.\nHandle missing values (Salary) to avoid errors in model fitting.\nGet comfortable with the packages we’ll use: ISLR and leaps.\nFit best subset models with regsubsets().\nRetrieve the best model for each size.\nInspect classical metrics: RSS, R², Adjusted R², Cp, BIC.\n\n\n\n\n🧰 Prerequisites\n\nR and RStudio installed.\nPackages: ISLR, leaps.\n\n\n\n\nOverview: Why Variable Selection Matters\nIn regression modeling, we often face a critical question: Which variables should we include?\nThe Problem:\n\nInclude too few variables → underfitting (bias, missing important relationships)\nInclude too many variables → overfitting (poor predictions, unstable estimates)\n\nWhen valuing agricultural land, we might have 20+ potential predictors: soil quality, rainfall, distance to markets, elevation, slope, nearby infrastructure, etc. Which combination best predicts land value without overfitting?\n\n# install.packages(c(\"ISLR\", \"leaps\", \"ggplot2\", \"dplyr\")) if needed\nlibrary(ISLR)\n\nWarning: package 'ISLR' was built under R version 4.4.3\n\nlibrary(leaps)\n\nWarning: package 'leaps' was built under R version 4.4.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Load the baseball players dataset\ndata(Hitters)\n\n# Understanding the Dataset\n# Inspect\nstr(Hitters)\n\n'data.frame':   322 obs. of  20 variables:\n $ AtBat    : int  293 315 479 496 321 594 185 298 323 401 ...\n $ Hits     : int  66 81 130 141 87 169 37 73 81 92 ...\n $ HmRun    : int  1 7 18 20 10 4 1 0 6 17 ...\n $ Runs     : int  30 24 66 65 39 74 23 24 26 49 ...\n $ RBI      : int  29 38 72 78 42 51 8 24 32 66 ...\n $ Walks    : int  14 39 76 37 30 35 21 7 8 65 ...\n $ Years    : int  1 14 3 11 2 11 2 3 2 13 ...\n $ CAtBat   : int  293 3449 1624 5628 396 4408 214 509 341 5206 ...\n $ CHits    : int  66 835 457 1575 101 1133 42 108 86 1332 ...\n $ CHmRun   : int  1 69 63 225 12 19 1 0 6 253 ...\n $ CRuns    : int  30 321 224 828 48 501 30 41 32 784 ...\n $ CRBI     : int  29 414 266 838 46 336 9 37 34 890 ...\n $ CWalks   : int  14 375 263 354 33 194 24 12 8 866 ...\n $ League   : Factor w/ 2 levels \"A\",\"N\": 1 2 1 2 2 1 2 1 2 1 ...\n $ Division : Factor w/ 2 levels \"E\",\"W\": 1 2 2 1 1 2 1 2 2 1 ...\n $ PutOuts  : int  446 632 880 200 805 282 76 121 143 0 ...\n $ Assists  : int  33 43 82 11 40 421 127 283 290 0 ...\n $ Errors   : int  20 10 14 3 4 25 7 9 19 0 ...\n $ Salary   : num  NA 475 480 500 91.5 750 70 100 75 1100 ...\n $ NewLeague: Factor w/ 2 levels \"A\",\"N\": 1 2 1 2 2 1 1 1 2 1 ...\n\ndim(Hitters)          # rows x columns\n\n[1] 322  20\n\nsum(is.na(Hitters$Salary))\n\n[1] 59\n\n# What are we predicting?\ncat(\"Response variable: Salary (1987 annual salary in thousands of dollars)\\n\\n\")\n\nResponse variable: Salary (1987 annual salary in thousands of dollars)\n\n# Predictor variables\ncat(\"Predictor variables:\\n\")\n\nPredictor variables:\n\nnames(Hitters)[-19]   # All except Salary\n\n [1] \"AtBat\"     \"Hits\"      \"HmRun\"     \"Runs\"      \"RBI\"       \"Walks\"    \n [7] \"Years\"     \"CAtBat\"    \"CHits\"     \"CHmRun\"    \"CRuns\"     \"CRBI\"     \n[13] \"CWalks\"    \"League\"    \"Division\"  \"PutOuts\"   \"Assists\"   \"Errors\"   \n[19] \"NewLeague\"\n\n# Handling Missing Values, drop rows with any NA (Salary has some NAs)\nHitters &lt;- na.omit(Hitters)\ndim(Hitters)\n\n[1] 263  20\n\nsum(is.na(Hitters))\n\n[1] 0\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe remove rows with missing Salary because selection routines (like regsubsets) require complete cases. Alternative approaches (not covered here): imputation, missing data models\n\n\n\n\nPART A - Best Subset Selection\nConcept: Fit every possible combination of predictors and choose the best according to some criterion.\nComputational Reality: With p predictors, there are 2^p possible models. For Hitters (19 predictors): 2^19 = 524,288 models! Feasible for p ≤ 20-30, but impractical for larger p.\n\n#library(leaps)\n\n# Exhaustive search over all subsets\nregfit.full &lt;- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)\n\n# Extract summary information\nreg.summary &lt;- summary(regfit.full)\n\n# What does summary() return? What information is available?\nnames(reg.summary)\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n# Quick look at R^2 growth as we add variables\nreg.summary$rsq\n\n [1] 0.3214501 0.4252237 0.4514294 0.4754067 0.4908036 0.5087146 0.5141227\n [8] 0.5285569 0.5346124 0.5404950 0.5426153 0.5436302 0.5444570 0.5452164\n[15] 0.5454692 0.5457656 0.5459518 0.5460945 0.5461159\n\n\n\n\n\n\n\n\nTip\n\n\n\nregsubsets() searches the model space and stores the best model for each size (1..nvmax). Use summary() to see a which matrix and metrics across sizes.\n\n\nUnderstanding the ouptut: The ‘which’ matrix shows selected variables for each model size; TRUE = variable included, FALSE = excluded.\n\nhead(reg.summary$which, 10)\n\n   (Intercept) AtBat  Hits HmRun  Runs   RBI Walks Years CAtBat CHits CHmRun\n1         TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  FALSE FALSE  FALSE\n2         TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  FALSE FALSE  FALSE\n3         TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  FALSE FALSE  FALSE\n4         TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  FALSE FALSE  FALSE\n5         TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  FALSE FALSE  FALSE\n6         TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE  FALSE FALSE  FALSE\n7         TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE   TRUE  TRUE   TRUE\n8         TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE  FALSE FALSE   TRUE\n9         TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE   TRUE FALSE  FALSE\n10        TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE   TRUE FALSE  FALSE\n   CRuns  CRBI CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  FALSE  TRUE  FALSE   FALSE     FALSE   FALSE   FALSE  FALSE      FALSE\n2  FALSE  TRUE  FALSE   FALSE     FALSE   FALSE   FALSE  FALSE      FALSE\n3  FALSE  TRUE  FALSE   FALSE     FALSE    TRUE   FALSE  FALSE      FALSE\n4  FALSE  TRUE  FALSE   FALSE      TRUE    TRUE   FALSE  FALSE      FALSE\n5  FALSE  TRUE  FALSE   FALSE      TRUE    TRUE   FALSE  FALSE      FALSE\n6  FALSE  TRUE  FALSE   FALSE      TRUE    TRUE   FALSE  FALSE      FALSE\n7  FALSE FALSE  FALSE   FALSE      TRUE    TRUE   FALSE  FALSE      FALSE\n8   TRUE FALSE   TRUE   FALSE      TRUE    TRUE   FALSE  FALSE      FALSE\n9   TRUE  TRUE   TRUE   FALSE      TRUE    TRUE   FALSE  FALSE      FALSE\n10  TRUE  TRUE   TRUE   FALSE      TRUE    TRUE    TRUE  FALSE      FALSE\n\n\nFor each model size (1 to 19 variables), regsubsets finds the BEST model according to RSS (equivalently, training R²)\nModel Selection Criteria Explained\n\nRSS (Residual Sum of Squares)\n\n\nMeasures training error\nAlways decreases as we add variables\nProblem: Will always prefer the full model (overfitting)\n\n\nR² (R-squared)\n\n\nProportion of variance explained\nAlways increases with more variables\nProblem: Same as RSS - no penalty for complexity\n\n\nAdjusted R²\n\n\nPenalizes model complexity\nAccounts for degrees of freedom\nUse: Prefer model where this peaks\n\n\nCp (Mallow’s Cp)\n\n\nEstimates test error, penalizes complexity\nSmall Cp indicates good model\nUse: Minimize Cp\n\n\nBIC (Bayesian Information Criterion)\n\n\nSimilar to Cp but with stronger penalty: \\(BIC=nlog⁡(RSS/n)+plog⁡(n)\\)\nTends to select smaller models than Cp\nUse: Minimize BIC\n\n\n\nVisual diagnostics for choosing model size\n\npar(mfrow = c(2,2))\n\n# 1. RSS - always decreases\nplot(reg.summary$rss, xlab = \"Number of Variables\", ylab = \"RSS\", type = \"l\")\ntitle(\"Training RSS (not useful for selection)\")\n\n# Adjusted R^2 (highlight maximum)\nplot(reg.summary$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted R^2\", type = \"l\")\nbest.adjr2 &lt;- which.max(reg.summary$adjr2)\npoints(best.adjr2, reg.summary$adjr2[best.adjr2], col = \"red\", cex = 2, pch = 20)\ntitle(\"Adjusted R² (maximize)\")\n\n# Cp (highlight minimum)\nplot(reg.summary$cp, xlab = \"Number of Variables\", ylab = \"Cp\", type = \"l\")\nbest.cp &lt;- which.min(reg.summary$cp)\npoints(best.cp, reg.summary$cp[best.cp], col = \"red\", cex = 2, pch = 20)\ntitle(\"Mallow's Cp (minimize)\")\n\n# BIC (highlight minimum)\nplot(reg.summary$bic, xlab = \"Number of Variables\", ylab = \"BIC\", type = \"l\")\nbest.bic &lt;- which.min(reg.summary$bic)\npoints(best.bic, reg.summary$bic[best.bic], col = \"red\", cex = 2, pch = 20)\ntitle(\"BIC (minimize)\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n# Interpretation\ncat(\"Optimal model sizes according to different criteria:\\n\")\n\nOptimal model sizes according to different criteria:\n\ncat(\"  Adjusted R²:\", best.adjr2, \"variables\\n\")\n\n  Adjusted R²: 11 variables\n\ncat(\"  Cp:        \", best.cp, \"variables\\n\")\n\n  Cp:         10 variables\n\ncat(\"  BIC:       \", best.bic, \"variables\\n\\n\")\n\n  BIC:        6 variables\n\ncat(\"Note: BIC typically selects the most parsimonious (smallest) model\\n\")\n\nNote: BIC typically selects the most parsimonious (smallest) model\n\n\n\n\n\n\n\n\nNote\n\n\n\nRule of thumb: prefer the model size where Adjusted R² peaks, or Cp/BIC are minimized. These are proxies for test error.\n\n\n\nInspect variables in the chosen model(s)\n\n# Example: variables in the BIC-optimal model\ncoef(regfit.full, best.bic)\n\n (Intercept)        AtBat         Hits        Walks         CRBI    DivisionW \n  91.5117981   -1.8685892    7.6043976    3.6976468    0.6430169 -122.9515338 \n     PutOuts \n   0.2643076 \n\n\n\n\n\n\n\n\nTip\n\n\n\nReading the leaps Plots\nBlack squares = variable included in model Top row = model with best (minimum/maximum) criterion value Each row = one model size Look for consistent variables that appear across different model sizes\n\n\n\n# regsubsets has its own plot helper:\nplot(regfit.full, scale = \"bic\")\n\n\n\n\n\n\n\nplot(regfit.full, scale = \"adjr2\")\n\n\n\n\n\n\n\nplot(regfit.full, scale = \"Cp\")\n\n\n\n\n\n\n\nplot(regfit.full, scale = \"r2\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nCommon Mistake Don’t use training R² to select model size! It will always prefer the full model. Use Adjusted R², Cp, or BIC instead.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Supervision 3</span>"
    ]
  },
  {
    "objectID": "supervision_3.html#lab-11-forward-stepwise-selection",
    "href": "supervision_3.html#lab-11-forward-stepwise-selection",
    "title": "Supervision 3",
    "section": "Lab 11 — Forward Stepwise Selection",
    "text": "Lab 11 — Forward Stepwise Selection\n\n🎯 Learning outcomes\nBy the end of this lab you will be able to:\n\nUnderstand the greedy algorithm behind forward selection\nImplement forward stepwise using method = “forward”\nCompare results with best subset selection\nRecognize when forward stepwise is preferable (computational constraints)\n\n\n\n\nThe Forward Stepwise Algorithm\nStep-by-step process:\n\nStart with null model (intercept only)\nFor each of p predictors, fit a model adding that predictor\nAdd the predictor that most improves the model (lowest RSS)\nRepeat: given k variables, try adding each of the (p-k) remaining\nContinue until all p predictors are included\n\nKey Properties:\n\nEvaluates only 1 + p(p+1)/2 models (vs. 2^p for best subset)\nFor p=19: 191 models (vs. 524,288!)\nGreedy: Once a variable enters, it stays\nMay not find the globally optimal model\n\n\n# Forward stepwise selection\nregfit.fwd &lt;- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = \"forward\")\n\nsummary(regfit.fwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"forward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: forward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\n# Compare a specific size (e.g., 7 variables) across methods\ncoef(regfit.full, 7)  # best subset\n\n (Intercept)         Hits        Walks       CAtBat        CHits       CHmRun \n  79.4509472    1.2833513    3.2274264   -0.3752350    1.4957073    1.4420538 \n   DivisionW      PutOuts \n-129.9866432    0.2366813 \n\ncoef(regfit.fwd, 7)   # forward stepwise\n\n (Intercept)        AtBat         Hits        Walks         CRBI       CWalks \n 109.7873062   -1.9588851    7.4498772    4.9131401    0.8537622   -0.3053070 \n   DivisionW      PutOuts \n-127.1223928    0.2533404 \n\n\n\nAre the 7-variable-models identical?\nWhich variables differ?\n\n\n\n\n\n\n\nWarning\n\n\n\nForward stepwise is greedy: after a variable enters, it stays. It evaluates far fewer models than best subset—great for speed—but it may miss the global optimum.\n\n\n\nWhen to Use Forward Stepwise\nAdvantages:\n\nMuch faster than best subset (critical for p &gt; 20)\nOften finds nearly optimal solutions\nRequired when n &lt; p (best subset impossible)\n\nDisadvantages:\n\nNot guaranteed to find global optimum\nOrder of variable entry matters\nCannot remove variables once added",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Supervision 3</span>"
    ]
  },
  {
    "objectID": "supervision_3.html#lab-12-backward-stepwise-selection",
    "href": "supervision_3.html#lab-12-backward-stepwise-selection",
    "title": "Supervision 3",
    "section": "Lab 12 — Backward Stepwise Selection",
    "text": "Lab 12 — Backward Stepwise Selection\n\n🎯 Learning outcomes\nBy the end of this lab you will be able to:\n\nUnderstand the backward elimination algorithm\nImplement backward stepwise using method = “backward”\nRecognize the n &gt; p requirement\nCompare all three methods systematically\n\n\n\n\nThe Backward Stepwise Selection\nStep-by-step process:\nStart with full model (all p predictors) Remove the predictor with smallest contribution (largest p-value) Fit all (p-1) models with one variable removed Choose model with lowest RSS (or highest R²) Repeat until intercept-only model\nKey Properties:\nAlso evaluates 1 + p(p+1)/2 models Requires n &gt; p (must fit full model first) Greedy: Once removed, variable cannot re-enter May differ from forward stepwise\n\n# Backward stepwise selection\nregfit.bwd &lt;- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = \"backward\")\nsummary(regfit.bwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"backward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: backward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n4  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\n# Compare at size 7 again\ncoef(regfit.bwd, 7)   # backward stepwise\n\n (Intercept)        AtBat         Hits        Walks        CRuns       CWalks \n 105.6487488   -1.9762838    6.7574914    6.0558691    1.1293095   -0.7163346 \n   DivisionW      PutOuts \n-116.1692169    0.3028847 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nBackward stepwise starts from the full model and removes variables. It requires n &gt; p so that the full least squares fit exists.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Supervision 3</span>"
    ]
  },
  {
    "objectID": "supervision_3.html#lab-13-validation-set-approach-reading-the-plots-coefficients",
    "href": "supervision_3.html#lab-13-validation-set-approach-reading-the-plots-coefficients",
    "title": "Supervision 3",
    "section": "Lab 13 — Validation Set Approach, Reading the Plots & Coefficients",
    "text": "Lab 13 — Validation Set Approach, Reading the Plots & Coefficients\n\n🎯 Learning outcomes\nBy the end of this lab you will be able to:\n\nSplit data into training and test sets\nFit models on training data only\nEvaluate test set performance (true prediction error)\nUnderstand why training error is optimistic\nSelect model size based on test error.\n\n\n# Choose a final size by any one of the criteria\nk_final &lt;- best.bic     # example: BIC-minimizer\n\n# Coefficients and variables\nfinal_coef &lt;- coef(regfit.full, k_final)\nfinal_coef\n\n (Intercept)        AtBat         Hits        Walks         CRBI    DivisionW \n  91.5117981   -1.8685892    7.6043976    3.6976468    0.6430169 -122.9515338 \n     PutOuts \n   0.2643076 \n\n# A compact report\ndata.frame(term = names(final_coef), estimate = as.numeric(final_coef))\n\n         term     estimate\n1 (Intercept)   91.5117981\n2       AtBat   -1.8685892\n3        Hits    7.6043976\n4       Walks    3.6976468\n5        CRBI    0.6430169\n6   DivisionW -122.9515338\n7     PutOuts    0.2643076\n\n\n\n\n\n\n\n\nTip\n\n\n\nReporting tip: Always state (i) the criterion used (e.g., BIC), (ii) the model size, and (iii) the selected variables with their coefficients. Avoid training R² alone—prefer Cp/BIC/Adjusted R².\n\n\n\n\n\n✅ What you should now be able to do\n\nPrepare data and run best subset and stepwise selection.\nUse Adjusted R², Cp, and BIC to choose model size.\nExtract and communicate the chosen variables and coefficients.\n\n\nNext session we’ll compare these selections using a validation set and cross‑validation, and then move to ridge and lasso.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Supervision 3</span>"
    ]
  },
  {
    "objectID": "supervision_4.html",
    "href": "supervision_4.html",
    "title": "Supervision 4",
    "section": "",
    "text": "Lab 14 — Collinearity",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervision 4</span>"
    ]
  },
  {
    "objectID": "supervision_4.html#lab-14-collinearity",
    "href": "supervision_4.html#lab-14-collinearity",
    "title": "Supervision 4",
    "section": "",
    "text": "🎯 Learning outcomes\n\nUnderstand what collinearity (multicollinearity) is and why it matters for regression inference.\nRecognise how collinearity inflates standard errors, weakening t‑tests for individual coefficients.\nDiagnose collinearity using the Variance Inflation Factor (VIF) and interpret typical thresholds.\nImplement VIF diagnostics in R with car::vif() and report results clearly (tables, captions).\nConsider practical remedies (drop variables, transform/aggregate predictors, or rethink specification).\n\n\n\n\n🧰 Prerequisites\n\nMultiple regression basics (OLS, \\(R^2\\), F‑test, t‑tests).\nR and RStudio installed.\nPackages: PoEdata, broom, knitr, car.\n\n\n\n\nOverview: why collinearity matters\nCollinearity does not bias OLS, but it inflates the variance of estimated coefficients. As a result, t‑tests lose power and coefficient signs may look unstable across specifications—even when the model has a high \\(R^2\\). Diagnostics such as VIF help decide whether variables overlap so much that individual effects are hard to interpret.\n\nCollinearity (multicollinearity) among regressors occurs when two or more move closely together or have limited variability. It inflates the variance of estimated parameters (less precise t-tests), even though OLS estimates remain unbiased. As a result, a model may have a high \\(R^2\\) or a large F‑statistic but insignificant individual coefficients.\n\n\n🧰 Packages\nPoEdata, broom, knitr, car\n\n# install.packages(c(\"PoEdata\",\"broom\",\"knitr\",\"car\"))  # if needed\nlibrary(PoEdata); library(broom); library(knitr); library(car)\n\nWarning: package 'broom' was built under R version 4.4.3\n\n\nWarning: package 'knitr' was built under R version 4.4.3\n\n\nLoading required package: carData\n\n# Example data: cars (mpg = miles per gallon; cyl = cylinders; eng = engine displacement; wgt = weight)\ndata(\"cars\", package = \"PoEdata\")\n\n# Simple model\nmod1 &lt;- lm(mpg ~ cyl, data = cars)\nkable(tidy(mod1), caption = \"A simple linear 'mpg' model\")\n\n\nA simple linear ‘mpg’ model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n42.915505\n0.8348668\n51.40401\n0\n\n\ncyl\n-3.558078\n0.1456755\n-24.42468\n0\n\n\n\n\n\nNow add more regressors:\n\nmod2 &lt;- lm(mpg ~ cyl + eng + wgt, data = cars)\nkable(tidy(mod2), caption = \"Multivariate 'mpg' model\")\n\n\nMultivariate ‘mpg’ model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n44.3709616\n1.4806851\n29.9665086\n0.0000000\n\n\ncyl\n-0.2677968\n0.4130673\n-0.6483126\n0.5171663\n\n\neng\n-0.0126740\n0.0082501\n-1.5362247\n0.1252983\n\n\nwgt\n-0.0057079\n0.0007139\n-7.9951428\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen adding eng and wgt, the coefficient for cyl can turn insignificant—a classic sign of collinearity because these vehicle traits tend to move together.\n\n\n\n\nVariance Inflation Factor (VIF)\nThe VIF for regressor \\(x_k\\) is \\(VIF_k = \\dfrac{1}{1-R_k^2}\\), where \\(R_k^2\\) is from regressing \\(x_k\\) on the other regressors.\n\nv &lt;- car::vif(mod2)\ntab &lt;- data.frame(regressor = names(v), VIF = as.numeric(v), row.names = NULL)\nkable(tab, caption = \"Variance inflation factors for the 'mpg' regression model.\")\n\n\nVariance inflation factors for the ‘mpg’ regression model.\n\n\nregressor\nVIF\n\n\n\n\ncyl\n10.515508\n\n\neng\n15.786455\n\n\nwgt\n7.788716\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nRules of thumb\n- VIF &gt; 10 → strong collinearity (commonly used cutoff)\n- VIF &gt; 5 → moderate collinearity (early warning)\n\n\n\n\nMitigations (when VIFs are high)\n\nRemove or combine collinear variables (e.g., combine cyl and eng into a power index).\n\nUse PCA to produce orthogonal components.\n\nStandardise/centre variables (helps especially with interactions/polynomials).\n\nUse Ridge regression if retaining all variables is preferred.\n\nConsider transformations (e.g., logs).\n\n\n\n\nSummary\n\nCollinearity leaves OLS unbiased but makes estimates imprecise.\nAlways check VIFs when several predictors are conceptually related.\nIf VIFs are high, consider dropping, combining, or re‑specifying predictors—and justify choices with theory.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervision 4</span>"
    ]
  },
  {
    "objectID": "supervision_4.html#lab-15-heteroskedasticity",
    "href": "supervision_4.html#lab-15-heteroskedasticity",
    "title": "Supervision 4",
    "section": "Lab 15 — Heteroskedasticity",
    "text": "Lab 15 — Heteroskedasticity\n\n🎯 Learning outcomes\n\nUnderstand the key concept(s) introduced in this lab and their purpose in empirical analysis.\nCorrectly implement the core R workflow for this topic and present clean, well-captioned results.\nInterpret model output in context and link diagnostics to modeling choices.\nRecognise common pitfalls and apply simple robustness checks.\n\n\n\n\n🧰 Prerequisites\n\nPrior exposure to multiple regression and basic inference (t-tests, F-tests).\nR and RStudio installed.\nPackages: PoEdata, broom, knitr, plus any additional packages used in this lab.\n\n\n\n\nOverview\nThis lab introduces the core idea, why it matters for inference and decision-making, and where it fits in the broader modeling workflow. You will practise the implementation in R, interpret outputs carefully, and connect diagnostics back to modeling decisions.\n\nThe Gauss–Markov theorem assumes homoskedasticity (constant error variance \\(\\sigma^2\\)). In many economic datasets, variance grows with some regressors (e.g., higher‑income households show more dispersion in expenditure). With heteroskedasticity, OLS coefficients remain unbiased, but standard errors and therefore inference are wrong unless corrected.\n\n\n🧰 Packages\nlmtest, broom, PoEdata, car, sandwich, knitr\n\n# install.packages(c(\"lmtest\",\"broom\",\"PoEdata\",\"car\",\"sandwich\",\"knitr\"))  # if needed\nlibrary(lmtest); library(broom); library(PoEdata); library(car); library(sandwich); library(knitr)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\ndata(\"food\", package=\"PoEdata\")\nmod1 &lt;- lm(food_exp ~ income, data = food)\n\n# Scatter and fitted line\nplot(food$income, food$food_exp, xlab=\"income\", ylab=\"food expenditure\", pch=19, col=\"grey\")\nabline(mod1, lwd=2)\n\n\n\n\n\n\n\n\n\n\nSpotting heteroskedasticity with residual plots\n\nres &lt;- residuals(mod1); yhat &lt;- fitted(mod1)\nplot(food$income, res, xlab=\"income\", ylab=\"residuals\", pch=19, col=\"grey\")\nabline(h=0, lty=2)\n\n\n\n\n\n\n\nplot(yhat, res, xlab=\"fitted values\", ylab=\"residuals\", pch=19, col=\"grey\")\nabline(h=0, lty=2)\n\n\n\n\n\n\n\n\n\n\nBreusch–Pagan test (manual construction)\n\nalpha &lt;- 0.05\nressq &lt;- resid(mod1)^2\nmodres &lt;- lm(ressq ~ income, data = food)       # auxiliary regression\nN &lt;- nobs(modres)\nS &lt;- summary(modres)$df[2]                       # df for regression (k)\nchisqcr &lt;- qchisq(1 - alpha, S)                  # critical value (right-tail)\nRsqres &lt;- summary(modres)$r.squared\nchisq &lt;- N * Rsqres\npval &lt;- 1 - pchisq(chisq, S)\nc(statistic = chisq, crit = chisqcr, p.value = pval)\n\nstatistic      crit   p.value \n 7.384424 53.383541  1.000000 \n\n\n\n\nWhite (quadratic) version\n\nmodres2 &lt;- lm(ressq ~ income + I(income^2), data = food)\nRsq2 &lt;- summary(modres2)$r.squared\nS2 &lt;- summary(modres2)$df[2]\nchisq2 &lt;- N * Rsq2\npval2 &lt;- 1 - pchisq(chisq2, S2)\nc(statistic = chisq2, df = S2, p.value = pval2)\n\nstatistic        df   p.value \n 7.555079 37.000000  1.000000 \n\n\n\n\nBreusch–Pagan with bptest()\n\nkable(tidy(bptest(mod1)), caption = \"Breusch–Pagan heteroskedasticity test\")\n\n\nBreusch–Pagan heteroskedasticity test\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n\n\n7.384424\n0.0065791\n1\nstudentized Breusch-Pagan test\n\n\n\n\n\n\n\nGoldfeld–Quandt test (indicator split: metro vs rural)\n\nalpha &lt;- 0.05\ndata(\"cps2\", package=\"PoEdata\")\nm &lt;- cps2[cps2$metro==1, ]\nr &lt;- cps2[cps2$metro==0, ]\n\nwg1 &lt;- lm(wage ~ educ + exper, data = m)\nwg0 &lt;- lm(wage ~ educ + exper, data = r)\n\ndf1 &lt;- wg1$df.residual; df0 &lt;- wg0$df.residual\nsig1 &lt;- summary(wg1)$sigma^2; sig0 &lt;- summary(wg0)$sigma^2\nfstat &lt;- sig1/sig0\nFlc &lt;- qf(alpha/2, df1, df0); Fuc &lt;- qf(1-alpha/2, df1, df0)\nc(F = fstat, Flc = Flc, Fuc = Fuc)\n\n        F       Flc       Fuc \n2.0877623 0.8051984 1.2617297 \n\n\nGoldfeld–Quandt without an indicator (split by median income):\n\nalpha &lt;- 0.05\nmedianincome &lt;- median(food$income)\nli &lt;- food[food$income &lt;= medianincome, ]\nhi &lt;- food[food$income &gt;= medianincome, ]\n\neqli &lt;- lm(food_exp ~ income, data = li)\neqhi &lt;- lm(food_exp ~ income, data = hi)\n\ndfli &lt;- eqli$df.residual; dfhi &lt;- eqhi$df.residual\nsqli &lt;- summary(eqli)$sigma^2; sqhi &lt;- summary(eqhi)$sigma^2\n\nfstat &lt;- sqhi/sqli\nFc &lt;- qf(1 - alpha, dfhi, dfli)\npval &lt;- 1 - pf(fstat, dfhi, dfli)\nc(F = fstat, Fcrit = Fc, p.value = pval)\n\n         F      Fcrit    p.value \n3.61475572 2.21719713 0.00459643 \n\n\nOr use gqtest() directly:\n\nfoodeq &lt;- lm(food_exp ~ income, data = food)\ntst &lt;- lmtest::gqtest(foodeq, point = 0.5, alternative = \"greater\", order.by = food$income)\nkable(tidy(tst), caption = \"R function `gqtest()` with the 'food' equation\")\n\nMultiple parameters; naming those columns df1 and df2.\n\n\n\nR function gqtest() with the ‘food’ equation\n\n\n\n\n\n\n\n\n\n\ndf1\ndf2\nstatistic\np.value\nmethod\nalternative\n\n\n\n\n18\n18\n3.614756\n0.0045964\nGoldfeld-Quandt test\nvariance increases from segment 1 to 2\n\n\n\n\n\n\n\nHeteroskedasticity‑consistent (HC) standard errors\n\nfoodeq &lt;- lm(food_exp ~ income, data = food)\nkable(tidy(foodeq), caption = \"Regular standard errors in the 'food' equation\")\n\n\nRegular standard errors in the ‘food’ equation\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n83.41600\n43.410163\n1.921578\n0.0621824\n\n\nincome\n10.20964\n2.093263\n4.877381\n0.0000195\n\n\n\n\ncov1 &lt;- car::hccm(foodeq, type = \"hc1\")\nfood.HC1 &lt;- lmtest::coeftest(foodeq, vcov. = cov1)\nkable(tidy(food.HC1), caption = \"Robust (HC1) standard errors in the 'food' equation\")\n\n\nRobust (HC1) standard errors in the ‘food’ equation\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n83.41600\n27.463748\n3.037313\n0.0042989\n\n\nincome\n10.20964\n1.809077\n5.643566\n0.0000018\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nRobust (HC) errors fix inference under heteroskedasticity, but the point estimates are unchanged.\n\n\n\n\n\nSummary\n\nYou applied the technique in R, produced tidy outputs, and interpreted them in context.\nKeep focusing on clean tables, explicit assumptions, and diagnostics that justify your specification.\nUse the provided patterns as a template for future empirical work.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervision 4</span>"
    ]
  },
  {
    "objectID": "supervision_4.html#lab-16-serial-correlation-autocorrelation",
    "href": "supervision_4.html#lab-16-serial-correlation-autocorrelation",
    "title": "Supervision 4",
    "section": "Lab 16 — Serial Correlation (Autocorrelation)",
    "text": "Lab 16 — Serial Correlation (Autocorrelation)\n\n🎯 Learning outcomes\n\nUnderstand the core ideas and assumptions for this lab’s topic.\nImplement the workflow in R and create tidy outputs with clear captions.\nInterpret results in context and connect diagnostics to modeling decisions.\nRecognise common pitfalls and perform basic robustness checks.\n\n\n\n\n🧰 Prerequisites\n\nMultiple regression and inference (t-tests, F-tests).\nR and RStudio installed.\nPackages: broom, knitr, and any other packages used in the lab code.\n\n\n\n\nOverview\nThis lab situates the technique within the broader econometric workflow, explaining why it matters for inference and decision‑making. You will implement it in R, assess assumptions/diagnostics, and justify specification choices.\n\nSerial correlation is correlation across time in a series. In regression, autocorrelated errors lead to incorrect standard errors (and misleading (t)-tests).\n\n\nExample: growth vs unemployment (Okun’s data)\n\ndata(\"okun\", package=\"PoEdata\")\nokun.ts &lt;- ts(okun, start=c(1948,1), frequency=4)\n\nplot(okun.ts[,\"g\"], ylab=\"growth\")\n\n\n\n\n\n\n\nplot(okun.ts[,\"u\"], ylab=\"unemployment\")\n\n\n\n\n\n\n\n# Scatter with lags\nggL1 &lt;- data.frame(g = okun.ts[,\"g\"], gL1 = stats::lag(okun.ts[,\"g\"], -1))\nplot(ggL1); abline(h=mean(ggL1$gL1, na.rm=TRUE), v=mean(ggL1$g, na.rm=TRUE), lty=2)\n\n\n\n\n\n\n\nggL2 &lt;- data.frame(g = okun.ts[,\"g\"], gL2 = stats::lag(okun.ts[,\"g\"], -2))\nplot(ggL2); abline(h=mean(ggL2$gL2, na.rm=TRUE), v=mean(ggL2$g, na.rm=TRUE), lty=2)\n\n\n\n\n\n\n\n# Correlogram\nacf(okun.ts[,\"g\"])\n\n\n\n\n\n\n\n\n\n\nPhillips curve example and BG/DW tests\n\nlibrary(dynlm)\n\nWarning: package 'dynlm' was built under R version 4.4.3\n\ndata(\"phillips_aus\", package=\"PoEdata\")\nphill.ts &lt;- ts(phillips_aus, start=c(1987,1), end=c(2009,3), frequency=4)\ninflation &lt;- phill.ts[,\"inf\"]\nDu &lt;- diff(phill.ts[,\"u\"])\n\n# FDL model: inf_t = beta1 + beta2 * diff(u_t) + e_t\nphill.dyn &lt;- dynlm(inf ~ diff(u), data = phill.ts)\nkable(tidy(phill.dyn), caption=\"Summary of the `phillips` model\")\n\nWarning: The `tidy()` method for objects of class `dynlm` is not maintained by the broom team, and is only supported through the `lm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n\n\n\nSummary of the phillips model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.7776213\n0.0658249\n11.813474\n0.0000000\n\n\ndiff(u)\n-0.5278638\n0.2294049\n-2.301014\n0.0237539\n\n\n\n\n# Residual plot and correlogram\nehat &lt;- resid(phill.dyn)\nplot(ehat); abline(h=0, lty=2)\n\n\n\n\n\n\n\nacf(ehat)\n\n\n\n\n\n\n\n\nBreusch–Godfrey tests (different orders/statistics):\n\n# NOTE: preserving your object names a, b, c, d and test types.\n# (We only use base::c() below to avoid masking by object 'c')\na &lt;- lmtest::bgtest(phill.dyn, order=1, type=\"F\",    fill=0)\nb &lt;- lmtest::bgtest(phill.dyn, order=1, type=\"F\",    fill=NA)\nc &lt;- lmtest::bgtest(phill.dyn, order=4, type=\"Chisq\",fill=0)\nd &lt;- lmtest::bgtest(phill.dyn, order=4, type=\"Chisq\",fill=NA)\n\ndfr &lt;- data.frame(\n  Method    = base::c(\"1, F, 0\",\"1, F, NA\",\"4, Chisq, 0\",\"4, Chisq, NA\"),\n  Statistic = base::c(unname(a$statistic), unname(b$statistic), unname(c$statistic), unname(d$statistic)),\n  Parameters= base::c(sprintf(\"df1=%g, df2=%g\", a$parameter[1], a$parameter[2]),\n                      sprintf(\"df1=%g, df2=%g\", b$parameter[1], b$parameter[2]),\n                      sprintf(\"df=%g\",          c$parameter),\n                      sprintf(\"df=%g\",          d$parameter)),\n  p_value   = base::c(a$p.value, b$p.value, c$p.value, d$p.value)\n)\nknitr::kable(dfr, caption = \"Breusch–Godfrey test for the Phillips example.\")\n\n\nBreusch–Godfrey test for the Phillips example.\n\n\nMethod\nStatistic\nParameters\np_value\n\n\n\n\n1, F, 0\n38.46538\ndf1=1, df2=87\n0e+00\n\n\n1, F, NA\n38.69456\ndf1=1, df2=86\n0e+00\n\n\n4, Chisq, 0\n36.67190\ndf=4\n2e-07\n\n\n4, Chisq, NA\n33.59372\ndf=4\n9e-07\n\n\n\n\n\nDurbin–Watson (legacy but useful for small samples):\n\nlmtest::dwtest(phill.dyn)\n\n\n    Durbin-Watson test\n\ndata:  phill.dyn\nDW = 0.88729, p-value = 2.198e-09\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\n\n\nSummary\n\nYou implemented the method in R and produced clean, well‑captioned outputs.\nDiagnostics and assumptions inform how you specify and interpret the model.\nUse these patterns as a template for future empirical work.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervision 4</span>"
    ]
  },
  {
    "objectID": "supervision_4.html#lab-17-hac-neweywest-standard-errors",
    "href": "supervision_4.html#lab-17-hac-neweywest-standard-errors",
    "title": "Supervision 4",
    "section": "Lab 17 — HAC (Newey–West) Standard Errors",
    "text": "Lab 17 — HAC (Newey–West) Standard Errors\n\n🎯 Learning outcomes\n\nGrasp the core concept(s) of this lab and where they fit in the modelling workflow.\nImplement the R workflow cleanly, producing tidy outputs with clear captions.\nInterpret results carefully and connect diagnostics/assumptions to modelling choices.\nIdentify pitfalls and perform simple robustness checks.\n\n\n\n\n🧰 Prerequisites\n\nMultiple regression, inference (t-tests, F-tests), and model diagnostics basics.\nR and RStudio installed.\nPackages: broom, knitr, plus any additional packages used in this lab.\n\n\n\n\nOverview\nThis lab introduces the concept, explains why it matters for inference and decision‑making, and demonstrates implementation in R. You will interpret results in context and motivate specification choices using diagnostics.\n\nCorrect standard errors under autocorrelation (and heteroskedasticity) using HAC estimators.\n\nlibrary(sandwich); library(lmtest)\n\ns0 &lt;- coeftest(phill.dyn)                              # incorrect under AC\ns1 &lt;- coeftest(phill.dyn, vcov.=vcovHAC(phill.dyn))    # HAC\ns2 &lt;- coeftest(phill.dyn, vcov.=NeweyWest(phill.dyn))  # Newey–West\ns3 &lt;- coeftest(phill.dyn, vcov.=kernHAC(phill.dyn))    # kernel HAC\n\ntbl &lt;- data.frame(cbind(s0[c(3,4)], s1[c(3,4)], s2[c(3,4)], s3[c(3,4)]))\nnames(tbl) &lt;- c(\"Incorrect\",\"vcovHAC\",\"NeweyWest\",\"kernHAC\")\nrow.names(tbl) &lt;- c(\"(Intercept)\",\"Du\")\nkable(tbl, digits=3, caption = \"Comparing standard errors for the Phillips model.\")\n\n\nComparing standard errors for the Phillips model.\n\n\n\nIncorrect\nvcovHAC\nNeweyWest\nkernHAC\n\n\n\n\n(Intercept)\n0.066\n0.095\n0.128\n0.131\n\n\nDu\n0.229\n0.304\n0.331\n0.335\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nHAC fixes inference but not efficiency: OLS with HAC is still not minimum‑variance when errors are autocorrelated.\n\n\n\n\n\nSummary\n\nYou implemented the method in R, produced clean outputs, and interpreted them in context.\nDiagnostics/assumptions inform specification choices and robustness checks.\nReuse this structure as a template for future empirical work.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervision 4</span>"
    ]
  },
  {
    "objectID": "supervision_5.html",
    "href": "supervision_5.html",
    "title": "Supervision 5 — Hedonic House Prices (Ames, Iowa — real data)",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervision 5 — Hedonic House Prices (Ames, Iowa — real data)</span>"
    ]
  },
  {
    "objectID": "supervision_5.html#learning-goals",
    "href": "supervision_5.html#learning-goals",
    "title": "Supervision 5 — Hedonic House Prices (Ames, Iowa — real data)",
    "section": "",
    "text": "Specify and estimate a hedonic pricing model for house prices using a real dataset.\nJustify variable selection and model-building choices.\nEvaluate model fit, assumptions, and interpretability.\nCommunicate results clearly to a non-technical audience.\n\n\nNote on data: We use the Ames Housing dataset (De Cock, 2011), provided by the R package AmesHousing. It contains individual residential sales with rich structural attributes. The context is U.S.-based (Ames, Iowa), but the hedonic logic is the same.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervision 5 — Hedonic House Prices (Ames, Iowa — real data)</span>"
    ]
  },
  {
    "objectID": "supervision_5.html#step-0-build-the-student-dataset-run-once",
    "href": "supervision_5.html#step-0-build-the-student-dataset-run-once",
    "title": "Supervision 5 — Hedonic House Prices (Ames, Iowa — real data)",
    "section": "Step 0 — Build the student dataset (run once)",
    "text": "Step 0 — Build the student dataset (run once)\n\nThis code loads the dataset from the R package and writes a cleaned CSV: hedonic_ames.csv. Staff can run this once and share the CSV with students, or students can run it themselves.\n\n\n\nCode\n# Packages\nif (!requireNamespace(\"AmesHousing\", quietly = TRUE)) {\n  install.packages(\"AmesHousing\")\n}\nlibrary(AmesHousing)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(janitor)\nlibrary(stringr)\n\n# Load full Ames data\names &lt;- AmesHousing::make_ames()\n\n# Minimal cleaning + variable selection for hedonic teaching ---------\n# Keep interpretable structural and locational proxies\ndf &lt;- ames %&gt;%\n  janitor::clean_names() %&gt;%\n  dplyr::transmute(\n    # target\n    sale_price,\n    # structural size & rooms\n    gr_liv_area, total_bsmt_sf, lot_area,\n    bedroom_abv_gr, full_bath, half_bath, kitchen_abv_gr,\n    # quality/condition\n    overall_qual, overall_cond, year_built, year_remod_add,\n    exter_qual, kitchen_qual, central_air,\n    # garage & fireplaces\n    garage_cars, garage_area, fireplace_qu,\n    # building & neighborhood\n    bldg_type, house_style, neighborhood, overall = overall_qual,\n    # simple conveniences\n    ms_zoning, foundation\n  ) %&gt;%\n  # Convert a few to factors\n  dplyr::mutate(\n    central_air = factor(central_air, levels = c(\"N\",\"Y\")),\n    exter_qual = factor(exter_qual, ordered = TRUE),\n    kitchen_qual = factor(kitchen_qual, ordered = TRUE),\n    fireplace_qu = factor(fireplace_qu, ordered = TRUE),\n    bldg_type = factor(bldg_type),\n    house_style = factor(house_style),\n    neighborhood = factor(neighborhood),\n    ms_zoning = factor(ms_zoning),\n    foundation = factor(foundation),\n    # engineered features\n    baths_total = full_bath + 0.5 * half_bath,\n    age = pmax(0, 2010 - year_built),           # 2010 is the end of the series\n    remod_age = pmax(0, 2010 - year_remod_add)\n  ) %&gt;%\n  # Basic sanity filters (trim extreme outliers)\n  dplyr::filter(sale_price &gt; 20000, sale_price &lt; 600000, gr_liv_area &gt; 300, lot_area &lt; 100000) %&gt;%\n  tidyr::drop_na(sale_price, gr_liv_area, bedroom_abv_gr, baths_total, overall_qual)\n\n# Shuffle and add an id\nset.seed(42)\ndf &lt;- df %&gt;% dplyr::mutate(listing_id = dplyr::row_number()) %&gt;% dplyr::relocate(listing_id)\n\n# Save CSV for students\nreadr::write_csv(df, \"hedonic_ames.csv\")\n\n# Quick peek\ndplyr::glimpse(df)\n\n\nRows: 2,918\nColumns: 28\n$ listing_id     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n$ sale_price     &lt;int&gt; 215000, 105000, 172000, 244000, 189900, 195500, 213500,…\n$ gr_liv_area    &lt;int&gt; 1656, 896, 1329, 2110, 1629, 1604, 1338, 1280, 1616, 18…\n$ total_bsmt_sf  &lt;dbl&gt; 1080, 882, 1329, 2110, 928, 926, 1338, 1280, 1595, 994,…\n$ lot_area       &lt;int&gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005, 53…\n$ bedroom_abv_gr &lt;int&gt; 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 1, 4, 4, 1, 2…\n$ full_bath      &lt;int&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, 1, 1…\n$ half_bath      &lt;int&gt; 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0…\n$ kitchen_abv_gr &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ overall_qual   &lt;fct&gt; Above_Average, Average, Above_Average, Good, Average, A…\n$ overall_cond   &lt;fct&gt; Average, Above_Average, Above_Average, Average, Average…\n$ year_built     &lt;int&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 1995, 1…\n$ year_remod_add &lt;int&gt; 1960, 1961, 1958, 1968, 1998, 1998, 2001, 1992, 1996, 1…\n$ exter_qual     &lt;ord&gt; Typical, Typical, Typical, Good, Typical, Typical, Good…\n$ kitchen_qual   &lt;ord&gt; Typical, Typical, Good, Excellent, Typical, Good, Good,…\n$ central_air    &lt;fct&gt; Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y…\n$ garage_cars    &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2…\n$ garage_area    &lt;dbl&gt; 528, 730, 312, 522, 482, 470, 582, 506, 608, 442, 440, …\n$ fireplace_qu   &lt;ord&gt; Good, No_Fireplace, No_Fireplace, Typical, Typical, Goo…\n$ bldg_type      &lt;fct&gt; OneFam, OneFam, OneFam, OneFam, OneFam, OneFam, TwnhsE,…\n$ house_style    &lt;fct&gt; One_Story, One_Story, One_Story, One_Story, Two_Story, …\n$ neighborhood   &lt;fct&gt; North_Ames, North_Ames, North_Ames, North_Ames, Gilbert…\n$ overall        &lt;fct&gt; Above_Average, Average, Above_Average, Good, Average, A…\n$ ms_zoning      &lt;fct&gt; Residential_Low_Density, Residential_High_Density, Resi…\n$ foundation     &lt;fct&gt; CBlock, CBlock, CBlock, CBlock, PConc, PConc, PConc, PC…\n$ baths_total    &lt;dbl&gt; 1.0, 1.0, 1.5, 2.5, 2.5, 2.5, 2.0, 2.0, 2.0, 2.5, 2.5, …\n$ age            &lt;dbl&gt; 50, 49, 52, 42, 13, 12, 9, 18, 15, 11, 17, 18, 12, 20, …\n$ remod_age      &lt;dbl&gt; 50, 49, 52, 42, 12, 12, 9, 18, 14, 11, 16, 3, 12, 20, 2…",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervision 5 — Hedonic House Prices (Ames, Iowa — real data)</span>"
    ]
  },
  {
    "objectID": "supervision_5.html#the-task-do-at-home",
    "href": "supervision_5.html#the-task-do-at-home",
    "title": "Supervision 5 — Hedonic House Prices (Ames, Iowa — real data)",
    "section": "The task (do at home)",
    "text": "The task (do at home)\nUsing the dataset hedonic_ames.csv, build a regression model to explain variation in sale_price using suitable structural and neighbourhood attributes. You may transform variables (e.g., logs), create interactions, or try regularised models (e.g., LASSO), but your final choice should be well-justified and interpretable.\n\nDeliverable (submit 1 day before class)\nUpload a single .R script containing: 1. Your final model (clearly named). 2. A short comment block describing your variable selection process (e.g., theory-driven shortlist → EDA → nested models → cross-validation). 3. A brief note on pros/cons of your chosen specification.\nWe will select a subset of students to present in class (5–6 mins).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervision 5 — Hedonic House Prices (Ames, Iowa — real data)</span>"
    ]
  },
  {
    "objectID": "supervision_5.html#data-dictionary-student-csv-hedonic_ames.csv",
    "href": "supervision_5.html#data-dictionary-student-csv-hedonic_ames.csv",
    "title": "Supervision 5 — Hedonic House Prices (Ames, Iowa — real data)",
    "section": "Data dictionary (student CSV: hedonic_ames.csv)",
    "text": "Data dictionary (student CSV: hedonic_ames.csv)\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nlisting_id\nRow identifier\n\n\nsale_price\nSale price in USD (target)\n\n\ngr_liv_area\nAbove-ground living area (sq ft)\n\n\ntotal_bsmt_sf\nBasement area (sq ft)\n\n\nlot_area\nLot size (sq ft)\n\n\nbedroom_abv_gr\nBedrooms (above ground)\n\n\nfull_bath, half_bath\nNumber of bathrooms (counts)\n\n\nkitchen_abv_gr\nNumber of kitchens\n\n\nbaths_total\nfull_bath + 0.5*half_bath\n\n\noverall_qual, overall_cond\nOverall quality/condition (ordinal 1–10)\n\n\nyear_built, year_remod_add\nConstruction & last remodel year\n\n\nage, remod_age\n2010 minus year built/remodel (engineered)\n\n\nexter_qual, kitchen_qual, fireplace_qu\nQuality ratings (ordered factors)\n\n\ncentral_air\nHas central air (Y/N)\n\n\ngarage_cars, garage_area\nGarage capacity (cars) and area (sq ft)\n\n\nbldg_type, house_style, neighborhood\nCategorical attributes\n\n\nms_zoning, foundation\nZoning and foundation type",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervision 5 — Hedonic House Prices (Ames, Iowa — real data)</span>"
    ]
  },
  {
    "objectID": "supervision_5.html#starter-code-r",
    "href": "supervision_5.html#starter-code-r",
    "title": "Supervision 5 — Hedonic House Prices (Ames, Iowa — real data)",
    "section": "Starter code (R)",
    "text": "Starter code (R)\n\n\nCode\nlibrary(readr); library(dplyr); library(ggplot2); library(broom); library(modelsummary)\n\n# 1) Load data (ensure you've run Step 0 once to create the CSV)\ndf &lt;- readr::read_csv(\"hedonic_ames.csv\")\n\n# 2) Transformations\ndf &lt;- df %&gt;% dplyr::mutate(\n  ln_price = log(sale_price),\n  ln_area  = log(gr_liv_area),\n  ln_lot   = log(lot_area)\n)\n\n# 3) Baseline hedonic (illustrative; replace with your own spec)\nm0 &lt;- lm(ln_price ~ ln_area + bedroom_abv_gr + baths_total + total_bsmt_sf +\n           garage_cars + central_air + overall_qual + overall_cond +\n           exter_qual + kitchen_qual + fireplace_qu +\n           bldg_type + house_style + neighborhood,\n         data = df)\n\nmodelsummary::modelsummary(list(\"Baseline\" = m0))\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Baseline\n              \n        \n        \n        \n                \n                  (Intercept)\n                  8.734\n                \n                \n                  \n                  (0.134)\n                \n                \n                  ln_area\n                  0.422\n                \n                \n                  \n                  (0.018)\n                \n                \n                  bedroom_abv_gr\n                  -0.016\n                \n                \n                  \n                  (0.004)\n                \n                \n                  baths_total\n                  0.043\n                \n                \n                  \n                  (0.007)\n                \n                \n                  total_bsmt_sf\n                  0.000\n                \n                \n                  \n                  (0.000)\n                \n                \n                  garage_cars\n                  0.053\n                \n                \n                  \n                  (0.005)\n                \n                \n                  central_airY\n                  0.086\n                \n                \n                  \n                  (0.012)\n                \n                \n                  overall_qualAverage\n                  -0.044\n                \n                \n                  \n                  (0.008)\n                \n                \n                  overall_qualBelow_Average\n                  -0.122\n                \n                \n                  \n                  (0.012)\n                \n                \n                  overall_qualExcellent\n                  0.237\n                \n                \n                  \n                  (0.021)\n                \n                \n                  overall_qualFair\n                  -0.173\n                \n                \n                  \n                  (0.023)\n                \n                \n                  overall_qualGood\n                  0.046\n                \n                \n                  \n                  (0.009)\n                \n                \n                  overall_qualPoor\n                  -0.249\n                \n                \n                  \n                  (0.041)\n                \n                \n                  overall_qualVery_Excellent\n                  0.147\n                \n                \n                  \n                  (0.035)\n                \n                \n                  overall_qualVery_Good\n                  0.127\n                \n                \n                  \n                  (0.012)\n                \n                \n                  overall_qualVery_Poor\n                  -0.177\n                \n                \n                  \n                  (0.080)\n                \n                \n                  overall_condAverage\n                  -0.031\n                \n                \n                  \n                  (0.007)\n                \n                \n                  overall_condBelow_Average\n                  -0.105\n                \n                \n                  \n                  (0.015)\n                \n                \n                  overall_condExcellent\n                  0.084\n                \n                \n                  \n                  (0.022)\n                \n                \n                  overall_condFair\n                  -0.271\n                \n                \n                  \n                  (0.020)\n                \n                \n                  overall_condGood\n                  0.047\n                \n                \n                  \n                  (0.009)\n                \n                \n                  overall_condPoor\n                  -0.248\n                \n                \n                  \n                  (0.044)\n                \n                \n                  overall_condVery_Good\n                  0.055\n                \n                \n                  \n                  (0.013)\n                \n                \n                  overall_condVery_Poor\n                  -0.413\n                \n                \n                  \n                  (0.052)\n                \n                \n                  exter_qualFair\n                  -0.082\n                \n                \n                  \n                  (0.033)\n                \n                \n                  exter_qualGood\n                  -0.042\n                \n                \n                  \n                  (0.020)\n                \n                \n                  exter_qualTypical\n                  -0.063\n                \n                \n                  \n                  (0.021)\n                \n                \n                  kitchen_qualFair\n                  -0.125\n                \n                \n                  \n                  (0.022)\n                \n                \n                  kitchen_qualGood\n                  -0.061\n                \n                \n                  \n                  (0.014)\n                \n                \n                  kitchen_qualPoor\n                  -0.012\n                \n                \n                  \n                  (0.133)\n                \n                \n                  kitchen_qualTypical\n                  -0.105\n                \n                \n                  \n                  (0.015)\n                \n                \n                  fireplace_quFair\n                  -0.037\n                \n                \n                  \n                  (0.026)\n                \n                \n                  fireplace_quGood\n                  -0.042\n                \n                \n                  \n                  (0.021)\n                \n                \n                  fireplace_quNo_Fireplace\n                  -0.081\n                \n                \n                  \n                  (0.022)\n                \n                \n                  fireplace_quPoor\n                  -0.050\n                \n                \n                  \n                  (0.029)\n                \n                \n                  fireplace_quTypical\n                  -0.043\n                \n                \n                  \n                  (0.022)\n                \n                \n                  bldg_typeOneFam\n                  0.118\n                \n                \n                  \n                  (0.015)\n                \n                \n                  bldg_typeTwnhs\n                  -0.018\n                \n                \n                  \n                  (0.024)\n                \n                \n                  bldg_typeTwnhsE\n                  0.048\n                \n                \n                  \n                  (0.019)\n                \n                \n                  bldg_typeTwoFmCon\n                  0.112\n                \n                \n                  \n                  (0.022)\n                \n                \n                  house_styleOne_and_Half_Unf\n                  0.032\n                \n                \n                  \n                  (0.032)\n                \n                \n                  house_styleOne_Story\n                  0.066\n                \n                \n                  \n                  (0.011)\n                \n                \n                  house_styleSFoyer\n                  0.166\n                \n                \n                  \n                  (0.019)\n                \n                \n                  house_styleSLvl\n                  0.072\n                \n                \n                  \n                  (0.015)\n                \n                \n                  house_styleTwo_and_Half_Fin\n                  0.032\n                \n                \n                  \n                  (0.048)\n                \n                \n                  house_styleTwo_and_Half_Unf\n                  0.034\n                \n                \n                  \n                  (0.028)\n                \n                \n                  house_styleTwo_Story\n                  0.020\n                \n                \n                  \n                  (0.010)\n                \n                \n                  neighborhoodBlueste\n                  -0.025\n                \n                \n                  \n                  (0.050)\n                \n                \n                  neighborhoodBriardale\n                  -0.102\n                \n                \n                  \n                  (0.039)\n                \n                \n                  neighborhoodBrookside\n                  -0.120\n                \n                \n                  \n                  (0.031)\n                \n                \n                  neighborhoodClear_Creek\n                  0.067\n                \n                \n                  \n                  (0.034)\n                \n                \n                  neighborhoodCollege_Creek\n                  0.030\n                \n                \n                  \n                  (0.028)\n                \n                \n                  neighborhoodCrawford\n                  0.058\n                \n                \n                  \n                  (0.030)\n                \n                \n                  neighborhoodEdwards\n                  -0.101\n                \n                \n                  \n                  (0.029)\n                \n                \n                  neighborhoodGilbert\n                  -0.003\n                \n                \n                  \n                  (0.029)\n                \n                \n                  neighborhoodGreen_Hills\n                  0.511\n                \n                \n                  \n                  (0.094)\n                \n                \n                  neighborhoodGreens\n                  0.102\n                \n                \n                  \n                  (0.054)\n                \n                \n                  neighborhoodIowa_DOT_and_Rail_Road\n                  -0.215\n                \n                \n                  \n                  (0.032)\n                \n                \n                  neighborhoodLandmark\n                  0.026\n                \n                \n                  \n                  (0.132)\n                \n                \n                  neighborhoodMeadow_Village\n                  -0.134\n                \n                \n                  \n                  (0.036)\n                \n                \n                  neighborhoodMitchell\n                  -0.007\n                \n                \n                  \n                  (0.030)\n                \n                \n                  neighborhoodNorth_Ames\n                  -0.058\n                \n                \n                  \n                  (0.028)\n                \n                \n                  neighborhoodNorthpark_Villa\n                  -0.046\n                \n                \n                  \n                  (0.039)\n                \n                \n                  neighborhoodNorthridge\n                  0.117\n                \n                \n                  \n                  (0.032)\n                \n                \n                  neighborhoodNorthridge_Heights\n                  0.112\n                \n                \n                  \n                  (0.028)\n                \n                \n                  neighborhoodNorthwest_Ames\n                  -0.062\n                \n                \n                  \n                  (0.030)\n                \n                \n                  neighborhoodOld_Town\n                  -0.203\n                \n                \n                  \n                  (0.030)\n                \n                \n                  neighborhoodSawyer\n                  -0.045\n                \n                \n                  \n                  (0.030)\n                \n                \n                  neighborhoodSawyer_West\n                  -0.018\n                \n                \n                  \n                  (0.029)\n                \n                \n                  neighborhoodSomerset\n                  0.074\n                \n                \n                  \n                  (0.028)\n                \n                \n                  neighborhoodSouth_and_West_of_Iowa_State_University\n                  -0.094\n                \n                \n                  \n                  (0.034)\n                \n                \n                  neighborhoodStone_Brook\n                  0.151\n                \n                \n                  \n                  (0.032)\n                \n                \n                  neighborhoodTimberland\n                  0.047\n                \n                \n                  \n                  (0.031)\n                \n                \n                  neighborhoodVeenker\n                  0.035\n                \n                \n                  \n                  (0.037)\n                \n                \n                  Num.Obs.\n                  2918\n                \n                \n                  R2\n                  0.898\n                \n                \n                  R2 Adj.\n                  0.896\n                \n                \n                  AIC\n                  -3623.1\n                \n                \n                  BIC\n                  -3174.7\n                \n                \n                  Log.Lik.\n                  1886.573\n                \n                \n                  F\n                  343.905\n                \n                \n                  RMSE\n                  0.13\n                \n        \n      \n    \n\n\n\nCode\n# 4) Diagnostics you could consider\npar(mfrow=c(2,2)); plot(m0); par(mfrow=c(1,1))\n\n\n\n\n\n\n\n\n\n\nIdeas to try: interactions (e.g., overall_qual:ln_area), non-linear terms (splines for area/age), or LASSO for screening before refitting an interpretable OLS.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervision 5 — Hedonic House Prices (Ames, Iowa — real data)</span>"
    ]
  },
  {
    "objectID": "supervision_5.html#in-class-flow-what-well-do-together",
    "href": "supervision_5.html#in-class-flow-what-well-do-together",
    "title": "Supervision 5 — Hedonic House Prices (Ames, Iowa — real data)",
    "section": "In-class flow (what we’ll do together)",
    "text": "In-class flow (what we’ll do together)\n\nMini-brief (5 mins): Recap hedonic logic and evaluation criteria.\nStudent presentations (5–6 mins each): Final model, variable selection, pros/cons.\nGroup discussion (time-boxed): Compare specifications; surface general lessons.\nWrap-up (5 mins): What would you try next? (spatial terms? non-linearities? robustness checks?)\n\n\nPresenter selection (staff only)\nStudents will submit code by 17:00 the day before. The instructor will sample 6–8 diverse specs (different model styles, feature sets, and performance levels) to maximise learning in class.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervision 5 — Hedonic House Prices (Ames, Iowa — real data)</span>"
    ]
  },
  {
    "objectID": "supervision_5.html#marking-guidance-formative-not-graded",
    "href": "supervision_5.html#marking-guidance-formative-not-graded",
    "title": "Supervision 5 — Hedonic House Prices (Ames, Iowa — real data)",
    "section": "Marking guidance (formative, not graded)",
    "text": "Marking guidance (formative, not graded)\n\nSpecification & theory fit (40%) – Does the model reflect sensible hedonic reasoning?\nMethod & justification (30%) – Are choices (transforms/selection) explained and defensible?\nDiagnostics (20%) – Are key assumptions checked and discussed?\nCommunication (10%) – Is the final model and its implications clear?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervision 5 — Hedonic House Prices (Ames, Iowa — real data)</span>"
    ]
  },
  {
    "objectID": "supervision_5.html#submission-details",
    "href": "supervision_5.html#submission-details",
    "title": "Supervision 5 — Hedonic House Prices (Ames, Iowa — real data)",
    "section": "Submission details",
    "text": "Submission details\n\nSubmit one .R file named studentid_supervision5.R.\nInclude: (i) final model; (ii) notes on selection process; (iii) pros/cons.\nDeadline: 17:00, the day before supervision.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervision 5 — Hedonic House Prices (Ames, Iowa — real data)</span>"
    ]
  }
]