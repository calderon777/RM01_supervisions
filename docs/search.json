[
  {
    "objectID": "supervision_6.html",
    "href": "supervision_6.html",
    "title": "Supervision 6",
    "section": "",
    "text": "Lab 19 ‚Äî Qualitative Dependent Variables: Logit vs Probit on the Hedonic Model",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Supervision 6</span>"
    ]
  },
  {
    "objectID": "supervision_6.html#lab-19-qualitative-dependent-variables-logit-vs-probit-on-the-hedonic-model",
    "href": "supervision_6.html#lab-19-qualitative-dependent-variables-logit-vs-probit-on-the-hedonic-model",
    "title": "Supervision 6",
    "section": "",
    "text": "This supervision builds directly on Supervision 5. We will keep the same hedonic right-hand-side (RHS) variables, but transform the dependent variable into High vs Low Price using the sample median. You will fit logit and probit models, interpret marginal effects, and assess predictive performance.\n\n\n\nüéØ Learning Outcomes\nBy the end of this lab you will be able to:\n\nTransform a continuous target into a binary target using the median.\nFit Logit and Probit models in R with the same hedonic covariates used previously.\nInterpret Average Marginal Effects (AMEs) in probability terms.\nCompare model performance via ROC curves and AUC.\nBuild confusion matrices at a chosen threshold and compute basic metrics.\n\n\n‚úîÔ∏è Step-by-step workflow\n\nThe sections follow a logical sequence:\nData setup\nBinary transformation\nModel estimation\nInterpretation\nForecasting ‚Üí performance evaluation\n\n\n\n\nüì¶ Setup & Data\nWe reuse the dataset prepared in Supervision 5 (we called the dataset df, see Sup 5 - ‚ÄúData Preparation‚Äù).\n\n# If a package is missing on your machine, run install.packages(\"package name\")\n# Example: install.packages(\"readr\")\n\n# Load core packages used in this lab\nlibrary(readr)     # reading data\nlibrary(dplyr)     # data manipulation\nlibrary(ggplot2)   # plotting\n\n# Optional/advanced packages for inference and diagnostics\n# If missing, install as needed:\n# install.packages(c(\"margins\", \"sandwich\", \"lmtest\", \"pROC\"))\nlibrary(margins)   # marginal effects\nlibrary(sandwich)  # robust (HC) variance\nlibrary(lmtest)    # coeftest with robust SE\n#git addinstall.packages(\"pROC\", repos = \"https://cloud.r-project.org\")\nlibrary(pROC)      # ROC and AUC\n\n# ---- Load the data using Supervision 5 - Section: data preparation\n# Adjust the path if your file is elsewhere.\n\n# Quick preview\ndplyr::glimpse(df)\n\nRows: 2,918\nColumns: 29\n$ listing_id     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, ‚Ä¶\n$ sale_price     &lt;dbl&gt; 215000, 105000, 172000, 244000, 189900, 195500, 213500,‚Ä¶\n$ gr_liv_area    &lt;dbl&gt; 1656, 896, 1329, 2110, 1629, 1604, 1338, 1280, 1616, 18‚Ä¶\n$ total_bsmt_sf  &lt;dbl&gt; 1080, 882, 1329, 2110, 928, 926, 1338, 1280, 1595, 994,‚Ä¶\n$ lot_area       &lt;dbl&gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005, 53‚Ä¶\n$ bedroom_abv_gr &lt;dbl&gt; 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 1, 4, 4, 1, 2‚Ä¶\n$ full_bath      &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, 1, 1‚Ä¶\n$ half_bath      &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0‚Ä¶\n$ kitchen_abv_gr &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n$ overall_qual   &lt;ord&gt; Above_Average, Average, Above_Average, Good, Average, A‚Ä¶\n$ overall_cond   &lt;ord&gt; Average, Above_Average, Above_Average, Average, Average‚Ä¶\n$ year_built     &lt;dbl&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 1995, 1‚Ä¶\n$ year_remod_add &lt;dbl&gt; 1960, 1961, 1958, 1968, 1998, 1998, 2001, 1992, 1996, 1‚Ä¶\n$ exter_qual     &lt;ord&gt; Typical, Typical, Typical, Good, Typical, Typical, Good‚Ä¶\n$ kitchen_qual   &lt;ord&gt; Typical, Typical, Good, Excellent, Typical, Good, Good,‚Ä¶\n$ central_air    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", ‚Ä¶\n$ garage_cars    &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2‚Ä¶\n$ garage_area    &lt;dbl&gt; 528, 730, 312, 522, 482, 470, 582, 506, 608, 442, 440, ‚Ä¶\n$ fireplace_qu   &lt;ord&gt; Good, No_Fireplace, No_Fireplace, Typical, Typical, Goo‚Ä¶\n$ bldg_type      &lt;fct&gt; OneFam, OneFam, OneFam, OneFam, OneFam, OneFam, TwnhsE,‚Ä¶\n$ house_style    &lt;fct&gt; One_Story, One_Story, One_Story, One_Story, Two_Story, ‚Ä¶\n$ neighborhood   &lt;fct&gt; North_Ames, North_Ames, North_Ames, North_Ames, Gilbert‚Ä¶\n$ ms_zoning      &lt;chr&gt; \"Residential_Low_Density\", \"Residential_High_Density\", ‚Ä¶\n$ foundation     &lt;chr&gt; \"CBlock\", \"CBlock\", \"CBlock\", \"CBlock\", \"PConc\", \"PConc‚Ä¶\n$ baths_total    &lt;dbl&gt; 1.0, 1.0, 1.5, 2.5, 2.5, 2.5, 2.0, 2.0, 2.0, 2.5, 2.5, ‚Ä¶\n$ age            &lt;dbl&gt; 50, 49, 52, 42, 13, 12, 9, 18, 15, 11, 17, 18, 12, 20, ‚Ä¶\n$ remod_age      &lt;dbl&gt; 50, 49, 52, 42, 12, 12, 9, 18, 14, 11, 16, 3, 12, 20, 2‚Ä¶\n$ ln_price       &lt;dbl&gt; 12.27839, 11.56172, 12.05525, 12.40492, 12.15425, 12.18‚Ä¶\n$ ln_area        &lt;dbl&gt; 7.412160, 6.797940, 7.192182, 7.654443, 7.395722, 7.380‚Ä¶\n\n\n\n\n\nüß† From Continuous to Binary: Define High vs Low Price\nWe create a binary target equal to 1 if sale_price is at least the sample median and 0 otherwise. The median splits the sample into two balanced groups.\n\n# Compute the sample median of the sale price\nprice_med &lt;- median(df$sale_price, na.rm = TRUE)\n\n# Create the binary dependent variable\ndf &lt;- df %&gt;% mutate(high_price = as.integer(sale_price &gt;= price_med))\n\n# Check class balance\ntable(df$high_price)\n\n\n   0    1 \n1442 1476 \n\nprop.table(table(df$high_price))\n\n\n        0         1 \n0.4941741 0.5058259 \n\n\n\n\n\n\n\n\n‚ú® Why logit & probit exist (the aha! moment)\n\n\n\nOLS fails when the dependent variable is 0/1 because predicted values can be &lt;0 or &gt;1. Instead of predicting price category directly, logit and probit assume:\n\nThere is an unobserved (latent) index of ‚Äúhousing quality/value‚Äù and we only observe whether it crosses a threshold.\n\n\\(y_i^* = X_i \\beta + \\varepsilon_i\n\\quad\\rightarrow\\quad\ny_i = \\begin{cases}\n1 & \\text{if } y_i^* &gt; 0 \\\\\n0 & \\text{otherwise}\n\\end{cases}\\)\nThe difference is the distribution of the error term \\(\\varepsilon\\):\n\n\n\nModel\nAssumption for ( )\nEffect\n\n\n\n\nLogit\nLogistic distribution\nCoefficients scale with odds.\n\n\nProbit\nNormal distribution\nCoefficients on z-score scale.\n\n\n\nBecause logistic and normal are very similar:\n\nLogit coefficient ‚âà 1.6 √ó probit coefficient\n\nBut both produce nearly identical predicted probabilities and classification.\n\n\n\n\n\nüß© Model Specification (same RHS as Supervision 5)\nWe keep the hedonic RHS unchanged. Only the outcome changed (continuous -&gt; binary).\n\n# Hedonic formula reused for binary models\nf_bin &lt;- high_price ~ ln_area + bedroom_abv_gr + baths_total +\n  total_bsmt_sf + garage_cars + central_air + overall_qual +\n  overall_cond + exter_qual + kitchen_qual + fireplace_qu +\n  bldg_type + house_style + neighborhood\n\n\n\n\n‚öôÔ∏è Estimation (fit only) + ü™ì Backward Selection (BIC)\nWe now estimate the latent model using glm():\n\\(P(y = 1 \\mid X) = \\text{link}^{-1}(X\\beta)\\)\n\nlogit ‚Üí inverse-logit (plogis()): returns a probability via odds\nprobit ‚Üí normal CDF (pnorm()): returns a probability via z-score\n\nLogit uses the logistic CDF (cumulative distribution function); probit uses the (scaled) normal CDF. They almost overlap, which is why results are so similar and logit ‚âà 1.6 √ó probit in scale. Let‚Äôs compare the CDFs:\n\n# Logistic vs Probit (variance-matched) ‚Äî super short CDF plot\nsd_match &lt;- pi / sqrt(3)              # match Normal variance to Logistic\ncurve(plogis(x), from=-6, to=6, lwd=2, xlab=\"Latent index (x)\", ylab=\"CDF\",\n      main=\"CDF: Logistic vs Normal (variance-matched)\")\ncurve(pnorm(x, sd=sd_match), from=-6, to=6, lwd=2, lty=2, add=TRUE)\nlegend(\"topleft\", c(\"Logistic (logit)\", \"Normal scaled (probit)\"),\n       lwd=2, lty=c(1,2), bty=\"n\")\n\n\n\n\n\n\n\n\nIn the next chunk we suppress printing baseline models to avoid overwhelming output (too many variables). However, we run backward stepwise (BIC) and present a compact, readable table of that selected model. Coefficient signs and significance are typically very similar; logit coefficients are usually ~1.6√ó probit coefficients (scale difference).\n\n# --- Packages used in this chunk -------------------------------------------\nlibrary(dplyr)      # data wrangling\nlibrary(broom)      # tidy model outputs (term, estimate, std.error, statistic, p.value)\nlibrary(stringr)    # clean up long variable names\nlibrary(gt)         # neat tables\nlibrary(lmtest)     # coeftest() with robust SEs\nlibrary(sandwich)   # vcovHC() for robust (HC) variance\n\n# --- Convert ordered factors to plain factors --------------------------------\n# Why? Ordered factors produce polynomial contrasts (.L, .Q, ...) that are hard to interpret.\n# Using plain factors yields standard dummies and lets step() drop/keep whole factors cleanly.\ndf &lt;- df |&gt;\n  mutate(\n    overall_qual = as.factor(overall_qual),\n    overall_cond = as.factor(overall_cond),\n    exter_qual   = as.factor(exter_qual),\n    kitchen_qual = as.factor(kitchen_qual),\n    fireplace_qu = as.factor(fireplace_qu),\n    central_air  = as.factor(central_air)  # ensure \"Y\"/\"N\" is categorical\n  )\n\n# --- Fit full LOGIT & PROBIT (we suppress long summaries for readability) ----\nlogit_full  &lt;- glm(f_bin, data = df, family = binomial(\"logit\"))\nprobit_full &lt;- glm(f_bin, data = df, family = binomial(\"probit\"))\n\n# --- Backward stepwise with BIC (k = log(n)) on both models ------------------\n# BIC penalizes complexity more than AIC -&gt; typically a leaner, more generalizable model.\nk_bic &lt;- log(nrow(df))\nlogit_bic  &lt;- step(logit_full,  direction = \"backward\", k = k_bic, trace = 0)\nprobit_bic &lt;- step(probit_full, direction = \"backward\", k = k_bic, trace = 0)\n\n# --- Quick fit comparison (lower is better) ----------------------------------\nbic_tab &lt;- tibble(\n  model = c(\"logit_bic\", \"probit_bic\"),\n  AIC   = c(AIC(logit_bic),  AIC(probit_bic)),\n  BIC   = c(BIC(logit_bic),  BIC(probit_bic))\n)\nbic_tab  # glance: which BIC-selected model scores better?\n\n# A tibble: 2 √ó 3\n  model        AIC   BIC\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 logit_bic  1251. 1418.\n2 probit_bic 1266. 1433.\n\n# --- Keep BOTH models for downstream sections --------------------------------\nmain_logit &lt;- logit_bic   # LOGIT (BIC-selected)\nmain_probit &lt;- probit_bic  # PROBIT (BIC-selected)\n\n# --- Also clarify which one is \"best\" by BIC ---------------------------------\nbest_model &lt;- if (BIC(logit_bic) &lt; BIC(probit_bic)) {\n  message(\"‚úÖ BIC selects the LOGIT model as best (more parsimonious).\")\n  logit_bic\n} else {\n  message(\"‚úÖ BIC selects the PROBIT model as best (more parsimonious).\")\n  probit_bic\n}\n\n# --- Helper: compact robust-SE table for any glm -----------------------------\nmake_compact_table &lt;- function(mod, title_text) {\n  # Robust HC1 variance-covariance and tidy coefficients\n  rob_vcov &lt;- sandwich::vcovHC(mod, type = \"HC1\")\n  out &lt;- broom::tidy(lmtest::coeftest(mod, vcov. = rob_vcov)) |&gt;\n    filter(term != \"(Intercept)\") |&gt;\n    arrange(p.value) |&gt;\n    # Friendlier labels for students (extend as needed)\n    mutate(\n      term = term |&gt;\n        str_replace(\"^ln_area$\", \"ln(living area)\") |&gt;\n        str_replace(\"^baths_total$\", \"Total baths\") |&gt;\n        str_replace(\"^garage_cars$\", \"Garage capacity\") |&gt;\n        str_replace(\"^central_airY$\", \"Central air: Yes\") |&gt;\n        str_replace(\"^bldg_type\", \"Bldg type: \") |&gt;\n        str_replace(\"^house_style\", \"Style: \") |&gt;\n        str_replace(\"^neighborhood\", \"Nbh: \")\n    ) |&gt;\n    slice_head(n = 20) |&gt;\n    mutate(\n      estimate  = round(estimate, 4),\n      std.error = round(std.error, 4),\n      statistic = round(statistic, 4),\n      p.value   = round(p.value, 4)\n    )\n\n  # Render a gt table (no exotic functions to avoid errors)\n  out |&gt;\n    gt() |&gt;\n    tab_header(title = title_text) |&gt;\n    cols_label(\n      term = \"Term\", estimate = \"Coef\", std.error = \"Robust SE\",\n      statistic = \"z\", p.value = \"p\"\n    ) |&gt;\n    tab_options(table.width = pct(100), data_row.padding = px(3))\n}\n\n# --- Print compact tables for BOTH models ------------------------------------\nmake_compact_table(main_logit, \"Logit (BIC-selected): Key Terms ‚Äî Robust SEs\")\n\n\n\n\n\n\n\nLogit (BIC-selected): Key Terms ‚Äî Robust SEs\n\n\nTerm\nCoef\nRobust SE\nz\np\n\n\n\n\noverall_qual^9\n15.1397\n0.7831\n19.3326\n0.0000\n\n\nexter_qual.L\n-10.6344\n0.5561\n-19.1225\n0.0000\n\n\noverall_qual^4\n-12.5784\n0.7454\n-16.8753\n0.0000\n\n\nexter_qual.Q\n7.0991\n0.6307\n11.2550\n0.0000\n\n\nTotal baths\n1.7096\n0.2143\n7.9788\n0.0000\n\n\nln(living area)\n3.9021\n0.5224\n7.4689\n0.0000\n\n\ntotal_bsmt_sf\n0.0020\n0.0003\n7.1608\n0.0000\n\n\noverall_qual.C\n-8.3438\n1.1840\n-7.0472\n0.0000\n\n\noverall_qual^5\n-5.4813\n0.7807\n-7.0212\n0.0000\n\n\nexter_qual.C\n-4.1839\n0.7068\n-5.9199\n0.0000\n\n\nGarage capacity\n0.8858\n0.1569\n5.6446\n0.0000\n\n\noverall_qual.L\n-4.1405\n0.9283\n-4.4603\n0.0000\n\n\nBldg type: OneFam\n2.0304\n0.5295\n3.8341\n0.0001\n\n\nbedroom_abv_gr\n-0.5331\n0.1417\n-3.7617\n0.0002\n\n\noverall_qual^8\n3.7655\n1.0726\n3.5106\n0.0004\n\n\nCentral air: Yes\n1.9202\n0.5563\n3.4518\n0.0006\n\n\nfireplace_qu.Q\n0.9649\n0.3096\n3.1168\n0.0018\n\n\nfireplace_qu^5\n-0.4994\n0.2292\n-2.1787\n0.0294\n\n\nBldg type: TwoFmCon\n1.5261\n0.7411\n2.0594\n0.0395\n\n\noverall_qual^6\n1.1504\n0.6616\n1.7389\n0.0820\n\n\n\n\n\n\nmake_compact_table(main_probit, \"Probit (BIC-selected): Key Terms ‚Äî Robust SEs  üìå We will also use this downstream\")\n\n\n\n\n\n\n\nProbit (BIC-selected): Key Terms ‚Äî Robust SEs üìå We will also use this downstream\n\n\nTerm\nCoef\nRobust SE\nz\np\n\n\n\n\nexter_qual.L\n-3.8454\n0.2067\n-18.5996\n0.0000\n\n\noverall_qual^4\n-3.2031\n0.2120\n-15.1082\n0.0000\n\n\noverall_qual^9\n4.2012\n0.3855\n10.8982\n0.0000\n\n\nexter_qual.Q\n2.4456\n0.2911\n8.4024\n0.0000\n\n\nTotal baths\n0.9290\n0.1245\n7.4616\n0.0000\n\n\noverall_qual.C\n-2.6018\n0.3538\n-7.3531\n0.0000\n\n\nln(living area)\n2.0371\n0.2844\n7.1631\n0.0000\n\n\ntotal_bsmt_sf\n0.0011\n0.0002\n6.7114\n0.0000\n\n\nGarage capacity\n0.4570\n0.0933\n4.8984\n0.0000\n\n\nexter_qual.C\n-1.6890\n0.3597\n-4.6950\n0.0000\n\n\nCentral air: Yes\n1.0294\n0.2772\n3.7132\n0.0002\n\n\nBldg type: OneFam\n1.0286\n0.3022\n3.4032\n0.0007\n\n\nbedroom_abv_gr\n-0.2609\n0.0787\n-3.3141\n0.0009\n\n\noverall_qual^7\n-1.1811\n0.3584\n-3.2956\n0.0010\n\n\nfireplace_qu.Q\n0.5618\n0.1757\n3.1978\n0.0014\n\n\noverall_qual^5\n-1.3032\n0.4239\n-3.0741\n0.0021\n\n\noverall_qual.Q\n0.8228\n0.3084\n2.6678\n0.0076\n\n\noverall_qual.L\n-0.7386\n0.2871\n-2.5723\n0.0101\n\n\nfireplace_qu^5\n-0.2574\n0.1272\n-2.0234\n0.0430\n\n\nBldg type: TwoFmCon\n0.7736\n0.4246\n1.8219\n0.0685\n\n\n\n\n\n\n# --- (Optional) Kept vs Dropped list for transparency ------------------------\n# Uncomment to show what BIC dropped vs. kept for each model.\n# kept1   &lt;- attr(terms(main_logit), \"term.labels\"); full1 &lt;- attr(terms(logit_full),  \"term.labels\")\n# kept2   &lt;- attr(terms(main_probit), \"term.labels\"); full2 &lt;- attr(terms(probit_full), \"term.labels\")\n# dropped1 &lt;- setdiff(full1, kept1); dropped2 &lt;- setdiff(full2, kept2)\n# tibble(model = \"LOGIT\", status = \"Kept\", term = kept1) |&gt;\n#   bind_rows(tibble(model = \"LOGIT\", status = \"Dropped\", term = dropped1)) |&gt;\n#   bind_rows(tibble(model = \"PROBIT\", status = \"Kept\", term = kept2)) |&gt;\n#   bind_rows(tibble(model = \"PROBIT\", status = \"Dropped\", term = dropped2)) |&gt;\n#   arrange(model, status, term) |&gt;\n#   gt() |&gt;\n#   tab_header(title = \"Backward BIC: Kept vs Dropped Terms (Logit & Probit)\") |&gt;\n#   tab_options(table.width = pct(100))\n\nRobust Standard Errors (recommended for cross-sections)\n\n# HC1 robust SEs using sandwich + lmtest\ncoeftest(main_logit,  vcov. = vcovHC(main_logit,  type = \"HC1\"))\n\n\nz test of coefficients:\n\n                     Estimate  Std. Error  z value  Pr(&gt;|z|)    \n(Intercept)       -3.5899e+01  3.8264e+00  -9.3819 &lt; 2.2e-16 ***\nln_area            3.9021e+00  5.2244e-01   7.4689 8.086e-14 ***\nbedroom_abv_gr    -5.3308e-01  1.4171e-01  -3.7617 0.0001688 ***\nbaths_total        1.7096e+00  2.1427e-01   7.9788 1.478e-15 ***\ntotal_bsmt_sf      1.9908e-03  2.7802e-04   7.1608 8.021e-13 ***\ngarage_cars        8.8578e-01  1.5693e-01   5.6446 1.656e-08 ***\ncentral_airY       1.9202e+00  5.5629e-01   3.4518 0.0005569 ***\noverall_qual.L    -4.1405e+00  9.2831e-01  -4.4603 8.185e-06 ***\noverall_qual.Q     1.2659e+00  1.1710e+00   1.0811 0.2796449    \noverall_qual.C    -8.3438e+00  1.1840e+00  -7.0472 1.825e-12 ***\noverall_qual^4    -1.2578e+01  7.4537e-01 -16.8753 &lt; 2.2e-16 ***\noverall_qual^5    -5.4813e+00  7.8068e-01  -7.0212 2.199e-12 ***\noverall_qual^6     1.1504e+00  6.6158e-01   1.7389 0.0820451 .  \noverall_qual^7    -1.3063e-01  7.9330e-01  -0.1647 0.8692110    \noverall_qual^8     3.7655e+00  1.0726e+00   3.5106 0.0004472 ***\noverall_qual^9     1.5140e+01  7.8312e-01  19.3326 &lt; 2.2e-16 ***\nexter_qual.L      -1.0634e+01  5.5612e-01 -19.1225 &lt; 2.2e-16 ***\nexter_qual.Q       7.0991e+00  6.3075e-01  11.2550 &lt; 2.2e-16 ***\nexter_qual.C      -4.1839e+00  7.0676e-01  -5.9199 3.222e-09 ***\nfireplace_qu.L    -1.9306e-01  3.6918e-01  -0.5230 0.6010086    \nfireplace_qu.Q     9.6486e-01  3.0957e-01   3.1168 0.0018281 ** \nfireplace_qu.C     3.9833e-01  3.8274e-01   1.0407 0.2980044    \nfireplace_qu^4    -7.1073e-02  3.7366e-01  -0.1902 0.8491463    \nfireplace_qu^5    -4.9939e-01  2.2922e-01  -2.1787 0.0293574 *  \nbldg_typeOneFam    2.0304e+00  5.2955e-01   3.8341 0.0001260 ***\nbldg_typeTwnhs    -5.7459e-01  6.8362e-01  -0.8405 0.4006233    \nbldg_typeTwnhsE    1.9953e-01  6.4498e-01   0.3094 0.7570455    \nbldg_typeTwoFmCon  1.5261e+00  7.4106e-01   2.0594 0.0394569 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncoeftest(main_probit, vcov. = vcovHC(main_probit, type = \"HC1\"))\n\n\nz test of coefficients:\n\n                     Estimate  Std. Error  z value  Pr(&gt;|z|)    \n(Intercept)       -1.8833e+01  2.0729e+00  -9.0853 &lt; 2.2e-16 ***\nln_area            2.0371e+00  2.8439e-01   7.1631 7.889e-13 ***\nbedroom_abv_gr    -2.6092e-01  7.8730e-02  -3.3141 0.0009193 ***\nbaths_total        9.2897e-01  1.2450e-01   7.4616 8.546e-14 ***\ntotal_bsmt_sf      1.0688e-03  1.5925e-04   6.7114 1.927e-11 ***\ngarage_cars        4.5703e-01  9.3300e-02   4.8984 9.660e-07 ***\ncentral_airY       1.0294e+00  2.7722e-01   3.7132 0.0002046 ***\noverall_qual.L    -7.3856e-01  2.8713e-01  -2.5723 0.0101034 *  \noverall_qual.Q     8.2278e-01  3.0841e-01   2.6678 0.0076351 ** \noverall_qual.C    -2.6018e+00  3.5383e-01  -7.3531 1.936e-13 ***\noverall_qual^4    -3.2031e+00  2.1201e-01 -15.1082 &lt; 2.2e-16 ***\noverall_qual^5    -1.3032e+00  4.2393e-01  -3.0741 0.0021117 ** \noverall_qual^6    -2.6588e-01  3.2419e-01  -0.8201 0.4121312    \noverall_qual^7    -1.1811e+00  3.5838e-01  -3.2956 0.0009821 ***\noverall_qual^8     3.6818e-01  5.6090e-01   0.6564 0.5115536    \noverall_qual^9     4.2012e+00  3.8550e-01  10.8982 &lt; 2.2e-16 ***\nexter_qual.L      -3.8454e+00  2.0675e-01 -18.5996 &lt; 2.2e-16 ***\nexter_qual.Q       2.4456e+00  2.9106e-01   8.4024 &lt; 2.2e-16 ***\nexter_qual.C      -1.6890e+00  3.5974e-01  -4.6950 2.666e-06 ***\nfireplace_qu.L    -1.3497e-01  1.9931e-01  -0.6772 0.4982982    \nfireplace_qu.Q     5.6178e-01  1.7568e-01   3.1978 0.0013850 ** \nfireplace_qu.C     2.1859e-01  2.1709e-01   1.0069 0.3139763    \nfireplace_qu^4    -4.2936e-03  2.1376e-01  -0.0201 0.9839743    \nfireplace_qu^5    -2.5744e-01  1.2724e-01  -2.0234 0.0430357 *  \nbldg_typeOneFam    1.0286e+00  3.0224e-01   3.4032 0.0006659 ***\nbldg_typeTwnhs    -3.3721e-01  3.8970e-01  -0.8653 0.3868682    \nbldg_typeTwnhsE    1.3499e-01  3.6188e-01   0.3730 0.7091348    \nbldg_typeTwoFmCon  7.7362e-01  4.2463e-01   1.8219 0.0684741 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\nWhy marginal effects?\n\n\n\nLogit/probit coefficients are not changes in probability ‚Äî they change the latent index.\nMarginal effects compute:\n\n‚ÄúHolding everything else constant, how does \\(P(y=1)\\) change when \\(x\\) increases by 1?‚Äù\n\nThat is why AMEs are interpretable in percentage points.\n\n\n\n\nüìà Interpreting Effects with Marginal Effects (AMEs)\nRaw coefficients are on latent/odds scales. Average Marginal Effects (AMEs) translate them to percentage-point changes in probability for a one-unit change in each predictor.\n\n# Average Marginal Effects for both models\nme_logit  &lt;- margins(main_logit)\nme_probit &lt;- margins(main_probit)\n\nsummary(me_logit)\n\n                     factor     AME      SE       z      p    lower   upper\n                baths_total  0.1052  0.0103 10.1961 0.0000   0.0850  0.1254\n             bedroom_abv_gr -0.0328  0.0080 -4.1198 0.0000  -0.0484 -0.0172\n            bldg_typeOneFam  0.1235  0.0282  4.3879 0.0000   0.0684  0.1787\n             bldg_typeTwnhs -0.0367  0.0370 -0.9921 0.3212  -0.1093  0.0358\n            bldg_typeTwnhsE  0.0125  0.0348  0.3608 0.7183  -0.0556  0.0807\n          bldg_typeTwoFmCon  0.0933  0.0407  2.2886 0.0221   0.0134  0.1731\n               central_airY  0.1196  0.0323  3.7083 0.0002   0.0564  0.1829\n             exter_qualFair -0.4655  0.7340 -0.6341 0.5260  -1.9041  0.9732\n             exter_qualGood -0.4043  0.7312 -0.5529 0.5803  -1.8373  1.0288\n          exter_qualTypical -0.5041  0.7311 -0.6895 0.4905  -1.9369  0.9288\n           fireplace_quFair -0.0314  0.1720 -0.1823 0.8554  -0.3685  0.3058\n           fireplace_quGood -0.0345  0.1703 -0.2025 0.8395  -0.3683  0.2993\n   fireplace_quNo_Fireplace -0.0953  0.1702 -0.5600 0.5755  -0.4289  0.2383\n           fireplace_quPoor -0.0475  0.1730 -0.2747 0.7835  -0.3866  0.2915\n        fireplace_quTypical  0.0002  0.1703  0.0013 0.9989  -0.3336  0.3341\n                garage_cars  0.0545  0.0085  6.4286 0.0000   0.0379  0.0711\n                    ln_area  0.2401  0.0299  8.0421 0.0000   0.1816  0.2986\n        overall_qualAverage -0.0792  0.0153 -5.1632 0.0000  -0.1093 -0.0492\n  overall_qualBelow_Average -0.1616  0.0354 -4.5675 0.0000  -0.2310 -0.0923\n      overall_qualExcellent -0.0424  0.0888 -0.4776 0.6329  -0.2166  0.1317\n           overall_qualFair -0.4835  0.9774 -0.4947 0.6208  -2.3991  1.4321\n           overall_qualGood  0.0595  0.0181  3.2782 0.0010   0.0239  0.0951\n           overall_qualPoor -0.4812  1.4431 -0.3334 0.7388  -3.3097  2.3473\n overall_qualVery_Excellent  0.4305 23.4932  0.0183 0.9854 -45.6152 46.4763\n      overall_qualVery_Good  0.2160  0.0462  4.6738 0.0000   0.1254  0.3066\n      overall_qualVery_Poor -0.4822  1.2336 -0.3909 0.6959  -2.9000  1.9356\n              total_bsmt_sf  0.0001  0.0000  8.2160 0.0000   0.0001  0.0002\n\nsummary(me_probit)\n\n                     factor     AME      SE       z      p    lower   upper\n                baths_total  0.1077  0.0106 10.1842 0.0000   0.0870  0.1285\n             bedroom_abv_gr -0.0303  0.0081 -3.7268 0.0002  -0.0462 -0.0143\n            bldg_typeOneFam  0.1178  0.0273  4.3118 0.0000   0.0643  0.1713\n             bldg_typeTwnhs -0.0397  0.0371 -1.0709 0.2842  -0.1124  0.0330\n            bldg_typeTwnhsE  0.0157  0.0340  0.4622 0.6439  -0.0510  0.0824\n          bldg_typeTwoFmCon  0.0888  0.0413  2.1505 0.0315   0.0079  0.1698\n               central_airY  0.1208  0.0320  3.7760 0.0002   0.0581  0.1836\n             exter_qualFair -0.4737  0.9976 -0.4749 0.6349  -2.4290  1.4815\n             exter_qualGood -0.3996  0.9951 -0.4016 0.6880  -2.3501  1.5508\n          exter_qualTypical -0.5063  0.9951 -0.5088 0.6109  -2.4566  1.4441\n           fireplace_quFair -0.0386  0.1353 -0.2854 0.7753  -0.3038  0.2266\n           fireplace_quGood -0.0419  0.1330 -0.3150 0.7528  -0.3027  0.2189\n   fireplace_quNo_Fireplace -0.1030  0.1330 -0.7746 0.4386  -0.3636  0.1576\n           fireplace_quPoor -0.0593  0.1370 -0.4327 0.6652  -0.3277  0.2092\n        fireplace_quTypical -0.0040  0.1331 -0.0303 0.9758  -0.2650  0.2569\n                garage_cars  0.0530  0.0087  6.0966 0.0000   0.0360  0.0700\n                    ln_area  0.2363  0.0302  7.8258 0.0000   0.1771  0.2955\n        overall_qualAverage -0.0868  0.0159 -5.4681 0.0000  -0.1179 -0.0557\n  overall_qualBelow_Average -0.1528  0.0339 -4.5071 0.0000  -0.2192 -0.0863\n      overall_qualExcellent -0.1140  0.0652 -1.7473 0.0806  -0.2418  0.0139\n           overall_qualFair -0.4761  2.2142 -0.2150 0.8298  -4.8159  3.8637\n           overall_qualGood  0.0638  0.0184  3.4662 0.0005   0.0277  0.0999\n           overall_qualPoor -0.3782 29.5342 -0.0128 0.9898 -58.2641 57.5077\n overall_qualVery_Excellent  0.0921 26.3486  0.0035 0.9972 -51.5502 51.7344\n      overall_qualVery_Good  0.1886  0.0420  4.4916 0.0000   0.1063  0.2709\n      overall_qualVery_Poor -0.4340 35.3525 -0.0123 0.9902 -69.7235 68.8555\n              total_bsmt_sf  0.0001  0.0000  8.2283 0.0000   0.0001  0.0002\n\n# Tip for reporting:\n# \"A 0.01 increase in ln_area (~1% more area) changes the probability of being high-priced by X percentage points (AME).\"\n\n\n\n\nüîÆ Predicted Probabilities & Agreement Between Models\n\n# Predicted probabilities from each model\ndf &lt;- df %&gt;%\n  mutate(\n    p_logit  = predict(main_logit,  type = \"response\"),\n    p_probit = predict(main_probit, type = \"response\")\n  )\n\n# Compare probability predictions\ncor(df$p_logit, df$p_probit, use = \"complete.obs\")\n\n[1] 0.9993592\n\nsummary(df$p_logit)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00000 0.02626 0.49132 0.50583 0.97866 1.00000 \n\nsummary(df$p_probit)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00000 0.02449 0.48779 0.50484 0.98036 1.00000 \n\n\n\n\n\nüß™ Classification Diagnostics: ROC & AUC\nWe evaluate ranking performance via ROC curves and AUC.\n\n# ROC objects (treating 1 as the positive class)\nroc_logit  &lt;- roc(df$high_price, df$p_logit)\nroc_probit &lt;- roc(df$high_price, df$p_probit)\n\n# AUC values\nauc(roc_logit)\n\nArea under the curve: 0.9746\n\nauc(roc_probit)\n\nArea under the curve: 0.9745\n\n# Simple ROC plots with ggplot2\nautoplot_roc &lt;- function(roc_obj, title){\n  # Build a data frame with TPR (sensitivity) and FPR (1 - specificity)\n  plot_df &lt;- data.frame(\n    tpr = roc_obj$sensitivities,\n    fpr = 1 - roc_obj$specificities\n  )\n  ggplot(plot_df, aes(x = fpr, y = tpr)) +\n    geom_line() +\n    geom_abline(slope = 1, intercept = 0, linetype = 2) +\n    labs(x = \"False Positive Rate\", y = \"True Positive Rate\", title = title)\n}\n\nautoplot_roc(roc_logit,  paste0(\"ROC ‚Äì Logit (AUC = \", round(auc(roc_logit), 3), \")\"))\n\n\n\n\n\n\n\nautoplot_roc(roc_probit, paste0(\"ROC ‚Äì Probit (AUC = \", round(auc(roc_probit), 3), \")\"))\n\n\n\n\n\n\n\n\n\n\n\n‚úÇÔ∏è Choosing a Threshold & Confusion Matrices\nClassification requires a threshold on predicted probabilities. The Youden J criterion often gives a better cut-off than the na√Øve 0.5.\n\n# Optimal thresholds by Youden's J\nthr_logit  &lt;- coords(roc_logit,  x = \"best\", best.method = \"youden\", transpose = TRUE)[\"threshold\"]\nthr_probit &lt;- coords(roc_probit, x = \"best\", best.method = \"youden\", transpose = TRUE)[\"threshold\"]\n\nthr_logit; thr_probit\n\nthreshold \n0.4980051 \n\n\nthreshold \n0.4479988 \n\n# Helper to compute a confusion matrix and basic metrics without extra packages\nconfusion_metrics &lt;- function(y_true, y_prob, thr = 0.5) {\n  y_pred &lt;- ifelse(y_prob &gt;= thr, 1L, 0L)\n  TP &lt;- sum(y_pred == 1 & y_true == 1, na.rm = TRUE)\n  TN &lt;- sum(y_pred == 0 & y_true == 0, na.rm = TRUE)\n  FP &lt;- sum(y_pred == 1 & y_true == 0, na.rm = TRUE)\n  FN &lt;- sum(y_pred == 0 & y_true == 1, na.rm = TRUE)\n  acc &lt;- (TP + TN) / (TP + TN + FP + FN)\n  tpr &lt;- TP / (TP + FN)  # sensitivity / recall\n  tnr &lt;- TN / (TN + FP)  # specificity\n  ppv &lt;- TP / (TP + FP)  # precision\n  npv &lt;- TN / (TN + FN)\n  list(\n    threshold = thr,\n    matrix = matrix(c(TN, FP, FN, TP), nrow = 2,\n                    dimnames = list(\"Predicted\" = c(\"0\",\"1\"), \"Actual\" = c(\"0\",\"1\"))),\n    accuracy = acc, sensitivity = tpr, specificity = tnr, precision = ppv, npv = npv\n  )\n}\n\n# Confusion matrices at 0.5 and at the optimal Youden threshold\ncm_logit_05 &lt;- confusion_metrics(df$high_price, df$p_logit,  thr = 0.5)\ncm_probit_05 &lt;- confusion_metrics(df$high_price, df$p_probit, thr = 0.5)\ncm_logit_opt &lt;- confusion_metrics(df$high_price, df$p_logit,  thr = as.numeric(thr_logit))\ncm_probit_opt &lt;- confusion_metrics(df$high_price, df$p_probit, thr = as.numeric(thr_probit))\n\ncm_logit_05\n\n$threshold\n[1] 0.5\n\n$matrix\n         Actual\nPredicted    0    1\n        0 1333  134\n        1  109 1342\n\n$accuracy\n[1] 0.9167238\n\n$sensitivity\n[1] 0.9092141\n\n$specificity\n[1] 0.9244105\n\n$precision\n[1] 0.9248794\n\n$npv\n[1] 0.9086571\n\ncm_probit_05\n\n$threshold\n[1] 0.5\n\n$matrix\n         Actual\nPredicted    0    1\n        0 1330  140\n        1  112 1336\n\n$accuracy\n[1] 0.9136395\n\n$sensitivity\n[1] 0.9051491\n\n$specificity\n[1] 0.9223301\n\n$precision\n[1] 0.9226519\n\n$npv\n[1] 0.9047619\n\ncm_logit_opt\n\n$threshold\n[1] 0.4980051\n\n$matrix\n         Actual\nPredicted    0    1\n        0 1333  132\n        1  109 1344\n\n$accuracy\n[1] 0.9174092\n\n$sensitivity\n[1] 0.9105691\n\n$specificity\n[1] 0.9244105\n\n$precision\n[1] 0.9249828\n\n$npv\n[1] 0.9098976\n\ncm_probit_opt\n\n$threshold\n[1] 0.4479988\n\n$matrix\n         Actual\nPredicted    0    1\n        0 1310  109\n        1  132 1367\n\n$accuracy\n[1] 0.9174092\n\n$sensitivity\n[1] 0.9261518\n\n$specificity\n[1] 0.9084605\n\n$precision\n[1] 0.9119413\n\n$npv\n[1] 0.9231853\n\n\n\n\n\nüìù What to Report (suggested structure)\n\nModel setup: how the binary target was defined; why the median.\nEstimation: logit & probit results; include robust SE tables.\nInterpretation: AMEs for key variables (e.g., ln_area, overall_qual, baths_total).\nPerformance: AUC for both models; comment on similarity/differences.\nClassification: confusion matrices at 0.5 and at the Youden-optimal threshold; discuss trade-offs.\nReflection: when would you prefer logit vs probit? (Hint: results are usually very similar; choice often driven by convention or interpretability.)\n\n\n\n\n‚úÖ Checklist\n\nCode runs from top to bottom without manual edits.\nAll variables in your RHS are present/constructed.\nYou report robust SEs and AMEs.\nYou include AUC and describe threshold choice.\nYour interpretations are in probability (percentage-point) terms.\n\n\nüèÅ End of Supervision 6\nüõë Remember to save your script üíæ",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Supervision 6</span>"
    ]
  },
  {
    "objectID": "formative_test_sup3.html",
    "href": "formative_test_sup3.html",
    "title": "In-Class Exercise Sup 3",
    "section": "",
    "text": "In-Class Formative Assessment: Variable Selection (Housing Prices)",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>In-Class Exercise Sup 3</span>"
    ]
  },
  {
    "objectID": "formative_test_sup3.html#in-class-formative-assessment-variable-selection-housing-prices",
    "href": "formative_test_sup3.html#in-class-formative-assessment-variable-selection-housing-prices",
    "title": "In-Class Exercise Sup 3",
    "section": "",
    "text": "‚è± Duration\nApprox. 40 minutes (in class)\n\n\n\nüéØ Objective\nApply the variable selection methods learned in Supervision 3 to a new dataset on housing prices. You will replicate the steps used previously with the Hitters dataset, but this time using the Housing Prices Dataset from Kaggle.\nYour task is to specify, estimate, and evaluate one or two regression models explaining house prices.\n\n\n\nüì¶ Dataset (Preloaded) or Download the housing dataset\nThis dataset has already been provided in your RStudio environment.\ninstall.packages(\"RKaggle\")\nlibrary(RKaggle)\n# Download and load dataset\nhouse_price_data &lt;- get_dataset(\"yasserh/housing-prices-dataset\")\nstr(house_price_data)\nDependent variable: price\nPredictors include: area, bedrooms, bathrooms, stories, mainroad, airconditioning, parking, furnishingstatus, etc.\n\n\n\nüß© Task\nYou have 40 minutes to complete the following:\n\nModel setup (5 min)\n\nInspect the dataset and identify relevant variables.\nChoose predictors that you expect to influence price.\n\nModel estimation (20 min)\n\nEstimate one OLS model explaining price.\nPerform stepwise variable selection (AIC or BIC).\n\nOptionally include one interaction term (e.g., area √ó stories).\n\nDiagnostics and reporting (15 min)\n\nReport the final regression equation.\nRecord Adjusted R¬≤, AIC/BIC, and RMSE (if possible).\n\nBriefly comment on which predictors are most influential and whether signs/magnitudes make sense.\n\n\n\n\n\nüßÆ Example Structure (you can adapt quickly)\nlibrary(MASS)\nlibrary(dplyr)\n\n# Example base model\ndf &lt;- house_price_data\nmodel0 &lt;- lm(price ~ area + bathrooms + stories + airconditioning + parking, data = df)\nsummary(model0)\n\n# Stepwise selection (AIC)\nstep_aic &lt;- stepAIC(model0, direction = \"both\", trace = FALSE)\nsummary(step_aic)\n\n# Optional: interaction example\nmodel_int &lt;- lm(price ~ area*stories + bathrooms + airconditioning, data = df)\nsummary(model_int)\n\n\n\nüßæ What to Submit (in class)\n\nFinal model equation\n\nAdjusted R¬≤, AIC/BIC, and any other metric you computed\n\nInterpretation of one key variable or interaction term\n\n1‚Äì2 sentences on whether the results make economic sense",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>In-Class Exercise Sup 3</span>"
    ]
  },
  {
    "objectID": "formative_test_sup3.html#submit-your-in-class-results",
    "href": "formative_test_sup3.html#submit-your-in-class-results",
    "title": "In-Class Exercise Sup 3",
    "section": "Submit your in-class results",
    "text": "Submit your in-class results\nIf the form doesn‚Äôt load below, use the backup link: Open the form in a new tab\n\n \n\n\n\nüí° Tip\n\nUse your Supervision 3 (Hitters) scripts as a reference.\n\nKeep your model parsimonious but explanatory.\n\nFocus on clarity and reasoning ‚Äî not the number of variables.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>In-Class Exercise Sup 3</span>"
    ]
  },
  {
    "objectID": "formative_test_sup4.html",
    "href": "formative_test_sup4.html",
    "title": "In-Class Exercise Sup 4",
    "section": "",
    "text": "Learning goals (40 min)",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>In-Class Exercise Sup 4</span>"
    ]
  },
  {
    "objectID": "formative_test_sup4.html#learning-goals-40-min",
    "href": "formative_test_sup4.html#learning-goals-40-min",
    "title": "In-Class Exercise Sup 4",
    "section": "",
    "text": "Fit a clear production model with meaningful regressors.\nDiagnose multicollinearity (VIF), autocorrelation (DW/BG & residual ACF), and heteroskedasticity (BP/White) and modify/refine the baseline model.\nApply robust/HAC standard errors for valid inference.\nCommunicate findings succinctly.\n\n\nMarking (formative)\n\nTask 1 (multicollinearity) ‚Äì 25 pts\n\nTask 2 (autocorrelation) ‚Äì 25 pts\n\nTask 3 (heteroskedasticity & robust SE) ‚Äì 25 pts\n\nTask 4 (report results: final model spec, economic analysis & evaluation) ‚Äì 25 pts\nTotal: 100 pts",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>In-Class Exercise Sup 4</span>"
    ]
  },
  {
    "objectID": "formative_test_sup4.html#setup",
    "href": "formative_test_sup4.html#setup",
    "title": "In-Class Exercise Sup 4",
    "section": "Setup",
    "text": "Setup\nData: The rice dataset contains a time series on rice production from Asian farms (1990-1997). Variables include: - prod: output (tons) - area: land (hectares) - fert: fertilizer (kg) - labor: labor hours (person-days)\n\n# Packages (minimal)\n# install the following packages if needed.\n\nlibrary(PoEdata); library(dplyr); library(car); library(lmtest); library(sandwich)\n\n\nLoad rice, order by year.\n\nDrop the firm identifier (firm).\n\nPreview the data.\n\n\n# Load the dataset 'rice' from the PoEdata package (must have library(PoEdata) loaded)\ndata(rice)\n\n# Sort the rows by the time variable so observations are in chronological order\nrice &lt;- rice[order(rice$year), ]\n\n# Drop the identifier column 'firm' (it's just an ID, not an economic variable)\nrice &lt;- subset(rice, select = -firm)\n\n# Look at the first 6 rows to get a quick sense of the data\nhead(rice, 6)\n\n   year  prod area labor  fert\n1  1990  7.87  2.5   160 207.5\n9  1990 10.35  3.8   184 303.5\n17 1990  9.98  3.4   170 252.0\n25 1990  4.83  1.4    68  88.0\n33 1990  8.74  3.6   130 149.8\n41 1990  1.84  0.5    34  21.0\n\n# List all column names so you know what variables are available\nnames(rice)\n\n[1] \"year\"  \"prod\"  \"area\"  \"labor\" \"fert\" \n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nWhy drop firm? It‚Äôs an identifier, not an economic input. Keep IDs only for estimations involving grouping/panels, not as regressors.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>In-Class Exercise Sup 4</span>"
    ]
  },
  {
    "objectID": "formative_test_sup4.html#task-1-multicollinearity-25-pts",
    "href": "formative_test_sup4.html#task-1-multicollinearity-25-pts",
    "title": "In-Class Exercise Sup 4",
    "section": "Task 1 ‚Äî Multicollinearity (25 pts)",
    "text": "Task 1 ‚Äî Multicollinearity (25 pts)\nFit the baseline model that you will need to modify/refine if needed: [ ]\n\nm &lt;- lm(prod ~ area + fert + labor, data = rice)\nsummary(m)\n\n\nCall:\nlm(formula = prod ~ area + fert + labor, data = rice)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2849 -0.9268  0.0351  0.9402  8.9061 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.138800   0.201714  -0.688 0.491845    \narea         1.321321   0.208748   6.330 7.57e-10 ***\nfert         0.004591   0.001289   3.562 0.000419 ***\nlabor        0.027505   0.003898   7.057 9.27e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.106 on 348 degrees of freedom\nMultiple R-squared:  0.8294,    Adjusted R-squared:  0.8279 \nF-statistic:   564 on 3 and 348 DF,  p-value: &lt; 2.2e-16\n\n\nCompute VIF and interpret.\n\nvif(m)\n\n    area     fert    labor \n7.266083 3.736222 7.064198 \n\n\nAnswer (brief, 150 words max):\n1. State the VIF values and threshold used 2. Identify which variables show problematic collinearity 3. Suggest ONE specific econometric solution appropriate for this production function",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>In-Class Exercise Sup 4</span>"
    ]
  },
  {
    "objectID": "formative_test_sup4.html#task-2-autocorrelation-25-pts",
    "href": "formative_test_sup4.html#task-2-autocorrelation-25-pts",
    "title": "In-Class Exercise Sup 4",
    "section": "Task 2 ‚Äî Autocorrelation (25 pts)",
    "text": "Task 2 ‚Äî Autocorrelation (25 pts)\nRun Durbin‚ÄìWatson and Breusch‚ÄìGodfrey (AR1) tests.\nThen plot the residual ACF to visualise the pattern. For example, the current model specification:\n\n# Durbin‚ÄìWatson test for first-order autocorrelation in OLS residuals\n# H0: no autocorrelation (rho = 0). DW ~ 2 means no autocorr; DW &lt; 2 = positive autocorr.\ndwtest(m)\n\n\n    Durbin-Watson test\n\ndata:  m\nDW = 1.7928, p-value = 0.02453\nalternative hypothesis: true autocorrelation is greater than 0\n\n# Breusch‚ÄìGodfrey test for serial correlation of order 1 (more general than DW)\n# H0: no serial correlation up to the specified order.\nbgtest(m, order = 1)\n\n\n    Breusch-Godfrey test for serial correlation of order up to 1\n\ndata:  m\nLM test = 3.711, df = 1, p-value = 0.05406\n\n# Extract OLS residuals from model m\nres_m &lt;- residuals(m)\n\n# Plot the autocorrelation function of residuals\n# Significant spike at lag 1 (outside blue bands) suggests AR(1)-type positive autocorrelation\nacf(res_m, main = \"ACF of OLS residuals (m)\")\n\n\n\n\n\n\n\n\nAnswer (brief, 150 words max):\n- Do tests indicate autocorrelation? Which sign? did you modify/refine the model? - What does the residual ACF suggest (e.g., AR(1)-type)? How would you address the issue for inference?\n\n\n\n\n\n\nTip\n\n\n\n\n\nHint: DW &lt; 2 ‚Üí positive serial correlation. A large spike at lag 1 in ACF is consistent with AR(1).",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>In-Class Exercise Sup 4</span>"
    ]
  },
  {
    "objectID": "formative_test_sup4.html#task-3-heteroskedasticity-robusthac-se-25-pts",
    "href": "formative_test_sup4.html#task-3-heteroskedasticity-robusthac-se-25-pts",
    "title": "In-Class Exercise Sup 4",
    "section": "Task 3 ‚Äî Heteroskedasticity & Robust/HAC SE (25 pts)",
    "text": "Task 3 ‚Äî Heteroskedasticity & Robust/HAC SE (25 pts)\n\nTest for heteroskedasticity: Breusch‚ÄìPagan and a White-style auxiliary test.\n\nReport HAC (Newey‚ÄìWest) standard errors for valid inference under both heteroskedasticity and autocorrelation.\n\n\n# --- Heteroskedasticity tests -----------------------------------------------\n\n# Breusch‚ÄìPagan test for heteroskedasticity\n# H0: constant variance (homoskedasticity). Small p-value =&gt; heteroskedasticity present.\nbptest(m)\n\n\n    studentized Breusch-Pagan test\n\ndata:  m\nBP = 81.47, df = 3, p-value &lt; 2.2e-16\n\n# \"White-style\" test: regress squared residuals on fitted and fitted^2 (more general form)\nfit &lt;- fitted(m)\nbptest(m, ~ fit + I(fit^2))  # small p-value =&gt; heteroskedasticity\n\n\n    studentized Breusch-Pagan test\n\ndata:  m\nBP = 73.769, df = 2, p-value &lt; 2.2e-16\n\n# --- Robust inference when heteroskedasticity and/or autocorrelation exist ---\n\n# Newey‚ÄìWest (HAC) robust standard errors for valid t-stats/p-values\n# Note: This adjusts SEs, not coefficients; residual patterns/tests remain the same.\ncoeftest(m, vcov. = NeweyWest(m, lag = 1, prewhite = FALSE))\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) -0.1388003  0.1754394 -0.7912    0.4294    \narea         1.3213211  0.3112151  4.2457 2.799e-05 ***\nfert         0.0045905  0.0024470  1.8760    0.0615 .  \nlabor        0.0275053  0.0059720  4.6057 5.774e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#**White test specification**: The `~ fit + I(fit^2)` is one \n# version; this is a simplified White test.\n\nAnswer (brief, 150 words max):\n- Are BP/White significant? Compare classical vs HAC p-values: which coefficients remain significant?\n\n\n\n\n\n\nTip\n\n\n\n\n\nNote: Robust/HAC SEs do not change the residuals or DW/BG results; they change inference (SEs/p-values).",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>In-Class Exercise Sup 4</span>"
    ]
  },
  {
    "objectID": "formative_test_sup4.html#task-4-report-results-final-model-spec-economic-analysis-evaluation-25-pts",
    "href": "formative_test_sup4.html#task-4-report-results-final-model-spec-economic-analysis-evaluation-25-pts",
    "title": "In-Class Exercise Sup 4",
    "section": "Task 4 ‚Äî report results: final model spec, economic analysis & evaluation (25 pts)",
    "text": "Task 4 ‚Äî report results: final model spec, economic analysis & evaluation (25 pts)\nSummarise (250 words max) (25 pts): 1. Your best/final model specification (equation) 2. The key diagnostic issues you found (which tests, what they showed) 3. The remedies you applied (e.g.¬†variable interactions, lags, HAC) 4. Why your final inference is valid for decision-making, results analysis and evaluation (brief economic interpretation).",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>In-Class Exercise Sup 4</span>"
    ]
  },
  {
    "objectID": "formative_test_sup4.html#submit-your-answers",
    "href": "formative_test_sup4.html#submit-your-answers",
    "title": "In-Class Exercise Sup 4",
    "section": "Submit your answers",
    "text": "Submit your answers\nPlease submit your completed answers using the embedded form below üëá If the form does not display correctly, you can open it directly using this link:\nüëâ Open Microsoft Form",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>In-Class Exercise Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup3.html",
    "href": "solution_formative_test_sup3.html",
    "title": "Solution to In-class Sup3",
    "section": "",
    "text": "Overview\nThis document reports the best model and key diagnostics based on an exhaustive subset selection using leaps::regsubsets. It also reproduces the R code used (with a few comment tweaks for clarity) and provides concise answers to:",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Solution to In-class Sup3</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup3.html#overview",
    "href": "solution_formative_test_sup3.html#overview",
    "title": "Solution to In-class Sup3",
    "section": "",
    "text": "Best model equation\n\nModel metrics (Adjusted R¬≤, BIC, RMSE)\n\nInterpretation of one key variable\n\nWhether the results make economic sense",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Solution to In-class Sup3</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup3.html#r-code-reproducible-workflow",
    "href": "solution_formative_test_sup3.html#r-code-reproducible-workflow",
    "title": "Solution to In-class Sup3",
    "section": "R Code (reproducible workflow)",
    "text": "R Code (reproducible workflow)\n\n\nCode\n# Load libraries\n# If needed: install.packages(c(\"ISLR\", \"leaps\", \"ggplot2\", \"dplyr\", \"RKaggle\"))\nlibrary(ISLR)\nlibrary(leaps)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(RKaggle)\n\n# Download and load dataset from Kaggle\nhouse_price_data &lt;- get_dataset(\"yasserh/housing-prices-dataset\")\n\n# Quick structure checks\nstr(house_price_data)\n\n\nspc_tbl_ [545 √ó 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ price           : num [1:545] 13300000 12250000 12250000 12215000 11410000 ...\n $ area            : num [1:545] 7420 8960 9960 7500 7420 7500 8580 16200 8100 5750 ...\n $ bedrooms        : num [1:545] 4 4 3 4 4 3 4 5 4 3 ...\n $ bathrooms       : num [1:545] 2 4 2 2 1 3 3 3 1 2 ...\n $ stories         : num [1:545] 3 4 2 2 2 1 4 2 2 4 ...\n $ mainroad        : chr [1:545] \"yes\" \"yes\" \"yes\" \"yes\" ...\n $ guestroom       : chr [1:545] \"no\" \"no\" \"no\" \"no\" ...\n $ basement        : chr [1:545] \"no\" \"no\" \"yes\" \"yes\" ...\n $ hotwaterheating : chr [1:545] \"no\" \"no\" \"no\" \"no\" ...\n $ airconditioning : chr [1:545] \"yes\" \"yes\" \"no\" \"yes\" ...\n $ parking         : num [1:545] 2 3 2 3 2 2 2 0 2 1 ...\n $ prefarea        : chr [1:545] \"yes\" \"no\" \"yes\" \"yes\" ...\n $ furnishingstatus: chr [1:545] \"furnished\" \"furnished\" \"semi-furnished\" \"furnished\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   price = col_double(),\n  ..   area = col_double(),\n  ..   bedrooms = col_double(),\n  ..   bathrooms = col_double(),\n  ..   stories = col_double(),\n  ..   mainroad = col_character(),\n  ..   guestroom = col_character(),\n  ..   basement = col_character(),\n  ..   hotwaterheating = col_character(),\n  ..   airconditioning = col_character(),\n  ..   parking = col_double(),\n  ..   prefarea = col_character(),\n  ..   furnishingstatus = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nCode\ndim(house_price_data)\n\n\n[1] 545  13\n\n\nCode\nsum(is.na(house_price_data$price))\n\n\n[1] 0\n\n\nCode\n# Brief data description\ncat(\"Response variable: price (1987 currency units).\\n\")\n\n\nResponse variable: price (1987 currency units).\n\n\nCode\ncat(\"Predictor variables:\\n\")\n\n\nPredictor variables:\n\n\nCode\nnames(house_price_data)[-13]\n\n\n [1] \"price\"           \"area\"            \"bedrooms\"        \"bathrooms\"      \n [5] \"stories\"         \"mainroad\"        \"guestroom\"       \"basement\"       \n [9] \"hotwaterheating\" \"airconditioning\" \"parking\"         \"prefarea\"       \n\n\nCode\n# Handle missing values (drop any rows with NAs)\nhouse_price_data &lt;- na.omit(house_price_data)\ndim(house_price_data)\n\n\n[1] 545  13\n\n\nCode\nsum(is.na(house_price_data))\n\n\n[1] 0\n\n\nCode\n# Ensure categorical variables are treated as factors (for lm consistency)\nchr_cols &lt;- sapply(house_price_data, is.character)\nhouse_price_data[chr_cols] &lt;- lapply(house_price_data[chr_cols], factor)\n\n# Exhaustive search over all subsets (up to 13 variables)\nregfit.full &lt;- regsubsets(price ~ ., data = house_price_data, nvmax = 13)\nreg.summary &lt;- summary(regfit.full)\n\n# Inspect what is available\nnames(reg.summary)\n\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n\nCode\nreg.summary$rsq[1:13]\n\n\n [1] 0.2872932 0.4650855 0.5446850 0.5815220 0.6149073 0.6365979 0.6520416\n [8] 0.6634325 0.6721634 0.6771562 0.6802180 0.6817071 0.6818018\n\n\nCode\nhead(reg.summary$which, 10)\n\n\n   (Intercept) area bedrooms bathrooms stories mainroadyes guestroomyes\n1         TRUE TRUE    FALSE     FALSE   FALSE       FALSE        FALSE\n2         TRUE TRUE    FALSE      TRUE   FALSE       FALSE        FALSE\n3         TRUE TRUE    FALSE      TRUE   FALSE       FALSE        FALSE\n4         TRUE TRUE    FALSE      TRUE    TRUE       FALSE        FALSE\n5         TRUE TRUE    FALSE      TRUE    TRUE       FALSE        FALSE\n6         TRUE TRUE    FALSE      TRUE    TRUE       FALSE        FALSE\n7         TRUE TRUE    FALSE      TRUE    TRUE       FALSE        FALSE\n8         TRUE TRUE    FALSE      TRUE    TRUE       FALSE        FALSE\n9         TRUE TRUE    FALSE      TRUE    TRUE       FALSE        FALSE\n10        TRUE TRUE    FALSE      TRUE    TRUE        TRUE        FALSE\n   basementyes hotwaterheatingyes airconditioningyes parking prefareayes\n1        FALSE              FALSE              FALSE   FALSE       FALSE\n2        FALSE              FALSE              FALSE   FALSE       FALSE\n3        FALSE              FALSE               TRUE   FALSE       FALSE\n4        FALSE              FALSE               TRUE   FALSE       FALSE\n5        FALSE              FALSE               TRUE   FALSE        TRUE\n6        FALSE              FALSE               TRUE    TRUE        TRUE\n7         TRUE              FALSE               TRUE    TRUE        TRUE\n8         TRUE              FALSE               TRUE    TRUE        TRUE\n9         TRUE               TRUE               TRUE    TRUE        TRUE\n10        TRUE               TRUE               TRUE    TRUE        TRUE\n   furnishingstatussemi-furnished furnishingstatusunfurnished\n1                           FALSE                       FALSE\n2                           FALSE                       FALSE\n3                           FALSE                       FALSE\n4                           FALSE                       FALSE\n5                           FALSE                       FALSE\n6                           FALSE                       FALSE\n7                           FALSE                       FALSE\n8                           FALSE                        TRUE\n9                           FALSE                        TRUE\n10                          FALSE                        TRUE\n\n\nCode\n# Selection plots\npar(mfrow = c(2,2))\nplot(reg.summary$rss, xlab = \"Number of Variables\", ylab = \"RSS\", type = \"l\")\ntitle(\"Training RSS (monotonically decreases)\")\n\nplot(reg.summary$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted R^2\", type = \"l\")\nbest.adjr2 &lt;- which.max(reg.summary$adjr2)\npoints(best.adjr2, reg.summary$adjr2[best.adjr2], col = \"red\", cex = 2, pch = 20)\ntitle(\"Adjusted R^2 (maximize)\")\n\nplot(reg.summary$cp, xlab = \"Number of Variables\", ylab = \"Cp\", type = \"l\")\nbest.cp &lt;- which.min(reg.summary$cp)\npoints(best.cp, reg.summary$cp[best.cp], col = \"red\", cex = 2, pch = 20)\ntitle(\"Mallows Cp (minimize)\")\n\nplot(reg.summary$bic, xlab = \"Number of Variables\", ylab = \"BIC\", type = \"l\")\nbest.bic &lt;- which.min(reg.summary$bic)\npoints(best.bic, reg.summary$bic[best.bic], col = \"red\", cex = 2, pch = 20)\ntitle(\"BIC (minimize)\")\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))\n\ncat(\"Optimal model sizes (by criterion):\\n\")\n\n\nOptimal model sizes (by criterion):\n\n\nCode\ncat(\"  Adjusted R^2:\", best.adjr2, \"variables\\n\")\n\n\n  Adjusted R^2: 12 variables\n\n\nCode\ncat(\"  Cp:          \", best.cp, \"variables\\n\")\n\n\n  Cp:           12 variables\n\n\nCode\ncat(\"  BIC:         \", best.bic, \"variables\\n\\n\")\n\n\n  BIC:          10 variables\n\n\nCode\ncat(\"Note: BIC typically favours more parsimonious models.\\n\")\n\n\nNote: BIC typically favours more parsimonious models.\n\n\nCode\n# Coefficients of the BIC-optimal model (size = best.bic)\nbic_coefs &lt;- coef(regfit.full, best.bic)\nbic_coefs\n\n\n                (Intercept)                        area \n                196289.1411                    252.1058 \n                  bathrooms                     stories \n               1034173.9440                 494356.1424 \n                mainroadyes                 basementyes \n                406815.3534                 459481.4981 \n         hotwaterheatingyes          airconditioningyes \n                859533.5760                 885120.3631 \n                    parking                 prefareayes \n                277731.5878                 667628.2774 \nfurnishingstatusunfurnished \n               -396086.1661 \n\n\nCode\n# For metrics, fit an lm() with the BIC-selected variables\nform_bic &lt;- price ~ area + bathrooms + stories + mainroad + basement + hotwaterheating + airconditioning + parking + prefarea + furnishingstatus\nmodel_bic &lt;- lm(form_bic, data = house_price_data)\n\n# Metrics\nadj_r2  &lt;- summary(model_bic)$adj.r.squared\nbic_val &lt;- BIC(model_bic)\nrmse    &lt;- sqrt(mean(residuals(model_bic)^2))\n\nlist(Adjusted_R2 = adj_r2, BIC = bic_val, RMSE = rmse)\n\n\n$Adjusted_R2\n[1] 0.6706188\n\n$BIC\n[1] 16752.61\n\n$RMSE\n[1] 1061594\n\n\nCode\n# Compact coefficient table\ncoef_table &lt;- data.frame(term = names(coef(model_bic)),\n                         estimate = as.numeric(coef(model_bic)))\ncoef_table\n\n\n                             term     estimate\n1                     (Intercept)  238328.2698\n2                            area     251.7261\n3                       bathrooms 1033284.6065\n4                         stories  493842.4982\n5                     mainroadyes  403925.7767\n6                     basementyes  459016.8636\n7              hotwaterheatingyes  861677.3128\n8              airconditioningyes  880534.6004\n9                         parking  277045.4515\n10                    prefareayes  665796.1037\n11 furnishingstatussemi-furnished  -52738.1140\n12    furnishingstatusunfurnished -430216.1525",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Solution to In-class Sup3</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup3.html#answers-concise",
    "href": "solution_formative_test_sup3.html#answers-concise",
    "title": "Solution to In-class Sup3",
    "section": "Answers (concise)",
    "text": "Answers (concise)\n\n1) Best model equation\nUsing BIC, the selected model is:\n[ ~ + + + + + + + + + ]\nCoefficient snapshot (from subset selection):\n(Intercept) area , 196289.1411 252.1058 , bathrooms stories , 1034173.9440 494356.1424 , mainroadyes basementyes , 406815.3534 459481.4981 , hotwaterheatingyes airconditioningyes , 859533.5760 885120.3631 , parking prefareayes , 277731.5878 667628.2774 , furnishingstatusunfurnished , -396086.1661\n\n\n\n2) Model metrics\nFrom the fitted BIC-selected model:\n\nAdjusted R¬≤: 0.671\n\nBIC: 1.67526^{4}\n\nRMSE: 1.061594^{6}\n\nNote: The maximum Adjusted R¬≤ across all sizes occurs around 12 variables (‚âà 0.675), but BIC favours the 10-variable model above for parsimony.\n\n\n\n3) Interpretation of a key variable\nConsider area. Holding other factors constant, the estimated coefficient on area is approximately 252.1. This implies that each additional unit of floor area increases the predicted price by about that amount, reflecting the strong and intuitive role of size in property valuation. The magnitude is economically plausible and aligns with the positive effects seen for amenities (e.g., airconditioning, prefarea).\n\n\n\n4) Economic sense?\nYes. Larger homes (area), additional bathrooms and stories, better amenities (air conditioning, hot-water heating), and desirable location (prefarea, access to mainroad) all increase predicted prices, while being unfurnished reduces price‚Äîprecisely what standard housing market theory would anticipate. The model therefore captures realistic drivers of housing values while balancing fit and simplicity via BIC.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Solution to In-class Sup3</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html",
    "href": "solution_formative_test_sup4.html",
    "title": "Solution to in‚Äëclass Sup 4",
    "section": "",
    "text": "Overview\nThis document reproduces the analysis with concise # comments inside each R chunk so students see why each step is taken.\nWork flow: run baseline model 1 &gt; VIF test &gt; fix model if high multicolinearity &gt; Test autocorrelation from previous step &gt; fix the model if high autocorrelation &gt; Test for heteroskedasticity &gt; fix the model if high heteroskedasticity.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Solution to in‚Äëclass Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html#packages",
    "href": "solution_formative_test_sup4.html#packages",
    "title": "Solution to in‚Äëclass Sup 4",
    "section": "0) Packages",
    "text": "0) Packages\n\n\nCode\n# Minimal package set for this workflow\npkgs &lt;- c(\"PoEdata\",\"car\",\"lmtest\",\"sandwich\",\"dplyr\",\"zoo\")\n\n# Install any missing packages (safe check)\nto_install &lt;- pkgs[!sapply(pkgs, requireNamespace, quietly = TRUE)]\nif (length(to_install)) install.packages(to_install, repos = \"https://cloud.r-project.org\")\n\n# Load libraries\nlibrary(PoEdata)     # provides 'rice' dataset\nlibrary(car)         # VIF for multicollinearity\nlibrary(lmtest)      # dwtest, bgtest, bptest\nlibrary(sandwich)    # robust variance estimators (HC)\nlibrary(dplyr)       # arrange, mutate, lag",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Solution to in‚Äëclass Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html#data",
    "href": "solution_formative_test_sup4.html#data",
    "title": "Solution to in‚Äëclass Sup 4",
    "section": "1) Data",
    "text": "1) Data\n\n\nCode\n# Load and time-order the data\ndata(rice)                                  # bring 'rice' into the workspace\nrice &lt;- rice[order(rice$year), ]            # ensure time order for lags and DW/BG tests\n\n# Baseline production function\nform &lt;- prod ~ area + fert + labor + firm   # 'firm' is an identifier; expect little role",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Solution to in‚Äëclass Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html#baseline-ols",
    "href": "solution_formative_test_sup4.html#baseline-ols",
    "title": "Solution to in‚Äëclass Sup 4",
    "section": "2) Baseline OLS",
    "text": "2) Baseline OLS\n\n\nCode\n# Estimate baseline OLS\nm &lt;- lm(form, data = rice)\n\n# Inspect fit and signs/magnitudes\ncat(\"\\n=== OLS summary ===\\n\"); print(summary(m))\n\n\n\n=== OLS summary ===\n\n\n\nCall:\nlm(formula = form, data = rice)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2718 -0.9266  0.0356  0.9459  8.9075 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.108138   0.318994  -0.339 0.734818    \narea         1.321087   0.209052   6.319 8.06e-10 ***\nfert         0.004583   0.001292   3.547 0.000442 ***\nlabor        0.027476   0.003911   7.026 1.13e-11 ***\nfirm        -0.001137   0.009158  -0.124 0.901232    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.109 on 347 degrees of freedom\nMultiple R-squared:  0.8294,    Adjusted R-squared:  0.8275 \nF-statistic: 421.8 on 4 and 347 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Solution to in‚Äëclass Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html#multicollinearity-vif",
    "href": "solution_formative_test_sup4.html#multicollinearity-vif",
    "title": "Solution to in‚Äëclass Sup 4",
    "section": "3) Multicollinearity (VIF)",
    "text": "3) Multicollinearity (VIF)\n\n\nCode\n# Diagnose collinearity: rule of thumb VIF &gt; 5 (moderate), &gt; 10 (high)\ncat(\"\\n=== Multicollinearity (VIF) ===\\n\"); print(vif(m))\n\n\n\n=== Multicollinearity (VIF) ===\n\n\n    area     fert    labor     firm \n7.266675 3.744241 7.090750 1.070505",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Solution to in‚Äëclass Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html#reduce-collinearity-via-transformation",
    "href": "solution_formative_test_sup4.html#reduce-collinearity-via-transformation",
    "title": "Solution to in‚Äëclass Sup 4",
    "section": "4) Reduce collinearity via transformation",
    "text": "4) Reduce collinearity via transformation\n\n\nCode\n# Construct labor per area to reduce collinearity between labor and area\nrice$labor_density &lt;- rice$labor / rice$area   # density transformation\n\n# Refit a more parsimonious model (drop 'firm' id)\nm2 &lt;- lm(prod ~ area + fert + labor_density, data = rice)\n\n# Check VIFs fall meaningfully\ncat(\"\\n=== VIF after transformation (m2) ===\\n\"); print(vif(m2))\n\n\n\n=== VIF after transformation (m2) ===\n\n\n         area          fert labor_density \n     3.739741      3.606256      1.098055 \n\n\nCode\n# Compare fit (R^2) and significance with baseline\ncat(\"\\n=== OLS summary (m2) ===\\n\"); print(summary(m2))\n\n\n\n=== OLS summary (m2) ===\n\n\n\nCall:\nlm(formula = prod ~ area + fert + labor_density, data = rice)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.7160 -0.9525  0.0347  1.0138  9.2159 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -1.794885   0.500123  -3.589  0.00038 ***\narea           2.581396   0.156507  16.494  &lt; 2e-16 ***\nfert           0.006033   0.001323   4.560 7.10e-06 ***\nlabor_density  0.031629   0.007850   4.029 6.87e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.201 on 348 degrees of freedom\nMultiple R-squared:  0.8137,    Adjusted R-squared:  0.8121 \nF-statistic: 506.6 on 3 and 348 DF,  p-value: &lt; 2.2e-16\n\n\nAnswer (VIF): Using a VIF threshold of 5 (NOte 5‚Äì10 considered moderate, &gt;10 high), the baseline model reports VIFs of area = 7.27, fert = 3.74, and labor = 7.06, indicating moderate multicollinearity for area and labor. The source is conceptual overlap between cultivated area and the scale of labor. Remedy: I replace labor with labor_density = labor/area to orthogonalise scale and intensity. In the revised model, VIFs fall to area = 3.74, fert = 3.61, labor_density = 1.10, which are comfortably below the threshold.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Solution to in‚Äëclass Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html#a-autocorrelation-diagnostics",
    "href": "solution_formative_test_sup4.html#a-autocorrelation-diagnostics",
    "title": "Solution to in‚Äëclass Sup 4",
    "section": "4a) Autocorrelation diagnostics",
    "text": "4a) Autocorrelation diagnostics\n\n\nCode\n# Durbin‚ÄìWatson (focus on AR(1) positive autocorrelation under default alt)\ncat(\"\\n=== Autocorrelation on m2 ===\\n\")\n\n\n\n=== Autocorrelation on m2 ===\n\n\nCode\ncat(\"Durbin‚ÄìWatson:\\n\"); print(dwtest(m2))\n\n\nDurbin‚ÄìWatson:\n\n\n\n    Durbin-Watson test\n\ndata:  m2\nDW = 1.7352, p-value = 0.005977\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nCode\n# Breusch‚ÄìGodfrey (general serial correlation; here AR(1) for comparability)\ncat(\"\\nBreusch‚ÄìGodfrey (AR1):\\n\"); print(bgtest(m2, order = 1))\n\n\n\nBreusch‚ÄìGodfrey (AR1):\n\n\n\n    Breusch-Godfrey test for serial correlation of order up to 1\n\ndata:  m2\nLM test = 6.1838, df = 1, p-value = 0.01289\n\n\nCode\n# Note: p &lt; 0.05 indicates evidence of autocorrelation\n\n\nAnswer (autocorrelation): The initial diagnostics on the revised level model show positive serial correlation: DW ‚âà 1.74 (p ‚âà 0.006) and BG(1) p ‚âà 0.013. The ACF/PACF of residuals suggest an AR(1) pattern with some evidence consistent with possible AR(2) dynamics. I therefore consider dynamic specifications and a log‚Äìlog transformation (which often stabilises variance and improves dynamics).\n\nCreate lag and plot correlograms\n\n\nCode\n# Create lag of dependent variable for dynamic model\nrice &lt;- rice |&gt;\n  arrange(year) |&gt;\n  mutate(prod_lag = dplyr::lag(prod))   # first observation becomes NA by construction\n\n# ACF/PACF of residuals (visual check)\nacf(m2$residuals, main = \"ACF of OLS residuals (m2)\")    # spikes suggest serial correlation\n\n\n\n\n\n\n\n\n\nCode\npacf(m2$residuals, main = \"PACF of OLS residuals (m2)\")\n\n\n\n\n\n\n\n\n\nCode\n# Align sample by dropping rows with NA lag\nrice_lag &lt;- na.omit(rice)               # keep complete cases for dynamic models",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Solution to in‚Äëclass Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html#b-dynamic-model-and-loglog-refinement",
    "href": "solution_formative_test_sup4.html#b-dynamic-model-and-loglog-refinement",
    "title": "Solution to in‚Äëclass Sup 4",
    "section": "4b) Dynamic model and log‚Äìlog refinement",
    "text": "4b) Dynamic model and log‚Äìlog refinement\n\n\nCode\n# Dynamic level model: include the lagged dependent variable\nm3 &lt;- lm(prod ~ prod_lag + area + fert + labor_density, data = rice_lag)\ncat(\"\\n=== Dynamic model in levels (m3) ===\\n\"); print(summary(m3))\n\n\n\n=== Dynamic model in levels (m3) ===\n\n\n\nCall:\nlm(formula = prod ~ prod_lag + area + fert + labor_density, data = rice_lag)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5972 -0.9054  0.0746  0.9975  9.1886 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -1.857053   0.504532  -3.681 0.000270 ***\nprod_lag       0.025986   0.024865   1.045 0.296725    \narea           2.536997   0.162464  15.616  &lt; 2e-16 ***\nfert           0.006149   0.001330   4.624 5.33e-06 ***\nlabor_density  0.030997   0.007892   3.928 0.000104 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.204 on 346 degrees of freedom\nMultiple R-squared:  0.8142,    Adjusted R-squared:  0.8121 \nF-statistic: 379.2 on 4 and 346 DF,  p-value: &lt; 2.2e-16\n\n\nCode\ncat(\"\\nDW on m3:\\n\"); print(dwtest(m3))\n\n\n\nDW on m3:\n\n\n\n    Durbin-Watson test\n\ndata:  m3\nDW = 1.7778, p-value = 0.01758\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nCode\ncat(\"\\nBG(AR1) on m3:\\n\"); print(bgtest(m3, order = 1))\n\n\n\nBG(AR1) on m3:\n\n\n\n    Breusch-Godfrey test for serial correlation of order up to 1\n\ndata:  m3\nLM test = 5.1696, df = 1, p-value = 0.02299\n\n\nCode\n# Log‚Äìlog specification: interpretable elasticities, often stabilizes variance\nbest &lt;- lm(log(prod) ~ log(prod_lag) + log(area) + log(fert) + log(labor_density),\n           data = rice_lag)\n\n# Multicollinearity check in final model\ncat(\"\\n=== VIF (final log‚Äìlog model) ===\\n\"); print(vif(best))\n\n\n\n=== VIF (final log‚Äìlog model) ===\n\n\n     log(prod_lag)          log(area)          log(fert) log(labor_density) \n          1.115383           4.698116           4.274338           1.221126 \n\n\nCode\n# Final model summary: coefficients are elasticities\ncat(\"\\n=== Final model (log‚Äìlog) summary ===\\n\"); print(summary(best))\n\n\n\n=== Final model (log‚Äìlog) summary ===\n\n\n\nCall:\nlm(formula = log(prod) ~ log(prod_lag) + log(area) + log(fert) + \n    log(labor_density), data = rice_lag)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.74604 -0.19002  0.05003  0.22904  1.40457 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        -1.47819    0.25611  -5.772 1.74e-08 ***\nlog(prod_lag)      -0.05305    0.02197  -2.414   0.0163 *  \nlog(area)           0.82324    0.04947  16.640  &lt; 2e-16 ***\nlog(fert)           0.20008    0.03826   5.230 2.94e-07 ***\nlog(labor_density)  0.44431    0.06676   6.655 1.11e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3396 on 346 degrees of freedom\nMultiple R-squared:  0.8502,    Adjusted R-squared:  0.8485 \nF-statistic: 491.1 on 4 and 346 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Autocorrelation checks on final model (expect no serial correlation)\ncat(\"\\nDW on final model:\\n\"); print(dwtest(best))\n\n\n\nDW on final model:\n\n\n\n    Durbin-Watson test\n\ndata:  best\nDW = 2.0162, p-value = 0.5545\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nCode\ncat(\"\\nBG(AR1) on final model:\\n\"); print(bgtest(best, order = 1))\n\n\n\nBG(AR1) on final model:\n\n\n\n    Breusch-Godfrey test for serial correlation of order up to 1\n\ndata:  best\nLM test = 0.048045, df = 1, p-value = 0.8265\n\n\nAnswer (autocorrelation ‚Äì final model): In the preferred log‚Äìlog model with log(prod_lag), both tests indicate no residual serial correlation: DW ‚âà 2.02 (p ‚âà 0.55) and BG(1) p ‚âà 0.83. This, together with acceptable VIFs (‚â§~4.7), supports using the log‚Äìlog model for inference and interpretation as elasticities.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Solution to in‚Äëclass Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html#heteroskedasticity-tests",
    "href": "solution_formative_test_sup4.html#heteroskedasticity-tests",
    "title": "Solution to in‚Äëclass Sup 4",
    "section": "5) Heteroskedasticity tests",
    "text": "5) Heteroskedasticity tests\n\n\nCode\n# Breusch‚ÄìPagan: H0 = homoskedasticity\ncat(\"\\n=== Heteroskedasticity ===\\n\")\n\n\n\n=== Heteroskedasticity ===\n\n\nCode\ncat(\"Breusch‚ÄìPagan:\\n\"); print(bptest(best))\n\n\nBreusch‚ÄìPagan:\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  best\nBP = 20.065, df = 4, p-value = 0.0004848\n\n\nCode\n# White-type (auxiliary regression on fitted and fitted^2)\ncat(\"\\nWhite-type (fitted & fitted^2):\\n\")\n\n\n\nWhite-type (fitted & fitted^2):\n\n\nCode\nprint(bptest(best, ~ fitted(best) + I(fitted(best)^2), data = rice_lag))\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  best\nBP = 24.196, df = 2, p-value = 5.572e-06",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Solution to in‚Äëclass Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html#robust-inference-hc1",
    "href": "solution_formative_test_sup4.html#robust-inference-hc1",
    "title": "Solution to in‚Äëclass Sup 4",
    "section": "6) Robust inference (HC1)",
    "text": "6) Robust inference (HC1)\n\n\nCode\n# Use heteroskedasticity-consistent (HC1) standard errors for inference\n# HAC is not needed because DW/BG suggest no serial correlation in the final model.\ncat(\"\\n=== Coefficients with robust (HC1) SE ===\\n\")\n\n\n\n=== Coefficients with robust (HC1) SE ===\n\n\nCode\nprint(lmtest::coeftest(best, vcov = sandwich::vcovHC(best, type = \"HC1\")))\n\n\n\nt test of coefficients:\n\n                    Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)        -1.478192   0.351465 -4.2058 3.317e-05 ***\nlog(prod_lag)      -0.053045   0.023119 -2.2944  0.022363 *  \nlog(area)           0.823241   0.079032 10.4166 &lt; 2.2e-16 ***\nlog(fert)           0.200076   0.066501  3.0086  0.002817 ** \nlog(labor_density)  0.444306   0.089841  4.9455 1.187e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\ncat(\"\\n=== Coefficients with HAC (Newey‚ÄìWest, lag = 1) ===\\n\")\n\n\n\n=== Coefficients with HAC (Newey‚ÄìWest, lag = 1) ===\n\n\nCode\nprint(lmtest::coeftest(best, vcov. = sandwich::NeweyWest(best, lag = 1, prewhite = FALSE)))\n\n\n\nt test of coefficients:\n\n                    Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)        -1.478192   0.331960 -4.4529 1.145e-05 ***\nlog(prod_lag)      -0.053045   0.024527 -2.1627  0.031249 *  \nlog(area)           0.823241   0.077409 10.6350 &lt; 2.2e-16 ***\nlog(fert)           0.200076   0.065352  3.0615  0.002375 ** \nlog(labor_density)  0.444306   0.085679  5.1857 3.670e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnswer (heteroskedasticity & inference): Both BP and White-type tests reject homoskedasticity, so I report HC1 and also HAC (Newey‚ÄìWest) inference. Comparing p‚Äëvalues across classical vs HC1 vs HAC, the significance pattern is robust: at the 5% level, log(area), log(fert), and log(labor_density) remain significant; log(prod_lag) also remains significant at ‚âà5% (small negative carryover). Hence, substantive conclusions are unchanged by robust/HAC corrections.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Solution to in‚Äëclass Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_test_sup4.html#short-interpretation-notes-for-students",
    "href": "solution_formative_test_sup4.html#short-interpretation-notes-for-students",
    "title": "Solution to in‚Äëclass Sup 4",
    "section": "Short interpretation notes (for students)",
    "text": "Short interpretation notes (for students)\n\nCollinearity: Replacing labor with labor_density reduced VIFs (improves stability of estimates).\nAutocorrelation: Final log‚Äìlog model (best) passes DW/BG (no serial correlation).\nHeteroskedasticity: BP/White indicate non-constant variance ‚Üí report robust SEs.\nElasticities: In the log‚Äìlog model, coefficients read as % changes (e.g., 0.82 on log(area) ‚âà 1% ‚Üë area ‚Üí 0.82% ‚Üë output, ceteris paribus).\n\nFinal report: The preferred specification is a log‚Äìlog production function with a dynamic term: \\(log(\\text{prod}_t) = \\alpha + \\beta1 \\log(\\text{prod}{t-1}) + \\beta_2 \\log(\\text{area}_t) + \\beta_3 \\log(\\text{fert}_t) +\\) \\(\\beta_4 \\log(\\text{labor/area}t) + u_t\\)\nDiagnostics follow textbook order. Multicollinearity: baseline VIFs were moderate (area ‚âà 7.27, labor ‚âà 7.06), so I replaced labor with labor density, reducing VIFs to ‚â§~3.74. Autocorrelation: the revised level model exhibited positive AR(1) serial correlation (DW ‚âà 1.74; BG p ‚âà 0.013; ACF consistent with AR(1) and possible AR(2)), which motivated adding a lagged dependent variable and moving to a log‚Äìlog form. In the final model, DW ‚âà 2.02 and BG(1) p ‚âà 0.83 indicate no remaining autocorrelation. Heteroskedasticity: both BP and White-type tests reject homoskedasticity, so I report HC1 and HAC (Newey‚ÄìWest) standard errors; inference is unchanged. Interpretation: Coefficients are elasticities. Output elasticity w.r.t. cultivated area is ‚âà0.82, fertiliser ‚âà0.20, and labor density ‚âà0.44: a 1% rise in each raises output by roughly 0.82%, 0.20%, and 0.44%, respectively. The small negative coefficient on (\\(\\log(\\text{prod}{t-1}\\))) suggests modest mean reversion after shocks. Overall, the transformed specification is well‚Äëbehaved and economically interpretable, with robust inference to heteroskedasticity and HAC corrections.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Solution to in‚Äëclass Sup 4</span>"
    ]
  },
  {
    "objectID": "solution_formative_task_sup5.html",
    "href": "solution_formative_task_sup5.html",
    "title": "Solution to in‚Äëclass Sup 5",
    "section": "",
    "text": "üéØ Purpose\nThis handout explains what diagnostics matter in hedonic pricing models, why we use HC1 robust standard errors, and why autocorrelation/HAC are not relevant for our cross-sectional housing dataset. It also gives you a clear workflow and short code snippets you can reuse.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Solution to in‚Äëclass Sup 5</span>"
    ]
  },
  {
    "objectID": "solution_formative_task_sup5.html#workflow-for-crosssectional-hedonic-models",
    "href": "solution_formative_task_sup5.html#workflow-for-crosssectional-hedonic-models",
    "title": "Solution to in‚Äëclass Sup 5",
    "section": "üß≠ Workflow for Cross‚ÄëSectional Hedonic Models",
    "text": "üß≠ Workflow for Cross‚ÄëSectional Hedonic Models\n\n\n  Import & explore the data (summary, histograms, missing values).\n  Select variables using economic reasoning (see Supervision 3).\n  Estimate the model with lm() (log-price is standard).\n  Diagnose assumptions:\n    \n      Multicollinearity ‚Üí VIF\n      Heteroskedasticity ‚Üí White test (Breusch‚ÄìPagan)\n      Optional (time/panel only): Autocorrelation (BG test)\n    \n  \n  Correct inference with HC1 robust SEs (cross‚Äësection default).\n  Visual checks: residuals vs fitted, normal Q‚ÄìQ.\n  Interpret coefficients and tell the economic story.\n\n\n\nKey takeaway: Hedonic models here are cross‚Äësectional. We use HC1 robust SEs. HAC/Newey‚ÄìWest and autocorrelation tests are for time series or panel data, not for this dataset.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Solution to in‚Äëclass Sup 5</span>"
    ]
  },
  {
    "objectID": "solution_formative_task_sup5.html#diagnostics-that-matter-crosssection",
    "href": "solution_formative_task_sup5.html#diagnostics-that-matter-crosssection",
    "title": "Solution to in‚Äëclass Sup 5",
    "section": "üîé Diagnostics That Matter (Cross‚ÄëSection)",
    "text": "üîé Diagnostics That Matter (Cross‚ÄëSection)\n\nA) Multicollinearity ‚Äî VIF\nMany house attributes are correlated (area, bedrooms, baths).\nRules of thumb: VIF &lt; 5 usually fine; 5‚Äì10 investigate; &gt;10 consider removing/re‚Äëcoding or combining variables.\n\n\nB) Heteroskedasticity ‚Äî White/Breusch‚ÄìPagan\nCross-sectional housing price data almost always violates homoskedasticity.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Solution to in‚Äëclass Sup 5</span>"
    ]
  },
  {
    "objectID": "solution_formative_task_sup5.html#correct-inference-hc1-robust-standard-errors",
    "href": "solution_formative_task_sup5.html#correct-inference-hc1-robust-standard-errors",
    "title": "Solution to in‚Äëclass Sup 5",
    "section": "üßÆ Correct Inference ‚Äî HC1 Robust Standard Errors",
    "text": "üßÆ Correct Inference ‚Äî HC1 Robust Standard Errors\nHC1 is consistent for cross-sectional data. HAC is consistent for time series.\n\nWe do not use HAC/Newey‚ÄìWest in this exercise because there is no time ordering in cross‚Äësectional data.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Solution to in‚Äëclass Sup 5</span>"
    ]
  },
  {
    "objectID": "solution_formative_task_sup5.html#visual-diagnostics-good-academic-practice",
    "href": "solution_formative_task_sup5.html#visual-diagnostics-good-academic-practice",
    "title": "Solution to in‚Äëclass Sup 5",
    "section": "üëÄ Visual Diagnostics (Good Academic Practice)",
    "text": "üëÄ Visual Diagnostics (Good Academic Practice)\npar(mfrow = c(1, 2))\nplot(m0, which = 1)  # Residuals vs Fitted (linearity & variance)\nplot(m0, which = 2)  # Normal Q‚ÄìQ (error distribution)\nInterpretation hints: - Left plot: no curve or funnel ‚Üí linearity & constant variance ok; patterns ‚Üí consider transforms or interactions. - Right plot: points near line ‚Üí residuals roughly normal; big tails ‚Üí consider log transforms or outlier checks.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Solution to in‚Äëclass Sup 5</span>"
    ]
  },
  {
    "objectID": "solution_formative_task_sup5.html#optional-variable-selection-link-to-supervision-3",
    "href": "solution_formative_task_sup5.html#optional-variable-selection-link-to-supervision-3",
    "title": "Solution to in‚Äëclass Sup 5",
    "section": "(Optional) Variable Selection (Link to Supervision 3)",
    "text": "(Optional) Variable Selection (Link to Supervision 3)\nUse only if multicollinearity is high or the model is unwieldy.\n\nAlways justify inclusion/exclusion with economic reasoning, not just AIC.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Solution to in‚Äëclass Sup 5</span>"
    ]
  },
  {
    "objectID": "solution_formative_task_sup5.html#optional-for-timepanel-data-autocorrelation",
    "href": "solution_formative_task_sup5.html#optional-for-timepanel-data-autocorrelation",
    "title": "Solution to in‚Äëclass Sup 5",
    "section": "(Optional, for Time/Panel Data) Autocorrelation",
    "text": "(Optional, for Time/Panel Data) Autocorrelation",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Solution to in‚Äëclass Sup 5</span>"
    ]
  },
  {
    "objectID": "solution_formative_task_sup5.html#not-used-for-this-crosssection.-shown-only-for-comparison-with-timeseries-workflows.",
    "href": "solution_formative_task_sup5.html#not-used-for-this-crosssection.-shown-only-for-comparison-with-timeseries-workflows.",
    "title": "Solution to in‚Äëclass Sup 5",
    "section": "Not used for this cross‚Äësection. Shown only for comparison with time‚Äëseries workflows.",
    "text": "Not used for this cross‚Äësection. Shown only for comparison with time‚Äëseries workflows.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Solution to in‚Äëclass Sup 5</span>"
    ]
  },
  {
    "objectID": "solution_formative_task_sup5.html#key-takeaways",
    "href": "solution_formative_task_sup5.html#key-takeaways",
    "title": "Solution to in‚Äëclass Sup 5",
    "section": "‚úÖ Key Takeaways",
    "text": "‚úÖ Key Takeaways\n\nThis dataset is cross‚Äësectional ‚Üí HC1 robust SEs are appropriate.\nFocus diagnostics on VIF and White test, plus residual visuals.\nAutocorrelation tests and HAC are for time/panel data and are not used here.\nCombine theory and evidence (Sup 3 + Sup 4) to justify your final model.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Solution to in‚Äëclass Sup 5</span>"
    ]
  }
]