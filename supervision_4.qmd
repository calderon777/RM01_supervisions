---
title: "Supervision 4"
format: html
---

## Lab 14 ‚Äî Collinearity


### üéØ Learning outcomes
- Understand what **collinearity (multicollinearity)** is and why it matters for regression inference.
- Recognise how collinearity inflates **standard errors**, weakening **t‚Äëtests** for individual coefficients.
- Diagnose collinearity using the **Variance Inflation Factor (VIF)** and interpret typical thresholds.
- Implement VIF diagnostics in R with `car::vif()` and report results clearly (tables, captions).
- Consider practical remedies (drop variables, transform/aggregate predictors, or rethink specification).

---

### üß∞ Prerequisites
- Multiple regression basics (OLS, $R^2$, F‚Äëtest, t‚Äëtests).
- R and RStudio installed.
- Packages: `PoEdata`, `broom`, `knitr`, `car`.

---

### Overview: why collinearity matters
Collinearity does **not bias** OLS, but it **inflates the variance** of estimated coefficients. As a result, t‚Äëtests lose power and coefficient signs may look unstable across specifications‚Äîeven when the model has a high $R^2$. Diagnostics such as **VIF** help decide whether variables overlap so much that individual effects are hard to interpret.

---

Collinearity (multicollinearity) among regressors occurs when two or more move closely together or have limited variability. It **inflates the variance** of estimated parameters (less precise t-tests), even though OLS estimates remain unbiased. As a result, a model may have a high $R^2$ or a large F‚Äëstatistic but **insignificant individual coefficients**.

### üß∞ Packages
`PoEdata`, `broom`, `knitr`, `car`

```{r}

# install.packages(c("PoEdata","broom","knitr","car")) ; install once if not already installed

library(PoEdata)  # example datasets used in the course
library(broom)    # tidier model outputs (tidy/glance/augment)
library(knitr)    # neat tables/printing (e.g., kable)
library(car)      # regression diagnostics (e.g., VIF)


# Example data: cars (mpg = miles per gallon; cyl = cylinders; disp = engine displacement; wt = weight)
data(mtcars)

# Simple model
# fit a linear regression: mpg explained by cylinders
mod1 <- lm(mpg ~ cyl, data = mtcars)
# convert model output to a neat table (broom::tidy + knitr::kable)
kable(tidy(mod1), caption = "A simple linear 'mpg' model")
```

Now add more regressors:

```{r}
# fit a multiple regression with 3 predictors (cyl, disp, wt)
mod2 <- lm(mpg ~ cyl + disp + wt, data = mtcars)
# format the model output into a clean table
kable(tidy(mod2), caption = "Multivariate 'mpg' model")
```

::: callout-note
When adding **eng** and **wgt**, the coefficient for **cyl** can turn **insignificant**‚Äîa classic sign of collinearity because these vehicle traits tend to move together.
:::

### Variance Inflation Factor (VIF)

The VIF for regressor $x_k$ is $VIF_k = \dfrac{1}{1-R_k^2}$, where $R_k^2$ is from regressing $x_k$ on the other regressors.

```{r}
v <- car::vif(mod2)                           # calculate VIF values to diagnose multicollinearity
tab <- data.frame(regressor = names(v),       # create a small table with regressor names
                  VIF = as.numeric(v),        # convert VIF values to numeric for display
                  row.names = NULL)           # avoid row names in the table
kable(tab, caption = "Variance inflation factors for the 'mpg' regression model.")  # nicely format output
```

::: callout-tip
**Rules of thumb**  
- **VIF > 10** ‚Üí strong collinearity (commonly used cutoff)  
- **VIF > 5** ‚Üí moderate collinearity (early warning)
These cut-offs are only heuristics. Evaluate VIFs alongside theory, sample size, and model purpose; high VIFs don‚Äôt automatically require dropping variables (O'Brien, 2007).
:::

### Mitigations (when VIFs are high)
- Remove or combine collinear variables (e.g., combine **cyl** and **eng** into a power index).  
- Use **PCA** to produce orthogonal components.  
- **Standardise/centre** variables (helps especially with interactions/polynomials (it won‚Äôt ‚Äúfix‚Äù genuine overlap among distinct variables).  
- Use **Ridge regression** if retaining all variables is preferred.  
- Consider **transformations** (e.g., logs).

---


### Summary
- Collinearity leaves OLS **unbiased** but makes estimates **imprecise**.
- Always check **VIFs** when several predictors are conceptually related.
- If VIFs are high, consider **dropping**, **combining**, or **re‚Äëspecifying** predictors‚Äîand justify choices with theory.


**References**
O'Brien, Robert. (2007). A Caution Regarding Rules of Thumb for Variance Inflation Factors. Quality & Quantity. 41. 673-690. 10.1007/s11135-006-9018-6. 

---


## Lab 15 ‚Äî Heteroskedasticity

### üéØ Learning outcomes
- Understand the key concept(s) introduced in this lab and their purpose in empirical analysis.
- Correctly implement the core R workflow for this topic and present clean, well-captioned results.
- Interpret model output in context and link diagnostics to modeling choices.
- Recognise common pitfalls and apply simple robustness checks.

---

### üß∞ Prerequisites
- Prior exposure to multiple regression and basic inference (t-tests, F-tests).
- R and RStudio installed.
- Packages: `PoEdata`, `broom`, `knitr`, plus any additional packages used in this lab.

---

### Overview
This lab introduces the core idea, why it matters for inference and decision-making, and where it fits in the broader modeling workflow. You will practise the implementation in R, interpret outputs carefully, and connect diagnostics back to modeling decisions.

---


The Gauss‚ÄìMarkov theorem assumes **homoskedasticity** (constant error variance $\sigma^2$). In many economic datasets, variance grows with some regressors (e.g., higher‚Äëincome households show more dispersion in expenditure). With heteroskedasticity, OLS coefficients remain unbiased, but **standard errors** and therefore, with heteroskedasticity, OLS coefficients remain unbiased (under exogeneity) but the usual SEs are inconsistent‚Äîuse HC (White) or other robust SEs. For completeness, apart from homoskedasticity, Gauss‚ÄìMarkov (for OLS to be BLUE) also requires linearity in parameters, no perfect collinearity, and exogeneity. Independence/no autocorrelation is also standard in time-series contexts.

### üß∞ Packages
`lmtest`, `broom`, `PoEdata`, `car`, `sandwich`, `knitr`

```{r}
# install.packages(c("lmtest","broom","PoEdata","car","sandwich","knitr"))  # install once if needed

library(lmtest)    # tests for regression models (e.g., Breusch-Pagan)
library(broom)     # tidy/glance/augment model outputs
library(PoEdata)   # course datasets
library(car)       # diagnostics (e.g., VIF)
library(sandwich)  # robust (heteroskedasticity-consistent) SEs
library(knitr)     # neat tables (kable)

data("food", package = "PoEdata")            # load the 'food' dataset
mod1 <- lm(food_exp ~ income, data = food)   # simple OLS: food expenditure explained by income

# Scatter and fitted line
plot(food$income, food$food_exp,             # scatterplot of income vs. food expenditure
     xlab = "income", ylab = "food expenditure",
     pch = 19, col = "grey")                 # filled points, grey color
abline(mod1, lwd = 2)                        # add fitted regression line with thicker stroke
```
---

### Spotting heteroskedasticity with residual plots

Residual plots help detect heteroskedasticity visually. Under homoskedasticity, residuals should scatter randomly around zero with constant spread. Look for:

- Fanning patterns: residuals spread out (or narrow) as x increases
- Clusters or groups: different variance across subpopulations
- Non-random patterns: suggests model misspecification

```{r}
res  <- residuals(mod1)          # residuals (observed - fitted)
yhat <- fitted(mod1)             # fitted (predicted) values from the model

# Residuals vs. income (check for nonlinearity or changing spread with income)
plot(food$income, res,
     xlab = "income", ylab = "residuals",
     pch = 19, col = "grey")
abline(h = 0, lty = 2)           # reference line at zero residual

# Residuals vs. fitted values (diagnose heteroskedasticity/patterns)
plot(yhat, res,
     xlab = "fitted values", ylab = "residuals",
     pch = 19, col = "grey")
abline(h = 0, lty = 2)           # reference line at zero residual

```

### Breusch‚ÄìPagan test (manual construction)
The Breusch‚ÄìPagan test is a formal test for heteroskedasticity. The null hypothesis is $H_0:$ $\text{homoskedasticity (constant variance)}$.

Test statistic: $\chi^2 = N \cdot R^2_{\text{aux}}$ <br>

where $R^2_{\text{aux}}$ comes from regressing **squared residuals** on the original regressors:

$\hat{e}_i^{\,2}
= \gamma_0 + \gamma_1 x_{i1} + \gamma_2 x_{i2} + \cdots + \gamma_k x_{ik} + \text{error}$ <br>

Under $H_0$,  $\chi^2 \sim \chi^2_k$ (k = number of regressors, excluding intercept).

```{r}
alpha <- 0.05                              # significance level (5%)

ressq <- resid(mod1)^2                     # squared residuals (used to detect heteroskedasticity)

modres <- lm(ressq ~ income, data = food)  # auxiliary regression: regress squared residuals on income

N <- nobs(modres)                           # number of observations
S <- summary(modres)$df[1]-1                  # degrees of freedom for regression (k = number of predictors)

chisqcr <- qchisq(1 - alpha, S)             # chi-square critical value (right tail)

Rsqres <- summary(modres)$r.squared         # R¬≤ from auxiliary regression
chisq  <- N * Rsqres                        # Breusch‚ÄìPagan test statistic
pval   <- 1 - pchisq(chisq, S)              # p-value (probability of observing that statistic or larger)

c(statistic = chisq, crit = chisqcr, p.value = pval)   # show results: test statistic, critical value, p-value
```

::: callout-note
## Interpreting the Breusch‚ÄìPagan test

**Decision rule:**
- If **p-value < 0.05** (or your chosen $\alpha$): reject $H_0$ ‚Üí evidence against homoskedasticity (i.e., evidence of heteroskedasticity)
- If **p-value ‚â• 0.05**: fail to reject $H_0$ ‚Üí no strong evidence of heteroskedasticity

Alternatively, compare the test statistic to the critical value:
- If **$\chi^2$ > critical value**: reject $H_0$
- If **$\chi^2$ ‚â§ critical value**: fail to reject $H_0$

In this example, if p-value is small (< 0.05), we have evidence that variance increases with income.
:::


### White (quadratic) version

White's test extends Breusch‚ÄìPagan by including squares and cross-products of regressors in the auxiliary regression. This detects more general forms of heteroskedasticity (not just linear).
For a model with one regressor $x$:

$\hat{e}_i^{\,2} = \gamma_0 + \gamma_1 x_{i1} + \gamma_2 x^2_{i2} + \text{error}$

Test statistic: $\chi^2 = N \cdot R^2_{\text{aux}}$ ‚àº $\chi^2_k$, where $k$ is the number of regressors in the auxiliary regression (excluding the intercept).

```{r}
# auxiliary regression with quadratic term (tests nonlinear heteroskedasticity)
modres2 <- lm(ressq ~ income + I(income^2), data = food)    
Rsq2 <- summary(modres2)$r.squared          # R¬≤ from auxiliary regression
S2   <- summary(modres2)$df[1]-1              # degrees of freedom (number of regressors)
chisq2 <- N * Rsq2                          # Breusch‚ÄìPagan-type chi-square statistic
pval2  <- 1 - pchisq(chisq2, S2)            # p-value (right-tail)
c(statistic = chisq2, df = S2, p.value = pval2)            # show results
```

::: callout-note
## Interpreting White's test

**Decision rule:**
- If **p-value < 0.05**: reject $H_0$ ‚Üí evidence of heteroskedasticity (possibly nonlinear)
- If **p-value ‚â• 0.05**: fail to reject $H_0$ ‚Üí no strong evidence

White's test is more general than Breusch‚ÄìPagan (detects nonlinear heteroskedasticity) but uses more degrees of freedom. In small samples, use Breusch‚ÄìPagan if you suspect linear heteroskedasticity, or White if you suspect more complex patterns.
:::

### Breusch‚ÄìPagan with `bptest()`

```{r}
# run Breusch‚ÄìPagan test and tidy the output
# display results as a formatted table
kable(tidy(bptest(mod1)), caption = "Breusch‚ÄìPagan heteroskedasticity test")   
```

::: callout-note
## Interpreting `bptest()` output

The `bptest()` function reports:
- **statistic**: the BP test statistic (approximately $\chi^2$ distributed)
- **p.value**: probability of observing this statistic under $H_0$
- **parameter**: degrees of freedom

**Decision:** If p-value < 0.05, reject homoskedasticity and consider using robust standard errors.
:::

### Goldfeld‚ÄìQuandt test (indicator split: metro vs rural)

Tests whether error variance differs between two subgroups. Split the sample by a grouping variable or by ordering observations (e.g., by income level).

**Test statistic:**  
$F = \dfrac{\hat{\sigma}_1^{\,2}}{\hat{\sigma}_2^{\,2}}$ <br>

where $\hat{\sigma}_1^{\,2}$ and $\hat{\sigma}_2^{\,2}$ are estimated error variances from each subsample.

Under $H_0$ (equal variances),  
$F \sim F_{df_1, df_2}$.

```{r}
alpha <- 0.05                           # significance level (5%)

data("cps2", package = "PoEdata")       # load CPS dataset
m <- cps2[cps2$metro == 1, ]            # subset: people living in metropolitan areas
r <- cps2[cps2$metro == 0, ]            # subset: people NOT living in metropolitan areas

wg1 <- lm(wage ~ educ + exper, data = m)           # regression for metro group
wg0 <- lm(wage ~ educ + exper, data = r)           # regression for non-metro group

df1 <- wg1$df.residual                   # residual degrees of freedom (metro model)
df0 <- wg0$df.residual                   # residual degrees of freedom (non-metro model)

sig1 <- summary(wg1)$sigma^2             # estimated error variance metro
sig0 <- summary(wg0)$sigma^2             # estimated error variance non-metro

fstat <- sig1 / sig0                     # F-statistic: ratio of variances

Flc <- qf(alpha/2, df1, df0)             # lower critical value from F distribution
Fuc <- qf(1 - alpha/2, df1, df0)         # upper critical value (two-tail test)

c(F = fstat, Flc = Flc, Fuc = Fuc)       # display F statistic and critical bounds
```

::: callout-note
## Interpreting the Goldfeld‚ÄìQuandt test

**Decision rule (two-tailed test):**
- If $F <$ lower critical value OR $F >$ upper critical value: reject $H_0$ ‚Üí variances differ
- If $F$ lies between the two critical values: fail to reject $H_0$

**Decision rule (one-tailed test):**
- If $F >$ upper critical value (or p-value < 0.05): reject $H_0$ ‚Üí variance in group 1 is larger

In this metro vs. rural example, a significant F-statistic suggests systematic differences in wage variance between regions.
:::

Goldfeld‚ÄìQuandt without an indicator (split by median income):

```{r}
alpha <- 0.05                                   # significance level (5%)

medianincome <- median(food$income)             # compute median income

# Split sample into low-income and high-income groups
li <- food[food$income <= medianincome, ]       # low-income group (<= median)
hi <- food[food$income >= medianincome, ]       # high-income group (>= median)

# Fit the same regression separately in each subgroup
eqli <- lm(food_exp ~ income, data = li)        # model for low-income group
eqhi <- lm(food_exp ~ income, data = hi)        # model for high-income group

# Extract degrees of freedom and residual variances
dfli <- eqli$df.residual; dfhi <- eqhi$df.residual   # residual degrees of freedom
sqli <- summary(eqli)$sigma^2; sqhi <- summary(eqhi)$sigma^2  # estimated error variances

# F test comparing variances (heteroskedasticity between groups)
fstat <- sqhi / sqli                            # F statistic: ratio of variances
Fc    <- qf(1 - alpha, dfhi, dfli)              # critical value (right-tail test)
pval  <- 1 - pf(fstat, dfhi, dfli)              # p-value

c(F = fstat, Fcrit = Fc, p.value = pval)        # display test statistic, critical value, and p-value
```

::: callout-note
## Interpreting the GQ test (one-tailed)

**Decision rule:**
- If **p-value < 0.05**: reject $H_0$ ‚Üí variance increases with income
- If **F > critical value**: reject $H_0$

This version tests whether high-income households have **more variable** food expenditure than low-income households.
:::


Or use `gqtest()` directly:

```{r}
foodeq <- lm(food_exp ~ income, data = food)          # baseline OLS

# Goldfeld‚ÄìQuandt test for heteroskedasticity:
# - order.by = food$income: sort observations by income
# - point = 0.5: split the sample around the middle (drops central portion)
# - alternative = "greater": tests whether variance is larger in the upper group
tst <- lmtest::gqtest(foodeq,
                      point = 0.5,
                      alternative = "greater",
                      order.by = food$income)
# tidy + nicely format test output
kable(tidy(tst),                              
      caption = "R function `gqtest()` with the 'food' equation")
```

::: callout-note
## Interpreting `gqtest()` output

The function reports:
- **statistic**: F-ratio comparing variances
- **p.value**: one-tailed (tests "greater" alternative by default)
- **parameter**: degrees of freedom for numerator and denominator

**Decision:** If p-value < 0.05, conclude that variance is significantly larger in the upper group (heteroskedasticity present).

**Note:** `point = 0.5` means observations are split around the median. Adjust this parameter if you want to drop more/fewer central observations.
:::

### Heteroskedasticity‚Äëconsistent (HC) standard errors

Heteroskedasticity-consistent (robust) standard errors
When heteroskedasticity is detected, one solution is to use robust standard errors (also called White standard errors or HC standard errors). These adjust the variance-covariance matrix to account for non-constant variance without changing the coefficient estimates.

Key types:

HC0: White's original estimator
HC1: degrees-of-freedom adjustment (recommended for small samples)
HC2, HC3: further refinements for leverage and influence

The OLS point estimates remain unchanged‚Äîonly the standard errors (and thus t-statistics and p-values) are corrected.

```{r}
foodeq <- lm(food_exp ~ income, data = food)          # baseline regression
kable(tidy(foodeq),                                   # show coefficients with regular SEs
      caption = "Regular standard errors in the 'food' equation")

cov1 <- car::hccm(foodeq, type = "hc1")               # compute robust (HC1) covariance matrix
food.HC1 <- lmtest::coeftest(foodeq, vcov. = cov1)    # apply robust SEs to regression output

kable(tidy(food.HC1),                                 # show coefficients with robust SEs
      caption = "Robust (HC1) standard errors in the 'food' equation")
```

::: callout-important
Robust (HC) errors **fix inference** under heteroskedasticity, but the **point estimates are unchanged**.
:::

---

### Summary
- You applied the technique in R, produced tidy outputs, and interpreted them in context.
- Keep focusing on clean tables, explicit assumptions, and diagnostics that justify your specification.
- Use the provided patterns as a template for future empirical work.



---

## Lab 16 ‚Äî Serial Correlation (Autocorrelation)

### üéØ Learning outcomes
- Understand the core ideas and assumptions for this lab's topic.
- Implement the workflow in R and create tidy outputs with clear captions.
- Interpret results in context and connect diagnostics to modeling decisions.
- Recognise common pitfalls and perform basic robustness checks.

---

### üß∞ Prerequisites
- Multiple regression and inference (t-tests, F-tests).
- R and RStudio installed.
- Packages: `broom`, `knitr`, and any other packages used in the lab code.

---

### Overview
This lab situates the technique within the broader econometric workflow, explaining why it matters for inference and decision‚Äëmaking. You will implement it in R, assess assumptions/diagnostics, and justify specification choices.

---


Serial correlation is correlation across time in a series. In regression, **autocorrelated errors** lead to **incorrect standard errors** (and misleading \(t\)-tests).

### Example: growth vs unemployment (Okun‚Äôs data)

```{r}
data("okun", package = "PoEdata")             # load Okun's law quarterly dataset
okun.ts <- ts(okun, start = c(1948, 1), frequency = 4)  # convert to quarterly time-series object

# Plot time series
plot(okun.ts[, "g"], ylab = "growth")         # real GDP growth over time
plot(okun.ts[, "u"], ylab = "unemployment")   # unemployment rate over time

# Scatter with lags/leads
ggL1 <- data.frame(
  g   = okun.ts[, "g"],
  gL1 = stats::lag(okun.ts[, "g"], -1)        # lead by 1 quarter (stats::lag with -1 shifts forward)
)
plot(ggL1)                                    # scatter of g vs. g(+1)
abline(h = mean(ggL1$gL1, na.rm = TRUE),      # horizontal line at mean of g(+1)
       v = mean(ggL1$g,   na.rm = TRUE), lty = 2)       # vertical line at mean of g

ggL2 <- data.frame(
  g   = okun.ts[, "g"],
  gL2 = stats::lag(okun.ts[, "g"], -2)        # lead by 2 quarters
)
plot(ggL2)                                    # scatter of g vs. g(+2)
abline(h = mean(ggL2$gL2, na.rm = TRUE),
       v = mean(ggL2$g,   na.rm = TRUE), lty = 2)

# Correlogram (autocorrelation function) for growth
acf(okun.ts[, "g"])
```

### Phillips curve example and BG/DW tests

```{r}
library(dynlm)                                 # dynamic linear models for time series

data("phillips_aus", package = "PoEdata")      # Australian Phillips-curve data
phill.ts <- ts(phillips_aus, start = c(1987, 1), end = c(2009, 3), frequency = 4)  # quarterly ts
inflation <- phill.ts[, "inf"]                 # inflation series
Du <- diff(phill.ts[, "u"])                    # change in unemployment (‚àÜu_t)

# FDL model: inf_t = Œ≤1 + Œ≤2 * ‚àÜu_t + e_t
phill.dyn <- dynlm(inf ~ diff(u), data = phill.ts)                 # regress inflation on ‚àÜunemployment
kable(tidy(phill.dyn), caption = "Summary of the `phillips` model")# tidy+format regression output

# Residual plot and correlogram (check for serial correlation)
ehat <- resid(phill.dyn)                       # regression residuals
plot(ehat); abline(h = 0, lty = 2)             # residuals over time, zero reference
acf(ehat)                                      # ACF of residuals
```

Breusch‚ÄìGodfrey tests (different orders/statistics):

```{r}
# NOTE: preserving your object names a, b, c, d and test types.
# (We only use base::c() below to avoid masking by object 'c')
# type = "F" can be preferable in small samples (finite-sample adjustment)

a <- lmtest::bgtest(phill.dyn, order = 1, type = "F",     fill = 0)   # BG test AR(1); replace initial NAs by 0
b <- lmtest::bgtest(phill.dyn, order = 1, type = "F",     fill = NA)  # BG test AR(1); drop obs with initial NAs
c <- lmtest::bgtest(phill.dyn, order = 4, type = "Chisq", fill = 0)   # BG test AR(4); large-sample chi-square; fill 0
d <- lmtest::bgtest(phill.dyn, order = 4, type = "Chisq", fill = NA)  # BG test AR(4); drop initial NAs

# Collect results into a neat table
dfr <- data.frame(
  Method     = base::c("1, F, 0", "1, F, NA", "4, Chisq, 0", "4, Chisq, NA"),
  Statistic  = base::c(unname(a$statistic), unname(b$statistic),
                       unname(c$statistic), unname(d$statistic)),      # test statistic values
  Parameters = base::c(sprintf("df1=%g, df2=%g", a$parameter[1], a$parameter[2]),
                       sprintf("df1=%g, df2=%g", b$parameter[1], b$parameter[2]),
                       sprintf("df=%g",          c$parameter),          # chi-square df
                       sprintf("df=%g",          d$parameter)),
  p_value    = base::c(a$p.value, b$p.value, c$p.value, d$p.value)     # p-values
)

knitr::kable(dfr, caption = "Breusch‚ÄìGodfrey test for the Phillips example.")  # nicely formatted summary

```

Durbin‚ÄìWatson (DW targets AR(1) and isn‚Äôt valid with a lagged dependent variable(prefer Breusch‚ÄìGodfrey for more general cases):

```{r}
lmtest::dwtest(phill.dyn)   # Durbin‚ÄìWatson test for AR(1) autocorrelation in residuals (H0: no autocorrelation)
```

---


### Summary
- You implemented the method in R and produced clean, well‚Äëcaptioned outputs.
- Diagnostics and assumptions inform how you specify and interpret the model.
- Use these patterns as a template for future empirical work.


---

## Lab 17 ‚Äî HAC (Newey‚ÄìWest) Standard Errors

### üéØ Learning outcomes
- Grasp the core concept(s) of this lab and where they fit in the modelling workflow.
- Implement the R workflow cleanly, producing tidy outputs with clear captions.
- Interpret results carefully and connect diagnostics/assumptions to modelling choices.
- Identify pitfalls and perform simple robustness checks.

---

### üß∞ Prerequisites
- Multiple regression, inference (t-tests, F-tests), and model diagnostics basics.
- R and RStudio installed.
- Packages: `broom`, `knitr`, plus any additional packages used in this lab.

---

### Overview
This lab introduces the concept, explains why it matters for inference and decision‚Äëmaking, and demonstrates implementation in R. You will interpret results in context and motivate specification choices using diagnostics.

---


Correct standard errors under autocorrelation (and heteroskedasticity) using **HAC** estimators.

```{r}
library(sandwich); library(lmtest)                        # robust VCOVs + testing tools

s0 <- coeftest(phill.dyn)                                 # OLS SEs (incorrect if residuals have autocorrelation)
s1 <- coeftest(phill.dyn, vcov. = vcovHAC(phill.dyn))     # HAC (heteroskedasticity & autocorrelation consistent)
s2 <- coeftest(phill.dyn, vcov. = NeweyWest(phill.dyn))   # Newey‚ÄìWest (a common HAC estimator)
s3 <- coeftest(phill.dyn, vcov. = kernHAC(phill.dyn))     # kernel-based HAC (Andrews-type)

# Collect just the (Std. Error, Pr(>|t|)) rows/cols and compare across methods
tbl <- data.frame(cbind(s0[c(3, 4)], s1[c(3, 4)], s2[c(3, 4)], s3[c(3, 4)]))
names(tbl) <- c("Incorrect", "vcovHAC", "NeweyWest", "kernHAC")
row.names(tbl) <- c("(Intercept)", "Du")

kable(tbl, digits = 3, caption = "Comparing standard errors for the Phillips model.")
```

::: callout-warning
HAC fixes **inference** but not **efficiency**: OLS with HAC is still not minimum‚Äëvariance when errors are autocorrelated.
:::

---


### Summary
- You implemented the method in R, produced clean outputs, and interpreted them in context.
- Diagnostics/assumptions inform specification choices and robustness checks.
- Reuse this structure as a template for future empirical work.

