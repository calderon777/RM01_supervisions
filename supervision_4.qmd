---
title: "Supervision 4"
format: html
---

## Lab 14 â€” Collinearity


### ðŸŽ¯ Learning outcomes
- Understand what **collinearity (multicollinearity)** is and why it matters for regression inference.
- Recognise how collinearity inflates **standard errors**, weakening **tâ€‘tests** for individual coefficients.
- Diagnose collinearity using the **Variance Inflation Factor (VIF)** and interpret typical thresholds.
- Implement VIF diagnostics in R with `car::vif()` and report results clearly (tables, captions).
- Consider practical remedies (drop variables, transform/aggregate predictors, or rethink specification).

---

### ðŸ§° Prerequisites
- Multiple regression basics (OLS, \(R^2\), Fâ€‘test, tâ€‘tests).
- R and RStudio installed.
- Packages: `PoEdata`, `broom`, `knitr`, `car`.

---

### Overview: why collinearity matters
Collinearity does **not bias** OLS, but it **inflates the variance** of estimated coefficients. As a result, tâ€‘tests lose power and coefficient signs may look unstable across specificationsâ€”even when the model has a high \(R^2\). Diagnostics such as **VIF** help decide whether variables overlap so much that individual effects are hard to interpret.

---

Collinearity (multicollinearity) among regressors occurs when two or more move closely together or have limited variability. It **inflates the variance** of estimated parameters (less precise \(t\)-tests), even though OLS estimates remain unbiased. As a result, a model may have a high \(R^2\) or a large Fâ€‘statistic but **insignificant individual coefficients**.

### ðŸ§° Packages
`PoEdata`, `broom`, `knitr`, `car`

```{r}
# install.packages(c("PoEdata","broom","knitr","car"))  # if needed
library(PoEdata); library(broom); library(knitr); library(car)

# Example data: cars (mpg = miles per gallon; cyl = cylinders; eng = engine displacement; wgt = weight)
data("cars", package = "PoEdata")

# Simple model
mod1 <- lm(mpg ~ cyl, data = cars)
kable(tidy(mod1), caption = "A simple linear 'mpg' model")
```

Now add more regressors:

```{r}
mod2 <- lm(mpg ~ cyl + eng + wgt, data = cars)
kable(tidy(mod2), caption = "Multivariate 'mpg' model")
```

::: callout-note
When adding **eng** and **wgt**, the coefficient for **cyl** can turn **insignificant**â€”a classic sign of collinearity because these vehicle traits tend to move together.
:::

### Variance Inflation Factor (VIF)

The VIF for regressor $x_k$ is $VIF_k = \dfrac{1}{1-R_k^2}$, where $R_k^2$ is from regressing $x_k$ on the other regressors.

```{r}
v <- car::vif(mod2)
tab <- data.frame(regressor = names(v), VIF = as.numeric(v), row.names = NULL)
kable(tab, caption = "Variance inflation factors for the 'mpg' regression model.")
```

::: callout-tip
**Rules of thumb**  
- **VIF > 10** â†’ strong collinearity (commonly used cutoff)  
- **VIF > 5** â†’ moderate collinearity (early warning)
:::

### Mitigations (when VIFs are high)
- Remove or combine collinear variables (e.g., combine **cyl** and **eng** into a power index).  
- Use **PCA** to produce orthogonal components.  
- **Standardise/centre** variables (helps especially with interactions/polynomials).  
- Use **Ridge regression** if retaining all variables is preferred.  
- Consider **transformations** (e.g., logs).

---


### Summary
- Collinearity leaves OLS **unbiased** but makes estimates **imprecise**.
- Always check **VIFs** when several predictors are conceptually related.
- If VIFs are high, consider **dropping**, **combining**, or **reâ€‘specifying** predictorsâ€”and justify choices with theory.



---


## Lab 15 â€” Heteroskedasticity

### ðŸŽ¯ Learning outcomes
- Understand the key concept(s) introduced in this lab and their purpose in empirical analysis.
- Correctly implement the core R workflow for this topic and present clean, well-captioned results.
- Interpret model output in context and link diagnostics to modeling choices.
- Recognise common pitfalls and apply simple robustness checks.

---

### ðŸ§° Prerequisites
- Prior exposure to multiple regression and basic inference (t-tests, F-tests).
- R and RStudio installed.
- Packages: `PoEdata`, `broom`, `knitr`, plus any additional packages used in this lab.

---

### Overview
This lab introduces the core idea, why it matters for inference and decision-making, and where it fits in the broader modeling workflow. You will practise the implementation in R, interpret outputs carefully, and connect diagnostics back to modeling decisions.

---


The Gaussâ€“Markov theorem assumes **homoskedasticity** (constant error variance \(\sigma^2\)). In many economic datasets, variance grows with some regressors (e.g., higherâ€‘income households show more dispersion in expenditure). With heteroskedasticity, OLS coefficients remain unbiased, but **standard errors** and therefore **inference** are wrong unless corrected.

### ðŸ§° Packages
`lmtest`, `broom`, `PoEdata`, `car`, `sandwich`, `knitr`

```{r}
# install.packages(c("lmtest","broom","PoEdata","car","sandwich","knitr"))  # if needed
library(lmtest); library(broom); library(PoEdata); library(car); library(sandwich); library(knitr)

data("food", package="PoEdata")
mod1 <- lm(food_exp ~ income, data = food)

# Scatter and fitted line
plot(food$income, food$food_exp, xlab="income", ylab="food expenditure", pch=19, col="grey")
abline(mod1, lwd=2)
```

### Spotting heteroskedasticity with residual plots

```{r}
res <- residuals(mod1); yhat <- fitted(mod1)
plot(food$income, res, xlab="income", ylab="residuals", pch=19, col="grey")
abline(h=0, lty=2)
plot(yhat, res, xlab="fitted values", ylab="residuals", pch=19, col="grey")
abline(h=0, lty=2)
```

### Breuschâ€“Pagan test (manual construction)

```{r}
alpha <- 0.05
ressq <- resid(mod1)^2
modres <- lm(ressq ~ income, data = food)       # auxiliary regression
N <- nobs(modres)
S <- summary(modres)$df[2]                       # df for regression (k)
chisqcr <- qchisq(1 - alpha, S)                  # critical value (right-tail)
Rsqres <- summary(modres)$r.squared
chisq <- N * Rsqres
pval <- 1 - pchisq(chisq, S)
c(statistic = chisq, crit = chisqcr, p.value = pval)
```

### White (quadratic) version

```{r}
modres2 <- lm(ressq ~ income + I(income^2), data = food)
Rsq2 <- summary(modres2)$r.squared
S2 <- summary(modres2)$df[2]
chisq2 <- N * Rsq2
pval2 <- 1 - pchisq(chisq2, S2)
c(statistic = chisq2, df = S2, p.value = pval2)
```

### Breuschâ€“Pagan with `bptest()`

```{r}
kable(tidy(bptest(mod1)), caption = "Breuschâ€“Pagan heteroskedasticity test")
```

### Goldfeldâ€“Quandt test (indicator split: metro vs rural)

```{r}
alpha <- 0.05
data("cps2", package="PoEdata")
m <- cps2[cps2$metro==1, ]
r <- cps2[cps2$metro==0, ]

wg1 <- lm(wage ~ educ + exper, data = m)
wg0 <- lm(wage ~ educ + exper, data = r)

df1 <- wg1$df.residual; df0 <- wg0$df.residual
sig1 <- summary(wg1)$sigma^2; sig0 <- summary(wg0)$sigma^2
fstat <- sig1/sig0
Flc <- qf(alpha/2, df1, df0); Fuc <- qf(1-alpha/2, df1, df0)
c(F = fstat, Flc = Flc, Fuc = Fuc)
```

Goldfeldâ€“Quandt without an indicator (split by median income):

```{r}
alpha <- 0.05
medianincome <- median(food$income)
li <- food[food$income <= medianincome, ]
hi <- food[food$income >= medianincome, ]

eqli <- lm(food_exp ~ income, data = li)
eqhi <- lm(food_exp ~ income, data = hi)

dfli <- eqli$df.residual; dfhi <- eqhi$df.residual
sqli <- summary(eqli)$sigma^2; sqhi <- summary(eqhi)$sigma^2

fstat <- sqhi/sqli
Fc <- qf(1 - alpha, dfhi, dfli)
pval <- 1 - pf(fstat, dfhi, dfli)
c(F = fstat, Fcrit = Fc, p.value = pval)
```

Or use `gqtest()` directly:

```{r}
foodeq <- lm(food_exp ~ income, data = food)
tst <- lmtest::gqtest(foodeq, point = 0.5, alternative = "greater", order.by = food$income)
kable(tidy(tst), caption = "R function `gqtest()` with the 'food' equation")
```

### Heteroskedasticityâ€‘consistent (HC) standard errors

```{r}
foodeq <- lm(food_exp ~ income, data = food)
kable(tidy(foodeq), caption = "Regular standard errors in the 'food' equation")

cov1 <- car::hccm(foodeq, type = "hc1")
food.HC1 <- lmtest::coeftest(foodeq, vcov. = cov1)
kable(tidy(food.HC1), caption = "Robust (HC1) standard errors in the 'food' equation")
```

::: callout-important
Robust (HC) errors **fix inference** under heteroskedasticity, but the **point estimates are unchanged**.
:::

---


### Summary
- You applied the technique in R, produced tidy outputs, and interpreted them in context.
- Keep focusing on clean tables, explicit assumptions, and diagnostics that justify your specification.
- Use the provided patterns as a template for future empirical work.



---

## Lab 16 â€” Serial Correlation (Autocorrelation)

### ðŸŽ¯ Learning outcomes
- Understand the core ideas and assumptions for this lab's topic.
- Implement the workflow in R and create tidy outputs with clear captions.
- Interpret results in context and connect diagnostics to modeling decisions.
- Recognise common pitfalls and perform basic robustness checks.

---

### ðŸ§° Prerequisites
- Multiple regression and inference (t-tests, F-tests).
- R and RStudio installed.
- Packages: `broom`, `knitr`, and any other packages used in the lab code.

---

### Overview
This lab situates the technique within the broader econometric workflow, explaining why it matters for inference and decisionâ€‘making. You will implement it in R, assess assumptions/diagnostics, and justify specification choices.

---


Serial correlation is correlation across time in a series. In regression, **autocorrelated errors** lead to **incorrect standard errors** (and misleading \(t\)-tests).

### Example: growth vs unemployment (Okunâ€™s data)

```{r}
data("okun", package="PoEdata")
okun.ts <- ts(okun, start=c(1948,1), frequency=4)

plot(okun.ts[,"g"], ylab="growth")
plot(okun.ts[,"u"], ylab="unemployment")

# Scatter with lags
ggL1 <- data.frame(g = okun.ts[,"g"], gL1 = stats::lag(okun.ts[,"g"], -1))
plot(ggL1); abline(h=mean(ggL1$gL1, na.rm=TRUE), v=mean(ggL1$g, na.rm=TRUE), lty=2)

ggL2 <- data.frame(g = okun.ts[,"g"], gL2 = stats::lag(okun.ts[,"g"], -2))
plot(ggL2); abline(h=mean(ggL2$gL2, na.rm=TRUE), v=mean(ggL2$g, na.rm=TRUE), lty=2)

# Correlogram
acf(okun.ts[,"g"])
```

### Phillips curve example and BG/DW tests

```{r}
library(dynlm)

data("phillips_aus", package="PoEdata")
phill.ts <- ts(phillips_aus, start=c(1987,1), end=c(2009,3), frequency=4)
inflation <- phill.ts[,"inf"]
Du <- diff(phill.ts[,"u"])

# FDL model: inf_t = beta1 + beta2 * diff(u_t) + e_t
phill.dyn <- dynlm(inf ~ diff(u), data = phill.ts)
kable(tidy(phill.dyn), caption="Summary of the `phillips` model")

# Residual plot and correlogram
ehat <- resid(phill.dyn)
plot(ehat); abline(h=0, lty=2)
acf(ehat)
```

Breuschâ€“Godfrey tests (different orders/statistics):

```{r}
# NOTE: preserving your object names a, b, c, d and test types.
# (We only use base::c() below to avoid masking by object 'c')
a <- lmtest::bgtest(phill.dyn, order=1, type="F",    fill=0)
b <- lmtest::bgtest(phill.dyn, order=1, type="F",    fill=NA)
c <- lmtest::bgtest(phill.dyn, order=4, type="Chisq",fill=0)
d <- lmtest::bgtest(phill.dyn, order=4, type="Chisq",fill=NA)

dfr <- data.frame(
  Method    = base::c("1, F, 0","1, F, NA","4, Chisq, 0","4, Chisq, NA"),
  Statistic = base::c(unname(a$statistic), unname(b$statistic), unname(c$statistic), unname(d$statistic)),
  Parameters= base::c(sprintf("df1=%g, df2=%g", a$parameter[1], a$parameter[2]),
                      sprintf("df1=%g, df2=%g", b$parameter[1], b$parameter[2]),
                      sprintf("df=%g",          c$parameter),
                      sprintf("df=%g",          d$parameter)),
  p_value   = base::c(a$p.value, b$p.value, c$p.value, d$p.value)
)
knitr::kable(dfr, caption = "Breuschâ€“Godfrey test for the Phillips example.")
```

Durbinâ€“Watson (legacy but useful for small samples):

```{r}
lmtest::dwtest(phill.dyn)
```

---


### Summary
- You implemented the method in R and produced clean, wellâ€‘captioned outputs.
- Diagnostics and assumptions inform how you specify and interpret the model.
- Use these patterns as a template for future empirical work.


---

## Lab 17 â€” HAC (Neweyâ€“West) Standard Errors

### ðŸŽ¯ Learning outcomes
- Grasp the core concept(s) of this lab and where they fit in the modelling workflow.
- Implement the R workflow cleanly, producing tidy outputs with clear captions.
- Interpret results carefully and connect diagnostics/assumptions to modelling choices.
- Identify pitfalls and perform simple robustness checks.

---

### ðŸ§° Prerequisites
- Multiple regression, inference (t-tests, F-tests), and model diagnostics basics.
- R and RStudio installed.
- Packages: `broom`, `knitr`, plus any additional packages used in this lab.

---

### Overview
This lab introduces the concept, explains why it matters for inference and decisionâ€‘making, and demonstrates implementation in R. You will interpret results in context and motivate specification choices using diagnostics.

---


Correct standard errors under autocorrelation (and heteroskedasticity) using **HAC** estimators.

```{r}
library(sandwich); library(lmtest)

s0 <- coeftest(phill.dyn)                              # incorrect under AC
s1 <- coeftest(phill.dyn, vcov.=vcovHAC(phill.dyn))    # HAC
s2 <- coeftest(phill.dyn, vcov.=NeweyWest(phill.dyn))  # Neweyâ€“West
s3 <- coeftest(phill.dyn, vcov.=kernHAC(phill.dyn))    # kernel HAC

tbl <- data.frame(cbind(s0[c(3,4)], s1[c(3,4)], s2[c(3,4)], s3[c(3,4)]))
names(tbl) <- c("Incorrect","vcovHAC","NeweyWest","kernHAC")
row.names(tbl) <- c("(Intercept)","Du")
kable(tbl, digits=3, caption = "Comparing standard errors for the Phillips model.")
```

::: callout-warning
HAC fixes **inference** but not **efficiency**: OLS with HAC is still not minimumâ€‘variance when errors are autocorrelated.
:::

---


### Summary
- You implemented the method in R, produced clean outputs, and interpreted them in context.
- Diagnostics/assumptions inform specification choices and robustness checks.
- Reuse this structure as a template for future empirical work.

