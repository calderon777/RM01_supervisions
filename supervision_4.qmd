---
title: "Supervision 4"
format: html
---

## Lab 14 ‚Äî Collinearity


### üéØ Learning outcomes
- Understand what **collinearity (multicollinearity)** is and why it matters for regression inference.
- Recognise how collinearity inflates **standard errors**, weakening **t‚Äëtests** for individual coefficients.
- Diagnose collinearity using the **Variance Inflation Factor (VIF)** and interpret typical thresholds.
- Implement VIF diagnostics in R with `car::vif()` and report results clearly (tables, captions).
- Consider practical remedies (drop variables, transform/aggregate predictors, or rethink specification).

---

### üß∞ Prerequisites
- Multiple regression basics (OLS, $R^2$, F‚Äëtest, t‚Äëtests).
- R and RStudio installed.
- Packages: `PoEdata`, `broom`, `knitr`, `car`.

---

### Overview: why collinearity matters
Collinearity does **not bias** OLS, but it **inflates the variance** of estimated coefficients. As a result, t‚Äëtests lose power and coefficient signs may look unstable across specifications‚Äîeven when the model has a high $R^2$. Diagnostics such as **VIF** help decide whether variables overlap so much that individual effects are hard to interpret.

---

Collinearity (multicollinearity) among regressors occurs when two or more move closely together or have limited variability. It **inflates the variance** of estimated parameters (less precise t-tests), even though OLS estimates remain unbiased. As a result, a model may have a high $R^2$ or a large F‚Äëstatistic but **insignificant individual coefficients**.

### üß∞ Packages
`PoEdata`, `broom`, `knitr`, `car`

```{r}
# install.packages(c("PoEdata","broom","knitr","car"))  # if needed
library(PoEdata); library(broom); library(knitr); library(car)

# Example data: cars (mpg = miles per gallon; cyl = cylinders; disp = engine displacement; wt = weight)
data(mtcars)

# Simple model
mod1 <- lm(mpg ~ cyl, data = mtcars)
kable(tidy(mod1), caption = "A simple linear 'mpg' model")
```

Now add more regressors:

```{r}
mod2 <- lm(mpg ~ cyl + disp + wt, data = mtcars)
kable(tidy(mod2), caption = "Multivariate 'mpg' model")
```

::: callout-note
When adding **eng** and **wgt**, the coefficient for **cyl** can turn **insignificant**‚Äîa classic sign of collinearity because these vehicle traits tend to move together.
:::

### Variance Inflation Factor (VIF)

The VIF for regressor $x_k$ is $VIF_k = \dfrac{1}{1-R_k^2}$, where $R_k^2$ is from regressing $x_k$ on the other regressors.

```{r}
v <- car::vif(mod2)
tab <- data.frame(regressor = names(v), VIF = as.numeric(v), row.names = NULL)
kable(tab, caption = "Variance inflation factors for the 'mpg' regression model.")
```

::: callout-tip
**Rules of thumb**  
- **VIF > 10** ‚Üí strong collinearity (commonly used cutoff)  
- **VIF > 5** ‚Üí moderate collinearity (early warning)
These cut-offs are only heuristics. Evaluate VIFs alongside theory, sample size, and model purpose; high VIFs don‚Äôt automatically require dropping variables (O'Brien, 2007).
:::

### Mitigations (when VIFs are high)
- Remove or combine collinear variables (e.g., combine **cyl** and **eng** into a power index).  
- Use **PCA** to produce orthogonal components.  
- **Standardise/centre** variables (helps especially with interactions/polynomials (it won‚Äôt ‚Äúfix‚Äù genuine overlap among distinct variables).  
- Use **Ridge regression** if retaining all variables is preferred.  
- Consider **transformations** (e.g., logs).

---


### Summary
- Collinearity leaves OLS **unbiased** but makes estimates **imprecise**.
- Always check **VIFs** when several predictors are conceptually related.
- If VIFs are high, consider **dropping**, **combining**, or **re‚Äëspecifying** predictors‚Äîand justify choices with theory.


**References**
O'Brien, Robert. (2007). A Caution Regarding Rules of Thumb for Variance Inflation Factors. Quality & Quantity. 41. 673-690. 10.1007/s11135-006-9018-6. 

---


## Lab 15 ‚Äî Heteroskedasticity

### üéØ Learning outcomes
- Understand the key concept(s) introduced in this lab and their purpose in empirical analysis.
- Correctly implement the core R workflow for this topic and present clean, well-captioned results.
- Interpret model output in context and link diagnostics to modeling choices.
- Recognise common pitfalls and apply simple robustness checks.

---

### üß∞ Prerequisites
- Prior exposure to multiple regression and basic inference (t-tests, F-tests).
- R and RStudio installed.
- Packages: `PoEdata`, `broom`, `knitr`, plus any additional packages used in this lab.

---

### Overview
This lab introduces the core idea, why it matters for inference and decision-making, and where it fits in the broader modeling workflow. You will practise the implementation in R, interpret outputs carefully, and connect diagnostics back to modeling decisions.

---


The Gauss‚ÄìMarkov theorem assumes **homoskedasticity** (constant error variance $\sigma^2$). In many economic datasets, variance grows with some regressors (e.g., higher‚Äëincome households show more dispersion in expenditure). With heteroskedasticity, OLS coefficients remain unbiased, but **standard errors** and therefore, with heteroskedasticity, OLS coefficients remain unbiased (under exogeneity) but the usual SEs are inconsistent‚Äîuse HC (White) or other robust SEs. For completeness, apart from homoskedasticity, Gauss‚ÄìMarkov (for OLS to be BLUE) also requires linearity in parameters, no perfect collinearity, and exogeneity. Independence/no autocorrelation is also standard in time-series contexts.

### üß∞ Packages
`lmtest`, `broom`, `PoEdata`, `car`, `sandwich`, `knitr`

```{r}
# install.packages(c("lmtest","broom","PoEdata","car","sandwich","knitr"))  # if needed
library(lmtest); library(broom); library(PoEdata); library(car); library(sandwich); library(knitr)

data("food", package="PoEdata")
mod1 <- lm(food_exp ~ income, data = food)

# Scatter and fitted line
plot(food$income, food$food_exp, xlab="income", ylab="food expenditure", pch=19, col="grey")
abline(mod1, lwd=2)
```

### Spotting heteroskedasticity with residual plots

```{r}
res <- residuals(mod1); yhat <- fitted(mod1)
plot(food$income, res, xlab="income", ylab="residuals", pch=19, col="grey")
abline(h=0, lty=2)
plot(yhat, res, xlab="fitted values", ylab="residuals", pch=19, col="grey")
abline(h=0, lty=2)
```

### Breusch‚ÄìPagan test (manual construction)

```{r}
alpha <- 0.05
ressq <- resid(mod1)^2
modres <- lm(ressq ~ income, data = food)       # auxiliary regression
N <- nobs(modres)
S <- summary(modres)$df[2]                       # df for regression (k)
chisqcr <- qchisq(1 - alpha, S)                  # critical value (right-tail)
Rsqres <- summary(modres)$r.squared
chisq <- N * Rsqres
pval <- 1 - pchisq(chisq, S)
c(statistic = chisq, crit = chisqcr, p.value = pval)
```

### White (quadratic) version

```{r}
modres2 <- lm(ressq ~ income + I(income^2), data = food)
Rsq2 <- summary(modres2)$r.squared
S2 <- summary(modres2)$df[2]
chisq2 <- N * Rsq2
pval2 <- 1 - pchisq(chisq2, S2)
c(statistic = chisq2, df = S2, p.value = pval2)
```

### Breusch‚ÄìPagan with `bptest()`

```{r}
kable(tidy(bptest(mod1)), caption = "Breusch‚ÄìPagan heteroskedasticity test")
```

### Goldfeld‚ÄìQuandt test (indicator split: metro vs rural)

```{r}
alpha <- 0.05
data("cps2", package="PoEdata")
m <- cps2[cps2$metro==1, ]
r <- cps2[cps2$metro==0, ]

wg1 <- lm(wage ~ educ + exper, data = m)
wg0 <- lm(wage ~ educ + exper, data = r)

df1 <- wg1$df.residual; df0 <- wg0$df.residual
sig1 <- summary(wg1)$sigma^2; sig0 <- summary(wg0)$sigma^2
fstat <- sig1/sig0
Flc <- qf(alpha/2, df1, df0); Fuc <- qf(1-alpha/2, df1, df0)
c(F = fstat, Flc = Flc, Fuc = Fuc)
```

Goldfeld‚ÄìQuandt without an indicator (split by median income):

```{r}
alpha <- 0.05
medianincome <- median(food$income)
li <- food[food$income <= medianincome, ]
hi <- food[food$income >= medianincome, ]

eqli <- lm(food_exp ~ income, data = li)
eqhi <- lm(food_exp ~ income, data = hi)

dfli <- eqli$df.residual; dfhi <- eqhi$df.residual
sqli <- summary(eqli)$sigma^2; sqhi <- summary(eqhi)$sigma^2

fstat <- sqhi/sqli
Fc <- qf(1 - alpha, dfhi, dfli)
pval <- 1 - pf(fstat, dfhi, dfli)
c(F = fstat, Fcrit = Fc, p.value = pval)
```

Or use `gqtest()` directly:

```{r}
foodeq <- lm(food_exp ~ income, data = food)
tst <- lmtest::gqtest(foodeq, point = 0.5, alternative = "greater", order.by = food$income)
kable(tidy(tst), caption = "R function `gqtest()` with the 'food' equation")
```

### Heteroskedasticity‚Äëconsistent (HC) standard errors

```{r}
foodeq <- lm(food_exp ~ income, data = food)
kable(tidy(foodeq), caption = "Regular standard errors in the 'food' equation")

cov1 <- car::hccm(foodeq, type = "hc1")
food.HC1 <- lmtest::coeftest(foodeq, vcov. = cov1)
kable(tidy(food.HC1), caption = "Robust (HC1) standard errors in the 'food' equation")
```

::: callout-important
Robust (HC) errors **fix inference** under heteroskedasticity, but the **point estimates are unchanged**.
:::

---


### Summary
- You applied the technique in R, produced tidy outputs, and interpreted them in context.
- Keep focusing on clean tables, explicit assumptions, and diagnostics that justify your specification.
- Use the provided patterns as a template for future empirical work.



---

## Lab 16 ‚Äî Serial Correlation (Autocorrelation)

### üéØ Learning outcomes
- Understand the core ideas and assumptions for this lab's topic.
- Implement the workflow in R and create tidy outputs with clear captions.
- Interpret results in context and connect diagnostics to modeling decisions.
- Recognise common pitfalls and perform basic robustness checks.

---

### üß∞ Prerequisites
- Multiple regression and inference (t-tests, F-tests).
- R and RStudio installed.
- Packages: `broom`, `knitr`, and any other packages used in the lab code.

---

### Overview
This lab situates the technique within the broader econometric workflow, explaining why it matters for inference and decision‚Äëmaking. You will implement it in R, assess assumptions/diagnostics, and justify specification choices.

---


Serial correlation is correlation across time in a series. In regression, **autocorrelated errors** lead to **incorrect standard errors** (and misleading \(t\)-tests).

### Example: growth vs unemployment (Okun‚Äôs data)

```{r}
data("okun", package="PoEdata")
okun.ts <- ts(okun, start=c(1948,1), frequency=4)

plot(okun.ts[,"g"], ylab="growth")
plot(okun.ts[,"u"], ylab="unemployment")

# Scatter with lags
ggL1 <- data.frame(g = okun.ts[,"g"], gL1 = stats::lag(okun.ts[,"g"], -1))
plot(ggL1); abline(h=mean(ggL1$gL1, na.rm=TRUE), v=mean(ggL1$g, na.rm=TRUE), lty=2)

ggL2 <- data.frame(g = okun.ts[,"g"], gL2 = stats::lag(okun.ts[,"g"], -2))
plot(ggL2); abline(h=mean(ggL2$gL2, na.rm=TRUE), v=mean(ggL2$g, na.rm=TRUE), lty=2)

# Correlogram
acf(okun.ts[,"g"])
```

### Phillips curve example and BG/DW tests

```{r}
library(dynlm)

data("phillips_aus", package="PoEdata")
phill.ts <- ts(phillips_aus, start=c(1987,1), end=c(2009,3), frequency=4)
inflation <- phill.ts[,"inf"]
Du <- diff(phill.ts[,"u"])

# FDL model: inf_t = beta1 + beta2 * diff(u_t) + e_t
phill.dyn <- dynlm(inf ~ diff(u), data = phill.ts)
kable(tidy(phill.dyn), caption="Summary of the `phillips` model")

# Residual plot and correlogram
ehat <- resid(phill.dyn)
plot(ehat); abline(h=0, lty=2)
acf(ehat)
```

Breusch‚ÄìGodfrey tests (different orders/statistics):

```{r}
# NOTE: preserving your object names a, b, c, d and test types.
# (We only use base::c() below to avoid masking by object 'c')
# type="F" may be preferable in small samples
a <- lmtest::bgtest(phill.dyn, order=1, type="F",    fill=0)
b <- lmtest::bgtest(phill.dyn, order=1, type="F",    fill=NA)
c <- lmtest::bgtest(phill.dyn, order=4, type="Chisq",fill=0)
d <- lmtest::bgtest(phill.dyn, order=4, type="Chisq",fill=NA)

dfr <- data.frame(
  Method    = base::c("1, F, 0","1, F, NA","4, Chisq, 0","4, Chisq, NA"),
  Statistic = base::c(unname(a$statistic), unname(b$statistic), unname(c$statistic), unname(d$statistic)),
  Parameters= base::c(sprintf("df1=%g, df2=%g", a$parameter[1], a$parameter[2]),
                      sprintf("df1=%g, df2=%g", b$parameter[1], b$parameter[2]),
                      sprintf("df=%g",          c$parameter),
                      sprintf("df=%g",          d$parameter)),
  p_value   = base::c(a$p.value, b$p.value, c$p.value, d$p.value)
)
knitr::kable(dfr, caption = "Breusch‚ÄìGodfrey test for the Phillips example.")
```

Durbin‚ÄìWatson (DW targets AR(1) and isn‚Äôt valid with a lagged dependent variable(prefer Breusch‚ÄìGodfrey for more general cases):

```{r}
lmtest::dwtest(phill.dyn)
```

---


### Summary
- You implemented the method in R and produced clean, well‚Äëcaptioned outputs.
- Diagnostics and assumptions inform how you specify and interpret the model.
- Use these patterns as a template for future empirical work.


---

## Lab 17 ‚Äî HAC (Newey‚ÄìWest) Standard Errors

### üéØ Learning outcomes
- Grasp the core concept(s) of this lab and where they fit in the modelling workflow.
- Implement the R workflow cleanly, producing tidy outputs with clear captions.
- Interpret results carefully and connect diagnostics/assumptions to modelling choices.
- Identify pitfalls and perform simple robustness checks.

---

### üß∞ Prerequisites
- Multiple regression, inference (t-tests, F-tests), and model diagnostics basics.
- R and RStudio installed.
- Packages: `broom`, `knitr`, plus any additional packages used in this lab.

---

### Overview
This lab introduces the concept, explains why it matters for inference and decision‚Äëmaking, and demonstrates implementation in R. You will interpret results in context and motivate specification choices using diagnostics.

---


Correct standard errors under autocorrelation (and heteroskedasticity) using **HAC** estimators.

```{r}
library(sandwich); library(lmtest)

s0 <- coeftest(phill.dyn)                              # incorrect under AC
s1 <- coeftest(phill.dyn, vcov.=vcovHAC(phill.dyn))    # HAC
s2 <- coeftest(phill.dyn, vcov.=NeweyWest(phill.dyn))  # Newey‚ÄìWest
s3 <- coeftest(phill.dyn, vcov.=kernHAC(phill.dyn))    # kernel HAC

tbl <- data.frame(cbind(s0[c(3,4)], s1[c(3,4)], s2[c(3,4)], s3[c(3,4)]))
names(tbl) <- c("Incorrect","vcovHAC","NeweyWest","kernHAC")
row.names(tbl) <- c("(Intercept)","Du")
kable(tbl, digits=3, caption = "Comparing standard errors for the Phillips model.")
```

::: callout-warning
HAC fixes **inference** but not **efficiency**: OLS with HAC is still not minimum‚Äëvariance when errors are autocorrelated.
:::

---


### Summary
- You implemented the method in R, produced clean outputs, and interpreted them in context.
- Diagnostics/assumptions inform specification choices and robustness checks.
- Reuse this structure as a template for future empirical work.

