---
title: "Supervision 2"
format: html
---

## ðŸ§ª Lab 7 â€” The Simple Linear Regression Model

### ðŸŽ¯ Learning outcomes
By the end of this lab you will be able to:

1. Understand the mathematical formulation of the simple linear regression model
2. Estimate regression coefficients using `lm()` in R
3. Interpret the intercept and slope parameters in an applied context
4. Extract and examine regression output using `summary()` and related functions
5. Visualize the fitted regression line on a scatter plot
6. Assess the quality of model fit using residuals and R-squared

---

### ðŸ§° Prerequisites
- R (â‰¥ 4.0) and RStudio installed
- Completed Labs 1-6
- Packages (install once):
```r
install.packages(c("remotes", "PoEdata"))
remotes::install_github("ccolonescu/PoEdata")
```

---

**Summary.** In simple linear regression, we use **one independent variable** to predict a **dependent variable**. The relationship is represented by a straight line with an intercept and a slope.

**Introduction.** We explore the fundamentals of the Simple Linear Regression Model, a method to understand the relationship between two variables. This model is foundational for more advanced analyses.

**Key terms.** The **dependent variable** $(y)$ is what we aim to predict; the **independent variable**, $x$, explains $y$.

### The General Model

Assume a linear relationship between the conditional expectation of \(y\) and \(x\):

$$
y_i = \beta_1 + \beta_2 x_i + e_i \tag{1}
$$

- $\beta_1$: intercept 
- $\beta_2$: slope  
- $e_i$: error term with variance $\sigma^2$  
- $i=1,\dots,N$: observation index

The predicted (estimated) value of $y$ given $x$ is:

$$
\hat{y} = b_1 + b_2 x \tag{2}
$$

Assumptions: 

(i) non-random \(x\); 
(ii) constant error variance (homoskedasticity); 
(iii) errors uncorrelated across observations; 
(iv) $E[e_i \mid x_i]=0$.

#### PART A. Example: Food Expenditure vs Income

```r
# Recall to install the package PoEdata once if needed:
install.packages("remotes")
remotes::install_github("ccolonescu/PoEdata")
```

```{r}
library(PoEdata)
data("cps_small")
plot(cps_small$educ, cps_small$wage, xlab="Education", ylab="Wage")

# Food data
data("food")
head(food)
plot(food$income, food$food_exp,
     ylim=c(0, max(food$food_exp)),
     xlim=c(0, max(food$income)),
     xlab="weekly income in $100", ylab="weekly food expenditure in $",
     type="p")
```

#### PART B. Estimating a Linear Regression

For the food data the model is

$$
\text{food\_exp}_i = \beta_1 + \beta_2\,\text{income}_i + e_i.\tag{3}
$$

```{r}
library(PoEdata)
mod1 <- lm(food_exp ~ income, data = food)
b1 <- coef(mod1)[[1]]
b2 <- coef(mod1)[[2]]
smod1 <- summary(mod1)
smod1
```

Add the regression line to the scatter plot:

```{r}
plot(food$income, food$food_exp,
     ylim=c(0, max(food$food_exp)),
     xlim=c(0, max(food$income)),
     xlab="weekly income in $100", ylab="weekly food expenditure in $",
     type = "p")
abline(b1, b2)
```

Retrieve common results:

```{r}
names(mod1)
names(smod1)

mod1$coefficients
smod1$coefficients
```

#### PART C. MCQs

1. What relationship does the simple linear regression assume between \(y\) and \(x\)?  
   A. Quadratic &nbsp;&nbsp; B. Logarithmic &nbsp;&nbsp; **C. Linear** &nbsp;&nbsp; D. Exponential

2. In Eq. (1), what is \(\beta_1\)?  
   A. Slope &nbsp;&nbsp; B. Error term &nbsp;&nbsp; **C. Intercept** &nbsp;&nbsp; D. Dependent variable

3. The slope parameter indicates:  
   A. Error variance &nbsp;&nbsp; **B. Change in \(y\) for a one-unit change in \(x\)** &nbsp;&nbsp; C. Intercept &nbsp;&nbsp; D. Sample size

4. "Residuals" are:  
   A. Predicted \(y\) &nbsp;&nbsp; B. \(x\) values &nbsp;&nbsp; **C. Observed minus predicted \(y\)** &nbsp;&nbsp; D. Coefficients

5. `summary(mod1)` is used to:  
   A. List object names &nbsp;&nbsp; B. Change coefficients &nbsp;&nbsp; C. Plot the line &nbsp;&nbsp; **D. Display regression results**

6. Estimating with random subsamples primarily helps to:  
   A. Change coefficients &nbsp;&nbsp; **B. Assess variability of coefficients** &nbsp;&nbsp; C. Increase N &nbsp;&nbsp; D. Make the model nonâ€‘linear

7. Adjusted \(R^2\) measures:  
   A. Number of observations &nbsp;&nbsp; B. Precision of intercept &nbsp;&nbsp; **C. Proportion of variance explained (adjusted)** &nbsp;&nbsp; D. SE of coefficients

---

## ðŸ§ª Lab 8 â€” Prediction with the Linear Regression Model

The estimates \(b_1\) and \(b_2\) allow prediction via Eq. (2). Example: income = \$2000 â†’ **income = 20** (hundreds of dollars).

```{r}
# Model
mod1 <- lm(food_exp ~ income, data=food)

# Predict at incomes = 2000, 2500, 2700 USD
newx <- data.frame(income = c(20, 25, 27))
yhat <- predict(mod1, newx)
names(yhat) <- c("income=$2000", "$2500", "$2700")
yhat
```

### Repeated Samples to Assess Regression Coefficients

```{r}
N <- nrow(food) # number of observations
C <- 50         # number of subsamples
S <- 38         # subsample size

sumb2 <- 0
for (i in 1:C){
  set.seed(3*i)
  subsample <- food[sample(1:N, size=S, replace=TRUE), ]
  mod2 <- lm(food_exp ~ income, data=subsample)
  sumb2 <- sumb2 + coef(mod2)[[2]]
}
print(sumb2/C, digits = 3)
```

### Estimated Variances and Covariance of Coefficients

```{r}
(varb1 <- vcov(mod1)[1, 1])
(varb2 <- vcov(mod1)[2, 2])
(covb1b2 <- vcov(mod1)[1, 2])
```

### Non-Linear Relationships

#### Quadratic model

$$
y_i = \beta_1 + \beta_2 x_i^2 + e_i.\tag{5}
$$

```{r}
library(PoEdata)
data(br)
mod3 <- lm(price ~ I(sqft^2), data=br)
b1 <- coef(mod3)[[1]]
b2 <- coef(mod3)[[2]]

sqftx <- c(2000, 4000, 6000)
pricex <- b1 + b2*sqftx^2
DpriceDsqft <- 2*b2*sqftx
elasticity <- DpriceDsqft*sqftx/pricex

b1; b2; DpriceDsqft; elasticity
```

Plotting alternatives:

```{r}
mod31 <- lm(price ~ I(sqft^2), data=br)
plot(br$sqft, br$price, xlab="Total square feet", ylab="Sale price, $", col="grey")
curve(b1 + b2*x^2, add=TRUE)
```

```{r}
ordat <- br[order(br$sqft), ]
mod31 <- lm(price ~ I(sqft^2), data=ordat)
plot(br$sqft, br$price, main="Dataset ordered after 'sqft'",
     xlab="Total square feet", ylab="Sale price, $", col="grey")
lines(fitted(mod31) ~ ordat$sqft)
```

#### Log-linear model

$$
\log(y_i) = \beta_1 + \beta_2 x_i + e_i.\tag{6}
$$

```{r}
hist(br$price)
hist(log(br$price))

mod4 <- lm(log(price) ~ sqft, data=br)
b1 <- coef(mod4)[[1]]
b2 <- coef(mod4)[[2]]

# Fitted curve on original price scale
ordat <- br[order(br$sqft), ]
mod4 <- lm(log(price) ~ sqft, data=ordat)
plot(br$sqft, br$price, col="grey")
lines(exp(fitted(mod4)) ~ ordat$sqft)
```

Elasticity and marginal effect at the median price:

```{r}
pricex <- median(br$price)
sqftx <- (log(pricex) - coef(mod4)[[1]])/coef(mod4)[[2]]
(DyDx <- pricex*coef(mod4)[[2]])
(elasticity <- sqftx*coef(mod4)[[2]])
```

Multiple points:

```{r}
b1 <- coef(mod4)[[1]]
b2 <- coef(mod4)[[2]]
sqftx <- c(2000, 3000, 4000)
pricex <- c(100000, exp(b1 + b2*sqftx))
sqftx <- (log(pricex) - b1)/b2
(elasticities <- b2*sqftx)
```

#### MCQs

1. Using \(b_1\) and \(b_2\) primarily helps to: **B. Predict \(E[y\mid x]\)**  
2. `lm()` is used to: **C. Estimate a linear model**  
3. `predict()` mainly: **B. Estimates \(\hat y\) for new data**  
4. Coefficients are random because: **C. They depend on the sample**  
5. Random subsamples help: **C. Evaluate stability/variability**  
6. `vcov()` is for: **B. Variances and covariances of coefficients**  
7. `data.frame()` with `predict()` to: **A. Format new \(x\) values**

---

## ðŸ§ª Lab 9 â€” Hypothesis Test, pâ€‘Value & Testing Linear Combinations

### Hypothesis Tests

Null vs alternative for a coefficient \(\beta_k\):

$$
H_0: \beta_k = c, \qquad H_A: \beta_k \ne c.
$$

Test statistic:
$$
t = \frac{b_k - c}{\operatorname{se}(b_k)},\quad t \sim t_{N-2}.\tag{6}
$$

Example for \(b_2\) in the food model:

```{r}
alpha <- 0.05
library(PoEdata); library(xtable); library(knitr)
data("food")
mod1 <- lm(food_exp ~ income, data=food); smod1 <- summary(mod1)

table <- data.frame(xtable(mod1))
kable(table, caption="Regression output showing the coefficients")

b2 <- coef(mod1)[["income"]]
seb2 <- sqrt(vcov(mod1)[2,2])
df  <- df.residual(mod1)

t   <- b2/seb2
tcr <- qt(1-alpha/2, df)
t; tcr
```

Rightâ€‘tail and leftâ€‘tail versions:

```{r}
# Right-tail: H0: beta2 <= 5.5; HA: beta2 > 5.5
c <- 5.5
t_right <- (b2 - c)/seb2
tcr_right <- qt(1-alpha, df)

# Left-tail: H0: beta2 >= 15; HA: beta2 < 15
c <- 15
t_left <- (b2 - c)/seb2
tcr_left <- qt(alpha, df)

c(t_right=t_right, tcr_right=tcr_right, t_left=t_left, tcr_left=tcr_left)
```

### The pâ€‘Value

Rightâ€‘tail: \(p = 1 - F_t(t)\). Leftâ€‘tail: \(p = F_t(t)\). Twoâ€‘tail: \(p = 2\big(1-F_t(|t|)\big)\).

```{r}
# Right tail
c <- 5.5; t <- (b2-c)/seb2; p_right <- 1-pt(t, df)

# Left tail
c <- 15; t <- (b2-c)/seb2; p_left <- pt(t, df)

# Two tail
c <- 0;  t <- (b2-c)/seb2; p_two <- 2*(1-pt(abs(t), df))

c(p_right=p_right, p_left=p_left, p_two=p_two)
```

### Testing Linear Combinations of Parameters

Expected food expenditure at income = \$2000 (i.e. \(x=20\)):

$$
L = E(\text{food\_exp}\mid \text{income}=20) = \beta_1 + 20\,\beta_2.\tag{3.7}
$$

Variance identities:

$$
\operatorname{var}(aX+bY) = a^2\,\operatorname{var}(X) + b^2\,\operatorname{var}(Y) + 2ab\,\operatorname{cov}(X,Y).\tag{3.8}
$$

$$
\operatorname{var}(b_1+20b_2) = \operatorname{var}(b_1) + 20^2\operatorname{var}(b_2) + 2\cdot20\operatorname{cov}(b_1,b_2).\tag{3.9}
$$

```{r}
alpha <- 0.05; x <- 20
m1 <- lm(food_exp ~ income, data=food)
df <- df.residual(m1)

b1 <- m1$coef[1]; b2 <- m1$coef[2]
varb1 <- vcov(m1)[1,1]; varb2 <- vcov(m1)[2,2]; covb1b2 <- vcov(m1)[1,2]

L <- b1 + b2*x
varL <- varb1 + x^2 * varb2 + 2*x*covb1b2
seL <- sqrt(varL)

tcr <- qt(1-alpha/2, df)
lowbL <- L - tcr*seL; upbL <- L + tcr*seL
c(L=L, seL=seL, low=lowbL, up=upbL)
```

Twoâ€‘tail, leftâ€‘tail, rightâ€‘tail tests for \(L\):

```{r}
c <- 250
t <- (L - c)/seL
p_value <- 2*(1-pt(abs(t), df))
c(t=t, p_value=p_value)
```

---

## ðŸ§ª Lab 10 â€” The pâ€‘Value (Recap)

- **Rightâ€‘tail:** \(p=1-pt(t,\,df)\)  
- **Leftâ€‘tail:** \(p=pt(t,\,df)\)  
- **Twoâ€‘tail:** \(p=2(1-pt(|t|,\,df))\)

```{r}
# Examples computed earlier are reproduced here:
c(p_right=p_right, p_left=p_left, p_two=p_two)
```

---

### References

Adkins (2014); Allaire et al. (2016); Colonescu (2016); Croissant & Millo (2015); Dahl (2016); Fox & Weisberg (2016); Fox et al. (2016); Ghalanos (2015); Graves (2014); Grolemund & Wickham (2016); Henningsen & Hamann (2015); Hill, Griffiths & Lim (2011); Hlavac (2015); Hothorn et al. (2015); Hyndman (2016); Kleiber & Zeileis (2015); Komashko (2016); Lander (2013); Lumley & Zeileis (2015); Pfaff (2013); R Core Team (2008); Reinhart (2015); Robinson (2016); RStudio Team (2015); Spada et al. (2012); Trapletti & Hornik (2016); Wickham & Chang (2016); Xie (2014, 2016a, 2016b); Zeileis (2016).
