---
title: "Supervision 2"
date-modified: last-modified
date-format: "D MMM YYYY, HH:mm"   # e.g., 07 Oct 2025, 14:32

---

## üß™ Lab 7 ‚Äî The Simple Linear Regression Model

### üéØ Learning outcomes
By the end of this lab you will be able to:

1. Understand the mathematical formulation of the simple linear regression model
2. Estimate regression coefficients using `lm()` in R
3. Interpret the intercept and slope parameters in an applied context
4. Extract and examine regression output using `summary()` and related functions
5. Visualize the fitted regression line on a scatter plot
6. Assess the quality of model fit using residuals and R-squared

---

### üß∞ Prerequisites
- R (‚â• 4.0) and RStudio installed
- Completed Labs 1-6
- Packages (install once):
```r
install.packages(c("remotes", "PoEdata"))
remotes::install_github("ccolonescu/PoEdata")
```

---

**Summary.** In simple linear regression, we use **one independent variable** to predict a **dependent variable**. The relationship is represented by a straight line with an intercept and a slope.

**Introduction.** We explore the fundamentals of the Simple Linear Regression Model, a method to understand the relationship between two variables. This model is foundational for more advanced analyses.

**Key terms.** The **dependent variable** $(y)$ is what we aim to predict; the **independent variable**, $x$, explains $y$.

### The General Model

Assume a linear relationship between the conditional expectation of \(y\) and \(x\):

$$
y_i = \beta_1 + \beta_2 x_i + e_i \tag{1}
$$

- $\beta_1$: intercept 
- $\beta_2$: slope  
- $e_i$: error term with variance $\sigma^2$  
- $i=1,\dots,N$: observation index

The predicted (estimated) value of $y$ given $x$ is:

$$
\hat{y} = b_1 + b_2 x \tag{2}
$$

Assumptions: 

(i) non-random \(x\); 
(ii) constant error variance (homoskedasticity); 
(iii) errors uncorrelated across observations; 
(iv) $E[e_i \mid x_i]=0$.

#### PART A. Example: Food Expenditure vs Income

```r
# Recall to install the package PoEdata once if needed:
install.packages("remotes")
remotes::install_github("ccolonescu/PoEdata")
```

```{r}
library(PoEdata)
data("cps_small")
plot(cps_small$educ, cps_small$wage, xlab="Education", ylab="Wage")

# Food data
data("food")
head(food)
plot(food$income, food$food_exp,
     ylim=c(0, max(food$food_exp)),
     xlim=c(0, max(food$income)),
     xlab="weekly income in $100", ylab="weekly food expenditure in $",
     type="p")
```

#### PART B. Estimating a Linear Regression

For the food data the model is

$$
\text{food\_exp}_i = \beta_1 + \beta_2\,\text{income}_i + e_i.\tag{3}
$$

```{r}
library(PoEdata)
mod1 <- lm(food_exp ~ income, data = food)
b1 <- coef(mod1)[[1]]
b2 <- coef(mod1)[[2]]
smod1 <- summary(mod1)
smod1
```

Add the regression line to the scatter plot:

```{r}
plot(food$income, food$food_exp,
     ylim=c(0, max(food$food_exp)),
     xlim=c(0, max(food$income)),
     xlab="weekly income in $100", ylab="weekly food expenditure in $",
     type = "p")
abline(b1, b2)
```

Retrieve common results:

```{r}
names(mod1)
names(smod1)

mod1$coefficients
smod1$coefficients
```


#### PART C. MCQs

<!-- FEEDBACK SCRIPT: sets .is-correct / .is-incorrect on the clicked label -->
<script>
(function () {
  const norm = s => (s || "").replace(/\s+/g, " ").trim();
  document.addEventListener("change", function (e) {
    const input = e.target.closest('.mcq-we-q input[type="radio"]');
    if (!input) return;

    // find the corresponding label (works for both nested and for= cases)
    const label = input.closest('label') || document.querySelector(`label[for="${input.id}"]`);
    if (!label) return;

    const q = label.closest(".mcq-we-q") || input.closest(".mcq-we-q");
    if (!q) return;

    const keyEl = q.querySelector(".mcq-key");
    if (!keyEl) return;

    const key = norm(keyEl.getAttribute("data-correct"));
    const choice = norm(label.textContent);

    // reset previous feedback in this question
    q.querySelectorAll("label").forEach(l => l.classList.remove("is-correct","is-incorrect"));

    // add feedback to the chosen option
    label.classList.add(choice === key ? "is-correct" : "is-incorrect");
  });
})();
</script>




<div class="mcq-we">

<div class="mcq-we-q">
**1) What relationship does simple linear regression assume between y and x?**  
`r webexercises::longmcq(c("Quadratic", "Logarithmic", answer = "Linear", "Exponential"))`
<span class="mcq-key" data-correct="Linear"></span>
<details class="hint"><summary>Hint</summary> Think straight line: one slope + one intercept.</details>Œ≤
</div>

<div class="mcq-we-q">
**2) In the model y = Œ≤‚ÇÄ + Œ≤‚ÇÅ x + Œµ, what does Œ≤‚ÇÅ represent?**  
`r webexercises::longmcq(c("The average of y", "The error variance", answer = "Expected change in y for a one-unit increase in x", "The intercept"))`
<span class="mcq-key" data-correct="Expected change in y for a one-unit increase in x"></span>
<details class="hint"><summary>Hint</summary> Œîy for +1 in x.</details>
</div>

<div class="mcq-we-q">
**3) In the model y = Œ≤‚ÇÄ + Œ≤‚ÇÅ x + Œµ, what does Œ≤‚ÇÄ represent?**  
`r webexercises::longmcq(c("The slope of x", answer = "The expected value of y when x = 0", "The variance of Œµ", "Correlation between x and y"))`
<span class="mcq-key" data-correct="The expected value of y when x = 0"></span>
<details class="hint"><summary>Hint</summary> Value of y at x = 0.</details>
</div>

<div class="mcq-we-q">
**4) A residual is defined as:**  
`r webexercises::longmcq(c("Predicted y minus observed y", "Observed x minus predicted x", answer = "Observed y minus predicted y", "The fitted value"))`
<span class="mcq-key" data-correct="Observed y minus predicted y"></span>
<details class="hint"><summary>Hint</summary> Actual ‚àí fitted.</details>
</div>

<div class="mcq-we-q">
**5) Ordinary Least Squares (OLS) chooses coefficients to:**  
`r webexercises::longmcq(c("Maximize R¬≤", "Minimize the sum of absolute residuals", answer = "Minimize the sum of squared residuals", "Maximize the likelihood under normal errors only"))`
<span class="mcq-key" data-correct="Minimize the sum of squared residuals"></span>
<details class="hint"><summary>Hint</summary> Squares penalize big errors more.</details>
</div>

<div class="mcq-we-q">
**6) With an intercept in the model, OLS residuals:**  
`r webexercises::longmcq(c("Sum to a positive number", "Sum to a negative number", answer = "Sum to zero", "Sum to the sample mean of y"))`
<span class="mcq-key" data-correct="Sum to zero"></span>
</div>

<div class="mcq-we-q">
**7) The OLS regression line passes through:**  
`r webexercises::longmcq(c("The origin (0,0)", answer = "The point (xÃÑ, »≥)", "The median of x and y", "The maximum of y"))`
<span class="mcq-key" data-correct="The point (xÃÑ, »≥)"></span>
<details class="hint"><summary>Hint</summary> It goes through the means.</details>
</div>

<div class="mcq-we-q">
**8) Which R function fits a simple linear regression model?**  
`r webexercises::longmcq(c("plot()", "cor()", answer = "lm()", "predict()"))`
<span class="mcq-key" data-correct="lm()"></span>
</div>

<div class="mcq-we-q">
**9) In `summary(mod)`, the p-value for the slope tests:**  
`r webexercises::longmcq(c("H‚ÇÄ: Œ≤‚ÇÅ ‚â† 0", answer = "H‚ÇÄ: Œ≤‚ÇÅ = 0", "H‚ÇÄ: Œ≤‚ÇÄ = 0", "H‚ÇÄ: errors are normal"))`
<span class="mcq-key" data-correct="H0: B‚ÇÅ = 0"></span>
</div>

<div class="mcq-we-q">
**10) The p-value is best described as:**  
`r webexercises::longmcq(c("The probability H‚ÇÅ is true", "The probability the estimate is correct", answer = "The probability (under H‚ÇÄ) of a result at least as extreme as observed", "The Type II error rate"))`
<span class="mcq-key" data-correct="The probability (under H0) of a result at least as extreme as observed"></span>
</div>

<div class="mcq-we-q">
**11) A 95% confidence interval for Œ≤‚ÇÅ is generally:**  
`r webexercises::longmcq(c("Estimate ¬± 1.96 √ó œÉ of y", answer = "Estimate ¬± critical t √ó SE(Œ≤‚ÇÅ)", "Estimate ¬± 2 √ó residual SD", "SE(Œ≤‚ÇÅ) ¬± estimate"))`
<span class="mcq-key" data-correct="Estimate ¬± critical t √ó SE(B‚ÇÅ)"></span>
</div>

<div class="mcq-we-q">
**12) R¬≤ measures:**  
`r webexercises::longmcq(c("The correlation between x and y", answer = "Proportion of variance in y explained by x", "The average residual size", "The probability the model is correct"))`
<span class="mcq-key" data-correct="Proportion of variance in y explained by x"></span>
</div>

<div class="mcq-we-q">
**13) Adjusted R¬≤ differs from R¬≤ because it:**  
`r webexercises::longmcq(c("Ignores sample size", "Is always larger than R¬≤", answer = "Penalizes adding predictors relative to sample size", "Equals 1 ‚àí R¬≤"))`
<span class="mcq-key" data-correct="Penalizes adding predictors relative to sample size"></span>
</div>

<div class="mcq-we-q">
**14) To make a prediction at x = x‚ÇÄ, you use:**  
`r webexercises::longmcq(c("Œ≤‚ÇÅ √ó x‚ÇÄ", "Œ≤‚ÇÄ √∑ x‚ÇÄ", answer = "≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó x‚ÇÄ", "SE(Œ≤‚ÇÅ) √ó x‚ÇÄ"))`
<span class="mcq-key" data-correct="≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó x‚ÇÄ"></span>
<details class="hint"><summary>Hint</summary> Plug x‚ÇÄ into the fitted line.</details>
</div>

<div class="mcq-we-q">
**15) For OLS to be unbiased, a key assumption is:**  
`r webexercises::longmcq(c("x is normally distributed", "Errors have zero variance", answer = "E[Œµ | x] = 0 (errors have zero mean conditional on x)", "y is standardized"))`
<span class="mcq-key" data-correct="E[Œµ | x] = 0 (errors have zero mean conditional on x)"></span>
</div>

<div class="mcq-we-q">
**16) Homoskedasticity means:**  
`r webexercises::longmcq(c("Errors are perfectly correlated", answer = "Variance of errors is constant across x", "Mean of errors is zero only at xÃÑ", "x has constant variance"))`
<span class="mcq-key" data-correct="Variance of errors is constant across x"></span>
</div>

<div class="mcq-we-q">
**17) In `summary(mod)`, the t value for Œ≤‚ÇÅ equals:**  
`r webexercises::longmcq(c("Œ≤‚ÇÅ", "SE(Œ≤‚ÇÅ)", answer = "Estimate(Œ≤‚ÇÅ) / SE(Œ≤‚ÇÅ)", "1 ‚àí p-value"))`
<span class="mcq-key" data-correct="Estimate(Œ≤‚ÇÅ) / SE(Œ≤‚ÇÅ)"></span>
<details class="hint"><summary>Hint</summary> Signal √∑ noise for the slope estimate.</details>
</div>

<div class="mcq-we-q">
**18) The units of Œ≤‚ÇÅ are best described as:**  
`r webexercises::longmcq(c("Units of y", answer = "Units of y per unit of x", "Unitless", "Units of x per unit of y"))`
<span class="mcq-key" data-correct="Units of y per unit of x"></span>
</div>

<div class="mcq-we-q">
**19) Rescaling x from dollars to hundreds of dollars will:**  
`r webexercises::longmcq(c("Leave Œ≤‚ÇÅ unchanged", answer = "Rescale Œ≤‚ÇÅ numerically but leave R¬≤ and the t-test for Œ≤‚ÇÅ unchanged", "Change the data but not the model", "Invalidate OLS"))`
<span class="mcq-key" data-correct="Rescale Œ≤‚ÇÅ numerically but leave R¬≤ and the t-test for Œ≤‚ÇÅ essentially unchanged"></span>
</div>

<div class="mcq-we-q">
**20) After fitting `mod <- lm(y ~ x, d)`, the fitted value at x‚ÇÄ is:**  
`r webexercises::longmcq(c("residuals(mod)[x==x‚ÇÄ]", answer = "predict(mod, newdata = data.frame(x = x‚ÇÄ))", "coef(mod)['x']", "fitted(mod)[1] always"))`
<span class="mcq-key" data-correct="predict(mod, newdata = data.frame(x = x‚ÇÄ))"></span>
<details class="hint"><summary>Hint</summary> Use <code>predict()</code> with a small data frame for x‚ÇÄ.</details>
</div>

</div>



---

## üß™ Lab 8 ‚Äî Prediction with the Linear Regression Model

The estimates $b_1$ and $b_2$ allow prediction via Eq. (2). In the following R script "income = \$2000" is **"income = 20"** (the data is in hundreds of dollars).

```{r}
# Fit a simple linear regression: food_exp (response) on income (predictor)
# 'food' is the data frame that contains the variables
mod1 <- lm(food_exp ~ income, data = food)

mod1

# Build a small data frame with the x-values where we want predictions.
# NOTE: if 'income' is measured in HUNDREDS of dollars in your dataset,
# then $2000 -> 20, $2500 -> 25, $2700 -> 27 (adjust if your units differ).
newx <- data.frame(income = c(20, 25, 27))

# Use the fitted model to predict food_exp at those income values
yhat <- predict(mod1, newx)

# Give friendly names to each prediction so the output is easy to read
names(yhat) <- c("income = $2000", "income = $2500", "income = $2700")

# Show the predictions
yhat

```

Compare the repeated samples witht the original regression output for $beta_1$


### Repeated Samples to Assess Regression Coefficients

```{r}
N <- nrow(food)   # observations
C <- 50           # repeats
S <- 38           # subsample size

sumb2 <- 0        # sum of slopes
for (i in 1:C){
  set.seed(3*i)   # reproducible
  subsample <- food[sample(1:N, S, replace = TRUE), ]  # bootstrap draw
  mod2 <- lm(food_exp ~ income, data = subsample)      # fit model
  sumb2 <- sumb2 + coef(mod2)[2]                       # add slope Œ≤‚ÇÅ
}

print(sumb2 / C, digits = 3)   # average slope
```

### Estimated Variances and Covariance of Coefficients

```{r}
(varb1 <- vcov(mod1)[1, 1])
(varb2 <- vcov(mod1)[2, 2])
(covb1b2 <- vcov(mod1)[1, 2])
```

### Non-Linear Relationships

#### Quadratic model

$$
y_i = \beta_1 + \beta_2 x_i^2 + e_i.\tag{5}
$$

```{r}
library(PoEdata)
data(br)
mod3 <- lm(price ~ I(sqft^2), data=br)
b1 <- coef(mod3)[[1]]
b2 <- coef(mod3)[[2]]

sqftx <- c(2000, 4000, 6000)
pricex <- b1 + b2*sqftx^2
DpriceDsqft <- 2*b2*sqftx
elasticity <- DpriceDsqft*sqftx/pricex

b1; b2; DpriceDsqft; elasticity
```

Plotting alternatives:

```{r}
mod31 <- lm(price ~ I(sqft^2), data=br)
plot(br$sqft, br$price, xlab="Total square feet", ylab="Sale price, $", col="grey")
curve(b1 + b2*x^2, add=TRUE)
```

```{r}
ordat <- br[order(br$sqft), ]
mod31 <- lm(price ~ I(sqft^2), data=ordat)
plot(br$sqft, br$price, main="Dataset ordered after 'sqft'",
     xlab="Total square feet", ylab="Sale price, $", col="grey")
lines(fitted(mod31) ~ ordat$sqft)
```

#### Log-linear model

$$
\log(y_i) = \beta_1 + \beta_2 x_i + e_i.\tag{6}
$$

```{r}
hist(br$price)
hist(log(br$price))

mod4 <- lm(log(price) ~ sqft, data=br)
b1 <- coef(mod4)[[1]]
b2 <- coef(mod4)[[2]]

# Fitted curve on original price scale
ordat <- br[order(br$sqft), ]
mod4 <- lm(log(price) ~ sqft, data=ordat)
plot(br$sqft, br$price, col="grey")
lines(exp(fitted(mod4)) ~ ordat$sqft)
```

Elasticity and marginal effect at the median price:

```{r}
pricex <- median(br$price)
sqftx <- (log(pricex) - coef(mod4)[[1]])/coef(mod4)[[2]]
(DyDx <- pricex*coef(mod4)[[2]])
(elasticity <- sqftx*coef(mod4)[[2]])
```

Multiple points:

```{r}
b1 <- coef(mod4)[[1]]
b2 <- coef(mod4)[[2]]
sqftx <- c(2000, 3000, 4000)
pricex <- c(100000, exp(b1 + b2*sqftx))
sqftx <- (log(pricex) - b1)/b2
(elasticities <- b2*sqftx)
```

#### MCQs

1. Using \(b_1\) and \(b_2\) primarily helps to: **B. Predict \(E[y\mid x]\)**  
2. `lm()` is used to: **C. Estimate a linear model**  
3. `predict()` mainly: **B. Estimates \(\hat y\) for new data**  
4. Coefficients are random because: **C. They depend on the sample**  
5. Random subsamples help: **C. Evaluate stability/variability**  
6. `vcov()` is for: **B. Variances and covariances of coefficients**  
7. `data.frame()` with `predict()` to: **A. Format new \(x\) values**

---

## üß™ Lab 9 ‚Äî Hypothesis Test, p‚ÄëValue & Testing Linear Combinations

### Hypothesis Tests

Null vs alternative for a coefficient \(\beta_k\):

$$
H_0: \beta_k = c, \qquad H_A: \beta_k \ne c.
$$

Test statistic:
$$
t = \frac{b_k - c}{\operatorname{se}(b_k)},\quad t \sim t_{N-2}.\tag{6}
$$

Example for \(b_2\) in the food model:

```{r}
alpha <- 0.05
library(PoEdata); library(xtable); library(knitr)
data("food")
mod1 <- lm(food_exp ~ income, data=food); smod1 <- summary(mod1)

table <- data.frame(xtable(mod1))
kable(table, caption="Regression output showing the coefficients")

b2 <- coef(mod1)[["income"]]
seb2 <- sqrt(vcov(mod1)[2,2])
df  <- df.residual(mod1)

t   <- b2/seb2
tcr <- qt(1-alpha/2, df)
t; tcr
```

Right‚Äëtail and left‚Äëtail versions:

```{r}
# Right-tail: H0: beta2 <= 5.5; HA: beta2 > 5.5
c <- 5.5
t_right <- (b2 - c)/seb2
tcr_right <- qt(1-alpha, df)

# Left-tail: H0: beta2 >= 15; HA: beta2 < 15
c <- 15
t_left <- (b2 - c)/seb2
tcr_left <- qt(alpha, df)

c(t_right=t_right, tcr_right=tcr_right, t_left=t_left, tcr_left=tcr_left)
```

### The p‚ÄëValue

Right‚Äëtail: \(p = 1 - F_t(t)\). Left‚Äëtail: \(p = F_t(t)\). Two‚Äëtail: \(p = 2\big(1-F_t(|t|)\big)\).

```{r}
# Right tail
c <- 5.5; t <- (b2-c)/seb2; p_right <- 1-pt(t, df)

# Left tail
c <- 15; t <- (b2-c)/seb2; p_left <- pt(t, df)

# Two tail
c <- 0;  t <- (b2-c)/seb2; p_two <- 2*(1-pt(abs(t), df))

c(p_right=p_right, p_left=p_left, p_two=p_two)
```

### Testing Linear Combinations of Parameters

Expected food expenditure at income = \$2000 (i.e. \(x=20\)):

$$
L = E(\text{food\_exp}\mid \text{income}=20) = \beta_1 + 20\,\beta_2.\tag{3.7}
$$

Variance identities:

$$
\operatorname{var}(aX+bY) = a^2\,\operatorname{var}(X) + b^2\,\operatorname{var}(Y) + 2ab\,\operatorname{cov}(X,Y).\tag{3.8}
$$

$$
\operatorname{var}(b_1+20b_2) = \operatorname{var}(b_1) + 20^2\operatorname{var}(b_2) + 2\cdot20\operatorname{cov}(b_1,b_2).\tag{3.9}
$$

```{r}
alpha <- 0.05; x <- 20
m1 <- lm(food_exp ~ income, data=food)
df <- df.residual(m1)

b1 <- m1$coef[1]; b2 <- m1$coef[2]
varb1 <- vcov(m1)[1,1]; varb2 <- vcov(m1)[2,2]; covb1b2 <- vcov(m1)[1,2]

L <- b1 + b2*x
varL <- varb1 + x^2 * varb2 + 2*x*covb1b2
seL <- sqrt(varL)

tcr <- qt(1-alpha/2, df)
lowbL <- L - tcr*seL; upbL <- L + tcr*seL
c(L=L, seL=seL, low=lowbL, up=upbL)
```

Two‚Äëtail, left‚Äëtail, right‚Äëtail tests for \(L\):

```{r}
c <- 250
t <- (L - c)/seL
p_value <- 2*(1-pt(abs(t), df))
c(t=t, p_value=p_value)
```

---

## üß™ Lab 10 ‚Äî The p‚ÄëValue (Recap)

- **Right‚Äëtail:** \(p=1-pt(t,\,df)\)  
- **Left‚Äëtail:** \(p=pt(t,\,df)\)  
- **Two‚Äëtail:** \(p=2(1-pt(|t|,\,df))\)

```{r}
# Examples computed earlier are reproduced here:
c(p_right=p_right, p_left=p_left, p_two=p_two)
```

---

### References

Adkins (2014); Allaire et al. (2016); Colonescu (2016); Croissant & Millo (2015); Dahl (2016); Fox & Weisberg (2016); Fox et al. (2016); Ghalanos (2015); Graves (2014); Grolemund & Wickham (2016); Henningsen & Hamann (2015); Hill, Griffiths & Lim (2011); Hlavac (2015); Hothorn et al. (2015); Hyndman (2016); Kleiber & Zeileis (2015); Komashko (2016); Lander (2013); Lumley & Zeileis (2015); Pfaff (2013); R Core Team (2008); Reinhart (2015); Robinson (2016); RStudio Team (2015); Spada et al. (2012); Trapletti & Hornik (2016); Wickham & Chang (2016); Xie (2014, 2016a, 2016b); Zeileis (2016).
