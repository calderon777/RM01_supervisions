---
title: "Supervision 2"
date-modified: last-modified
date-format: "D MMM YYYY, HH:mm"   # e.g., 07 Oct 2025, 14:32

---

## üß™ Lab 7 ‚Äî The Simple Linear Regression Model

### üéØ Learning outcomes
By the end of this lab you will be able to:

1. Understand the mathematical formulation of the simple linear regression model
2. Estimate regression coefficients using `lm()` in R
3. Interpret the intercept and slope parameters in an applied context
4. Extract and examine regression output using `summary()` and related functions
5. Visualize the fitted regression line on a scatter plot
6. Assess the quality of model fit using residuals and R-squared

---

### üß∞ Prerequisites
- R (‚â• 4.0) and RStudio installed
- Completed Labs 1-6
- Packages (install once):
```r
install.packages(c("remotes", "PoEdata"))
remotes::install_github("ccolonescu/PoEdata")
```

---

**Summary.** In simple linear regression, we use **one independent variable** to predict a **dependent variable**. The relationship is represented by a straight line with an intercept and a slope.

**Introduction.** We explore the fundamentals of the Simple Linear Regression Model, a method to understand the relationship between two variables. This model is foundational for more advanced analyses.

**Key terms.** The **dependent variable** $(y)$ is what we aim to predict; the **independent variable**, $x$, explains $y$.

### The General Model

Assume a linear relationship between the conditional expectation of \(y\) and \(x\):

$$
y_i = \beta_1 + \beta_2 x_i + e_i \tag{1}
$$

- $\beta_1$: intercept 
- $\beta_2$: slope  
- $e_i$: error term with variance $\sigma^2$  
- $i=1,\dots,N$: observation index

The predicted (estimated) value of $y$ given $x$ is:

$$
\hat{y} = b_1 + b_2 x \tag{2}
$$

Assumptions: 

(i) non-random \(x\); 
(ii) constant error variance (homoskedasticity); 
(iii) errors uncorrelated across observations; 
(iv) $E[e_i \mid x_i]=0$.

#### PART A. Example: Food Expenditure vs Income

```r
# Recall to install the package PoEdata once if needed:
install.packages("remotes")
remotes::install_github("ccolonescu/PoEdata")
```

```{r}
library(PoEdata)
data("cps_small")
plot(cps_small$educ, cps_small$wage, xlab="Education", ylab="Wage")

# Food data
data("food")
head(food)
plot(food$income, food$food_exp,
     ylim=c(0, max(food$food_exp)),
     xlim=c(0, max(food$income)),
     xlab="weekly income in $100", ylab="weekly food expenditure in $",
     type="p")
```

#### PART B. Estimating a Linear Regression

For the food data the model is

$$
\text{food\_exp}_i = \beta_1 + \beta_2\,\text{income}_i + e_i.\tag{3}
$$

```{r}
library(PoEdata)
mod1 <- lm(food_exp ~ income, data = food)
b1 <- coef(mod1)[[1]]
b2 <- coef(mod1)[[2]]
smod1 <- summary(mod1)
smod1
```

Add the regression line to the scatter plot:

```{r}
plot(food$income, food$food_exp,
     ylim=c(0, max(food$food_exp)),
     xlim=c(0, max(food$income)),
     xlab="weekly income in $100", ylab="weekly food expenditure in $",
     type = "p")
abline(b1, b2)
```

Retrieve common results:

```{r}
names(mod1)
names(smod1)

mod1$coefficients
smod1$coefficients
```


#### PART C. MCQs

<!-- FEEDBACK SCRIPT: sets .is-correct / .is-incorrect on the clicked label -->
<script>
(function () {
  const norm = s => (s || "").replace(/\s+/g, " ").trim();
  document.addEventListener("change", function (e) {
    const input = e.target.closest('.mcq-we-q input[type="radio"]');
    if (!input) return;

    // find the corresponding label (works for both nested and for= cases)
    const label = input.closest('label') || document.querySelector(`label[for="${input.id}"]`);
    if (!label) return;

    const q = label.closest(".mcq-we-q") || input.closest(".mcq-we-q");
    if (!q) return;

    const keyEl = q.querySelector(".mcq-key");
    if (!keyEl) return;

    const key = norm(keyEl.getAttribute("data-correct"));
    const choice = norm(label.textContent);

    // reset previous feedback in this question
    q.querySelectorAll("label").forEach(l => l.classList.remove("is-correct","is-incorrect"));

    // add feedback to the chosen option
    label.classList.add(choice === key ? "is-correct" : "is-incorrect");
  });
})();
</script>




<div class="mcq-we">

<div class="mcq-we-q">
**1) What relationship does simple linear regression assume between y and x?**  
`r webexercises::longmcq(c("Quadratic", "Logarithmic", answer = "Linear", "Exponential"))`
<span class="mcq-key" data-correct="Linear"></span>
<details class="hint"><summary>Hint</summary> Think straight line: one slope + one intercept.</details>Œ≤
</div>

<div class="mcq-we-q">
**2) In the model y = Œ≤‚ÇÄ + Œ≤‚ÇÅ x + Œµ, what does Œ≤‚ÇÅ represent?**  
`r webexercises::longmcq(c("The average of y", "The error variance", answer = "Expected change in y for a one-unit increase in x", "The intercept"))`
<span class="mcq-key" data-correct="Expected change in y for a one-unit increase in x"></span>
<details class="hint"><summary>Hint</summary> Œîy for +1 in x.</details>
</div>

<div class="mcq-we-q">
**3) In the model y = Œ≤‚ÇÄ + Œ≤‚ÇÅ x + Œµ, what does Œ≤‚ÇÄ represent?**  
`r webexercises::longmcq(c("The slope of x", answer = "The expected value of y when x = 0", "The variance of Œµ", "Correlation between x and y"))`
<span class="mcq-key" data-correct="The expected value of y when x = 0"></span>
<details class="hint"><summary>Hint</summary> Value of y at x = 0.</details>
</div>

<div class="mcq-we-q">
**4) A residual is defined as:**  
`r webexercises::longmcq(c("Predicted y minus observed y", "Observed x minus predicted x", answer = "Observed y minus predicted y", "The fitted value"))`
<span class="mcq-key" data-correct="Observed y minus predicted y"></span>
<details class="hint"><summary>Hint</summary> Actual ‚àí fitted.</details>
</div>

<div class="mcq-we-q">
**5) Ordinary Least Squares (OLS) chooses coefficients to:**  
`r webexercises::longmcq(c("Maximize R¬≤", "Minimize the sum of absolute residuals", answer = "Minimize the sum of squared residuals", "Maximize the likelihood under normal errors only"))`
<span class="mcq-key" data-correct="Minimize the sum of squared residuals"></span>
<details class="hint"><summary>Hint</summary> Squares penalize big errors more.</details>
</div>

<div class="mcq-we-q">
**6) With an intercept in the model, OLS residuals:**  
`r webexercises::longmcq(c("Sum to a positive number", "Sum to a negative number", answer = "Sum to zero", "Sum to the sample mean of y"))`
<span class="mcq-key" data-correct="Sum to zero"></span>
<details class="hint"><summary>Hint</summary> ‚àëe·µ¢ = 0 and X·µÄe = 0 when an intercept is included.</details>
</div>

<div class="mcq-we-q">
**7) In simple OLS with an intercept, the fitted line passes through...**  
`r webexercises::longmcq(c("The origin (0,0)", answer = "The point (xÃÑ, »≥)", "The median of x and y", "The maximum of y"))`
<span class="mcq-key" data-correct="The point (xÃÑ, »≥)"></span>
<details class="hint"><summary>Hint</summary> It goes through the means.</details>
</div>

<div class="mcq-we-q">
**8) Which R function fits a simple linear regression model?**  
`r webexercises::longmcq(c("plot()", "cor()", answer = "lm()", "predict()"))`
<span class="mcq-key" data-correct="lm()"></span>
</div>

<div class="mcq-we-q">
**9) In `summary(mod)`, the p-value for the slope tests:**  
`r webexercises::longmcq(c("H‚ÇÄ: Œ≤‚ÇÅ ‚â† 0", answer = "H‚ÇÄ: Œ≤‚ÇÅ = 0", "H‚ÇÄ: Œ≤‚ÇÄ = 0", "H‚ÇÄ: errors are normal"))`
<span class="mcq-key" data-correct="H‚ÇÄ: Œ≤‚ÇÅ = 0"></span>
</div>

<div class="mcq-we-q">
**10) The p-value is best described as:**  
`r webexercises::longmcq(c("The probability H‚ÇÅ is true", "The probability the estimate is correct", answer = "The probability (under H‚ÇÄ) of a result at least as extreme as observed", "The Type II error rate"))`
<span class="mcq-key" data-correct="The probability (under H‚ÇÄ) of a result at least as extreme as observed"></span>
</div>

<div class="mcq-we-q">
**11) A 95% confidence interval for Œ≤‚ÇÅ is generally:**  
`r webexercises::longmcq(c("Estimate ¬± 1.96 √ó œÉ of y", answer = "Estimate ¬± critical t √ó SE(Œ≤‚ÇÅ)", "Estimate ¬± 2 √ó residual SD", "SE(Œ≤‚ÇÅ) ¬± estimate"))`
<span class="mcq-key" data-correct="Estimate ¬± critical t √ó SE(Œ≤‚ÇÅ)"></span>
</div>

<div class="mcq-we-q">
**12) R¬≤ measures:**  
`r webexercises::longmcq(c("The correlation between x and y", answer = "Proportion of variance in y explained by x", "The average residual size", "The probability the model is correct"))`
<span class="mcq-key" data-correct="Proportion of variance in y explained by x"></span>
</div>

<div class="mcq-we-q">
**13) Adjusted R¬≤ differs from R¬≤ because it:**  
`r webexercises::longmcq(c("Ignores sample size", "Is always larger than R¬≤", answer = "Penalizes adding predictors relative to sample size", "Equals 1 ‚àí R¬≤"))`
<span class="mcq-key" data-correct="Penalizes adding predictors relative to sample size"></span>
</div>

<div class="mcq-we-q">
**14) To make a prediction at x = x‚ÇÄ, you use:**  
`r webexercises::longmcq(c("Œ≤‚ÇÅ √ó x‚ÇÄ", "Œ≤‚ÇÄ √∑ x‚ÇÄ", answer = "≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó x‚ÇÄ", "SE(Œ≤‚ÇÅ) √ó x‚ÇÄ"))`
<span class="mcq-key" data-correct="≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó x‚ÇÄ"></span>
<details class="hint"><summary>Hint</summary> Plug x‚ÇÄ into the fitted line.</details>
</div>

<div class="mcq-we-q">
**15) For OLS to be unbiased, a key assumption is:**  
`r webexercises::longmcq(c("x is normally distributed", "Errors have zero variance", answer = "E[Œµ | x] = 0 (errors have zero mean conditional on x)", "y is standardized"))`
<span class="mcq-key" data-correct="E[Œµ | x] = 0 (errors have zero mean conditional on x)"></span>
</div>

<div class="mcq-we-q">
**16) Homoskedasticity means:**  
`r webexercises::longmcq(c("Errors are perfectly correlated", answer = "Variance of errors is constant across x", "Mean of errors is zero only at xÃÑ", "x has constant variance"))`
<span class="mcq-key" data-correct="Variance of errors is constant across x"></span>
</div>

<div class="mcq-we-q">
**17) In `summary(mod)`, the t value for Œ≤‚ÇÅ equals:**  
`r webexercises::longmcq(c("Œ≤‚ÇÅ", "SE(Œ≤‚ÇÅ)", answer = "Estimate(Œ≤‚ÇÅ) / SE(Œ≤‚ÇÅ)", "1 ‚àí p-value"))`
<span class="mcq-key" data-correct="Estimate(Œ≤‚ÇÅ) / SE(Œ≤‚ÇÅ)"></span>
<details class="hint"><summary>Hint</summary> Signal √∑ noise for the slope estimate.</details>
</div>

<div class="mcq-we-q">
**18) The units of Œ≤‚ÇÅ are best described as:**  
`r webexercises::longmcq(c("Units of y", answer = "Units of y per unit of x", "Unitless", "Units of x per unit of y"))`
<span class="mcq-key" data-correct="Units of y per unit of x"></span>
</div>

<div class="mcq-we-q">
**19) Rescaling x from dollars to hundreds of dollars will:**  
`r webexercises::longmcq(c("Leave Œ≤‚ÇÅ unchanged", answer = "Rescale Œ≤‚ÇÅ numerically but leave R¬≤ and the t-test for Œ≤‚ÇÅ unchanged", "Change the data but not the model", "Invalidate OLS"))`
<span class="mcq-key" data-correct="Rescale Œ≤‚ÇÅ numerically but leave R¬≤ and the t-test for Œ≤‚ÇÅ unchanged"></span>
</div>

<div class="mcq-we-q">
**20) After fitting `mod <- lm(y ~ x, d)`, the fitted value at x‚ÇÄ is:**  
`r webexercises::longmcq(c("residuals(mod)[x==x‚ÇÄ]", answer = "predict(mod, newdata = data.frame(x = x‚ÇÄ))", "coef(mod)['x']", "fitted(mod)[1] always"))`
<span class="mcq-key" data-correct="predict(mod, newdata = data.frame(x = x‚ÇÄ))"></span>
<details class="hint"><summary>Hint</summary> Use <code>predict()</code> with a small data frame for x‚ÇÄ.</details>
</div>

</div>


---

üèÅ  *End of lab 7*
üõë Remember to save your script üíæ

---

## üß™ Lab 8 ‚Äî Prediction with the Linear Regression Model

### üéØ Learning outcomes
By the end of this lab you will be able to:

1. Generate point predictions and quantify their uncertainty using R's prediction framework
2. Distinguish between confidence intervals for average outcomes and prediction intervals for individual cases, and select the appropriate interval for applied research contexts
3. Assess the reliability of coefficient estimates through their variance-covariance structure
4. Recognize when predictions involve extrapolation beyond observed data and understand the associated risks
5. Handle non-linear relationships through quadratic and log-linear specifications

---

### üß∞ Prerequisites
**Knowledge:**
- Understanding of simple linear regression (Lab 7)
- Familiarity with residuals, fitted values, and R¬≤
- Basic probability concepts (sampling distributions, standard errors)

**Technical:**
- R (‚â• 4.0) and RStudio installed
- Completed Labs 1-7
- Required packages (install if needed):

```r
install.packages(c("remotes", "PoEdata")) 
remotes::install_github("ccolonescu/PoEdata")
library(PoEdata)
```

**Datasets:**

food - household food expenditure and income (40 observations)
br - Baton Rouge house prices and characteristics (1,080 observations)
---

::: {.callout-note}
Notation reminder: We use Greek letters (Œ≤1,Œ≤2‚Äã) for true population parameters and Latin letters (b1,b2‚Äã) for their sample estimates.

≈∑: predicted y value

$\hat{\beta}$ or $\hat{b}$ are sometimes used interchangeably with b for estimates
:::

In Lab 7, we learned how to estimate relationships, But estimation is only half the story. In applied research, we often need to make predictions. In applied land economics research, prediction is crucial‚Äîwhether forecasting property values, estimating rental yields, or projecting land use changes. This lab equips you with the tools to make robust predictions and quantify their uncertainty. Let's begin with the basic prediction workflow.

Once we have estimated our regression coefficients $b_1$ (intercept) and $b_2$ (slope), we can use them to predict food expenditure for any given income level using the fitted regression equation (Eq. 2).

$$
\hat{y} = b_1 + b_2 x \tag{2}
$$

‚ö†Ô∏è**Unit conversion:** In the following R script "income = \$2000" is **"income = 20"** (the data is in hundreds of dollars).

```{r}
#### Step 1: Fit the model (recap from Lab 7)

library(PoEdata)
data("food")

# Estimate the food expenditure model
mod1 <- lm(food_exp ~ income, data = food)

# Review estimates
summary(mod1)

#### Step 2: Create a data frame with target income values

# Scenario: Predict food expenditure for three household types
# Note: income is in $100s, so divide actual income by 100
newx <- data.frame(income = c(20, 25, 27))

#### Step 3: Generate predictions

# The predict() function takes two arguments:
#   1. A fitted model object (mod1)
#   2. A data frame with new x-values (must have same column names as original data)
yhat <- predict(mod1, newx)

# Give friendly names to each prediction so the output is   easy to read
names(yhat) <- c("Low income = $2000", "Median income = $2500", "High income = $2700")

# Show the predictions
yhat

#### Step 4: Interpret the results

# A household earning $2,000/week is predicted to spend $287.60 on food per week.
# This represents about 14.4% of their income (287.6/2000).

# For the median household ($2,500/week), predicted food expenditure is $338.70,
# or about 13.5% of income.

# Observation: The proportion of income spent on food decreases as income rises‚Äî
# this is consistent with Engel's Law from economics.
```

**What's Missing? Uncertainty!**

These are **point predictions**‚Äîour single best guess at each income level. But how confident are we?

Consider the household earning $2,000/week:
- We predict they'll spend $287.60 on food
- But there's sampling uncertainty (our $b_1$ and $b_2$ are estimates)
- And individual variation (not all $2,000/week households spend exactly $287.60)

**Two types of intervals address these concerns:**

1. **Confidence intervals** answer: "What's the average food expenditure for *all* households at this income level?"
2. **Prediction intervals** answer: "What might *one specific* household at this income level spend?"

---

### Understanding Prediction Uncertainty Through Sampling Variability

Our predictions depend entirely on our estimated coefficients ($b_1, b_2$). But these are *sample estimates*‚Äîif we collected a different dataset, we'd get different values and therefore different predictions.

Before making predictions, it's important to understand that our coefficient estimates vary across samples. This matters for land economics applications: if we're advising on property valuations, we need to know how sensitive our predictions are to sampling variation.

::: {.callout-tip}
### What is "sampling with replacement"?
Imagine our 40 observations are numbered balls in an urn. We draw a ball, record its data, **put it back**, then draw again. Some observations may appear multiple times in a bootstrap sample; others not at all. This mimics the randomness of drawing a new sample from the population.
:::

The following bootstrap exercise (simulating sampling variability) demonstrates this variability by repeatedly resampling our data and re-estimating the model:

```{r}
N <- nrow(food)   # observations
C <- 50           # repeats
S <- 38           # subsample size

sumb2 <- 0        # sum of slopes
for (i in 1:C){
  set.seed(3*i)   # reproducible
  # Draw a bootstrap sample (with replacement)
  subsample <- food[sample(1:N, S, replace = TRUE), ]  # bootstrap draw
  # Fit the model on this bootstrap sample
  mod2 <- lm(food_exp ~ income, data = subsample)      
  sumb2 <- sumb2 + coef(mod2)[2]                       # store slope Œ≤2
}

print(sumb2 / C, digits = 3)   # average slope
```

Compare this bootsrap average (repeated samples) witht the original OLS regression output (from `mod1`) for $b_2$. They should be similar, confirming that OLS is unbiased.


### Estimated Variances and Covariance of Coefficients

The variance-covariance matrix tells us two crucial things:
1. **Variances (diagonal):** How much each coefficient estimate varies
2. **Covariances (off-diagonal):** How the estimates move together

This matters for prediction because uncertainty in $\hat{y}$ depends on uncertainty in *both* coefficients *and* their correlation. For land valuation models, ignoring this covariance can lead to overconfident predictions.

::: {.callout-warning}
**Common Error in Applied Work**
Many analysts incorrectly calculate prediction variance as just $\text{Var}(b_1) + x_0^2 \cdot \text{Var}(b_2)$, omitting the covariance term. This can make predictions appear less precise than they actually are.
:::

The following R script extracts estimated variances and covariances from the object `mod 1`.

```{r}
# coef variance‚Äìcovariance matrix.
# This is a 2√ó2 symmetric matrix:
# - Top-left (1884.44): Var(b‚ÇÅ), variance of intercept
# - Bottom-right (4.38): Var(b‚ÇÇ), variance of slope  
# - Off-diagonals (-87.78): Cov(b‚ÇÅ,b‚ÇÇ), covariance between coefficients
vcov(mod1)                       

#### Extract individual components
(varb1   <- vcov(mod1)[1, 1])        # Var(B1), intercept variance
(varb2   <- vcov(mod1)[2, 2])        # Var(B2), slope variance
(covb1b2 <- vcov(mod1)[1, 2])        # Cov(B1, B2), covariance
```

Having established how to assess coefficient stability, we now turn to cases where linear relationships are insufficient to capture real-world patterns in land markets.

### Non-Linear Relationships - When Straight Lines Don't Fit

In practice, many land economic relationships are non-linear:
- Property prices don't increase linearly with size (diminishing returns to scale)
- Land values may have threshold effects near transport nodes
- Agricultural productivity often follows diminishing returns

We'll explore two common approaches **Quadratic Models** and **Log-Linear Models**. 

#### PART A. Quadratic model - for U-shaped or inverted-U relationships

$$
y_i = \beta_1 + \beta_2 x_i^2 + e_i.\tag{5}
$$

```{r}
library(PoEdata)            # load package
data(br)                    # dataset

mod3 <- lm(price ~ I(sqft^2), data = br)  # fit model
b1 <- coef(mod3)[1]        # intercept
b2 <- coef(mod3)[2]        # coeff on sqft^2

sqftx <- c(2000, 4000, 6000)              # evaluation points
pricex <- b1 + b2 * sqftx^2               # predicted price
DpriceDsqft <- 2 * b2 * sqftx             # marginal effect d(price)/d(sqft)
elasticity <- (DpriceDsqft * sqftx) / pricex  # elasticity

b1; b2; DpriceDsqft; elasticity           # output
```

Plotting two alternatives for the quadratic fit:

```{r}
mod31 <- lm(price ~ I(sqft^2), data = br)                # fit
plot(br$sqft, br$price, col = "grey",
     xlab = "Total square feet", ylab = "Sale price, $") # scatter

b <- coef(mod31)                                         # [intercept], sqft^2
curve(b[1] + b[2]*x^2,
      from = min(br$sqft), to = max(br$sqft),            # range
      add = TRUE, lwd = 2)                               # fitted curve

```

```{r}
ordat <- br[order(br$sqft), ]                             # sort by sqft
mod31 <- lm(price ~ I(sqft^2), data = ordat)              # fit model on sorted data

plot(br$sqft, br$price, col = "grey",                     # scatter
     main = "Dataset ordered by 'sqft'",
     xlab = "Total square feet", ylab = "Sale price, $")

lines(fitted(mod31) ~ ordat$sqft)                         # add fitted curve
```

#### PART B. Log-Linear Models - for proportional (percentage) relationships

$$
\log(y_i) = \beta_1 + \beta_2 x_i + e_i.\tag{6}
$$

```{r}
hist(br$price)                # price distribution
hist(log(br$price))           # log-price distribution

mod4 <- lm(log(price) ~ sqft, data = br)  # log-linear fit
b1 <- coef(mod4)[1]          # intercept
b2 <- coef(mod4)[2]          # slope

# Back-transform fitted curve to price scale
ordat <- br[order(br$sqft), ]                     # sort by sqft
mod4  <- lm(log(price) ~ sqft, data = ordat)      # refit (ordered)
plot(br$sqft, br$price, col = "grey",
     xlab = "Total square feet", ylab = "Sale price, $")  # scatter
lines(exp(fitted(mod4)) ~ ordat$sqft, lwd = 2)    # exp of fitted log-price

# ‚ö†Ô∏è Important: Simply exponentiating log predictions (exp(fitted)) gives the median, not the mean. For mean predictions, apply a smearing correction
```

Elasticity and marginal effect at the median price:

```{r}
pricex <- median(br$price)                                   # target price (median)
sqftx  <- (log(pricex) - coef(mod4)[1]) / coef(mod4)[2]      # back out sqft from log model
(DyDx <- pricex * coef(mod4)[2])                              # marginal effect d(price)/d(sqft) = b2 * price
(elasticity <- sqftx * coef(mod4)[2])                         # elasticity = (dP/dx)*(x/P) = b2 * sqft

```

Multiple points:

```{r}
b1 <- coef(mod4)[1]                     # intercept of log(price) ~ sqft
b2 <- coef(mod4)[2]                     # slope wrt sqft

sqftx  <- c(2000, 3000, 4000)           # sqft points
pricex <- c(100000, exp(b1 + b2*sqftx)) # prices: first fixed at 100k; others from model

sqftx  <- (log(pricex) - b1) / b2       # implied sqft from each price (now length = length(pricex))
(elasticities <- b2 * sqftx)            # elasticity = (dP/dx)*(x/P) = b2 * sqft

```

üí° **Land Economics Application**
In rental valuation, we rarely observe rental income for vacant plots. Prediction intervals help quantify the uncertainty in forecasted rents, which is crucial for development feasibility analysis.

‚ö†Ô∏è **Common Mistake**
Students often confuse confidence intervals (for the average) with prediction intervals (for individuals). In property valuation, this distinction matters: are you estimating the average price for houses of this type, or the likely sale price of one specific house?


#### PART C. MCQs

<div class="mcq-we">

<div class="mcq-we-q">
**1) Using B‚ÇÄ and B‚ÇÅ primarily helps to:**  
`r webexercises::longmcq(c("Test normality of residuals", "Estimate œÉ¬≤", answer = "Predict E[y|x]", "Standardize x and y"))`
<span class="mcq-key" data-correct="Predict E[y|x]"></span>
<details class="hint"><summary>Hint</summary> Plug x into the fitted line.</details>
</div>

<div class="mcq-we-q">
**2) The function lm() is used to:**  
`r webexercises::longmcq(c("Simulate data", "Draw histograms", answer = "Estimate a linear model", "Compute correlation only"))`
<span class="mcq-key" data-correct="Estimate a linear model"></span>
<details class="hint"><summary>Hint</summary> It returns coefficients and a model object.</details>
</div>

<div class="mcq-we-q">
**3) The function predict() mainly:**  
`r webexercises::longmcq(c("Fits a model", "Computes residuals only", answer = "Estimates y-hat for new data", "Sorts a data frame"))`
<span class="mcq-key" data-correct="Estimates y-hat for new data"></span>
<details class="hint"><summary>Hint</summary> You pass newdata.</details>
</div>

<div class="mcq-we-q">
**4) Coefficients are random because they:**  
`r webexercises::longmcq(c("Depend only on fixed formulas", "Are set by the user", answer = "Depend on the sample", "Don‚Äôt change across samples"))`
<span class="mcq-key" data-correct="Depend on the sample"></span>
</div>

<div class="mcq-we-q">
**5) Random subsamples help to:**  
`r webexercises::longmcq(c("Reduce file size", "Guarantee higher R¬≤", answer = "Evaluate stability/variability", "Eliminate outliers always"))`
<span class="mcq-key" data-correct="Evaluate stability/variability"></span>
</div>

<div class="mcq-we-q">
**6) vcov(model) returns:**  
`r webexercises::longmcq(c("Residuals", "Fitted values", answer = "Variances and covariances of coefficients", "Confidence intervals for y-hat"))`
<span class="mcq-key" data-correct="Variances and covariances of coefficients"></span>
</div>

<div class="mcq-we-q">
**7) Using data.frame() with predict() is to:**  
`r webexercises::longmcq(c(answer = "Provide new x values with correct column names", "Shuffle the rows", "Change variable types to character", "Compute R¬≤"))`
<span class="mcq-key" data-correct="Provide new x values with correct column names"></span>
</div>

<div class="mcq-we-q">
**8) To request standard errors from predict.lm you set:**  
`r webexercises::longmcq(c("stderr = TRUE", "se = TRUE", answer = "se.fit = TRUE", "ci = TRUE"))`
<span class="mcq-key" data-correct="se.fit = TRUE"></span>
<details class="hint"><summary>Hint</summary> It returns fit and se.fit.</details>
</div>

<div class="mcq-we-q">
**9) For a confidence interval for the mean response at x‚ÇÄ, use:**  
`r webexercises::longmcq(c("interval = 'pi'", "bands = 'mean'", answer = "interval = 'confidence'", "type = 'mean'"))`
<span class="mcq-key" data-correct="interval = 'confidence'"></span>
<details class="hint"><summary>Hint</summary> CI for E[y|x‚ÇÄ].</details>
</div>

<div class="mcq-we-q">
**10) For an interval predicting an individual future y at x‚ÇÄ, use:**  
`r webexercises::longmcq(c("interval = 'confidence'", answer = "interval = 'prediction'", "level = 'future'", "type = 'response'"))`
<span class="mcq-key" data-correct="interval = 'prediction'"></span>
<details class="hint"><summary>Hint</summary> Includes error variance.</details>
</div>

<div class="mcq-we-q">
**11) Prediction intervals are typically:**  
`r webexercises::longmcq(c("Narrower than confidence intervals", answer = "Wider than confidence intervals", "Identical to confidence intervals", "Unrelated to residual variance"))`
<span class="mcq-key" data-correct="Wider than confidence intervals"></span>
</div>

<div class="mcq-we-q">
**12) confint(model) returns:**  
`r webexercises::longmcq(c("Intervals for y-hat", answer = "Confidence intervals for coefficients", "Prediction intervals for new y", "Variance of residuals"))`
<span class="mcq-key" data-correct="Confidence intervals for coefficients"></span>
</div>

<div class="mcq-we-q">
**13) For models with multiple predictors, newdata must:**  
`r webexercises::longmcq(c(answer = "Contain columns matching model terms and types", "Be a vector in any order", "Include only the response", "Be sorted by the response"))`
<span class="mcq-key" data-correct="Contain columns matching model terms and types"></span>
<details class="hint"><summary>Hint</summary> Names must match the formula‚Äôs variables.</details>
</div>

<div class="mcq-we-q">
**14) In a log-linear model log(y) ~ x, to back-transform the mean prediction you should:**  
`r webexercises::longmcq(c("Use only exp(eta)", answer = "Multiply exp(eta) by a smearing factor", "Square the fitted values", "Add residual SD then exponentiate"))`
<span class="mcq-key" data-correct="Multiply exp(eta) by a smearing factor"></span>
<details class="hint"><summary>Hint</summary> Duan‚Äôs correction: mean(exp(ŒµÃÇ)).</details>
</div>

<div class="mcq-we-q">
**15) A bootstrap (or resampling with replacement) in R uses:**  
`r webexercises::longmcq(c("sample(x, replace = FALSE)", answer = "sample(x, replace = TRUE)", "sort(x)", "order(x)"))`
<span class="mcq-key" data-correct="sample(x, replace = TRUE)"></span>
</div>

<div class="mcq-we-q">
**16) set.seed(123) is used to:**  
`r webexercises::longmcq(c("Speed up lm()", "Normalize variables", answer = "Make random subsamples reproducible", "Center predictors"))`
<span class="mcq-key" data-correct="Make random subsamples reproducible"></span>
</div>

<div class="mcq-we-q">
**17) For out-of-sample performance, a good metric is:**  
`r webexercises::longmcq(c("Training R¬≤", answer = "Test RMSE", "Number of predictors", "Intercept size"))`
<span class="mcq-key" data-correct="Test RMSE"></span>
<details class="hint"><summary>Hint</summary> Evaluate on held-out data.</details>
</div>

<div class="mcq-we-q">
**18) Extrapolation risk means:**  
`r webexercises::longmcq(c("Predictions are always unbiased", answer = "x‚ÇÄ lies outside the observed x range so predictions can be unreliable", "R¬≤ increases automatically", "CI and PI become identical"))`
<span class="mcq-key" data-correct="x‚ÇÄ lies outside the observed x range so predictions can be unreliable"></span>
</div>

<div class="mcq-we-q">
**19) Variance of the predicted mean at x‚ÇÄ depends on:**  
`r webexercises::longmcq(c("Only œÉ¬≤", "Only the intercept", answer = "x‚ÇÄ·µÄ Var(BÃÇ) x‚ÇÄ", "Only sample size"))`
<span class="mcq-key" data-correct="x‚ÇÄ·µÄ Var(BÃÇ) x‚ÇÄ"></span>
<details class="hint"><summary>Hint</summary> Use the variance‚Äìcovariance of coefficients.</details>
</div>

<div class="mcq-we-q">
**20) When predicting with factors, newdata must:**  
`r webexercises::longmcq(c(answer = "Use levels seen in the training data", "Introduce new unseen levels freely", "Coerce factors to numeric", "Omit the factor columns"))`
<span class="mcq-key" data-correct="Use levels seen in the training data"></span>
<details class="hint"><summary>Hint</summary> Unseen levels cause errors in predict().</details>
</div>

</div>

---

üèÅ  *End of lab 8*
üõë Remember to save your script üíæ

---

## üß™ Lab 9 ‚Äî Hypothesis Test, p‚ÄëValue & Testing Linear Combinations

### üéØ Learning Outcomes
By the end of this lab you will be able to:

- Formulate null and alternative hypotheses for regression coefficients
- Compute test statistics and compare them to critical values
- Calculate and interpret p-values for one-tailed and two-tailed tests
- Understand the relationship between confidence intervals and hypothesis tests
- Test linear combinations of parameters (e.g., predictions at specific x values)
- Apply hypothesis testing to real-world land economics questions

### Hypothesis Tests
üß∞ Prerequisites
Knowledge:

Understanding of simple linear regression (Lab 7)
Familiarity with coefficient estimates and standard errors (Lab 8)
Basic understanding of statistical inference (confidence intervals, significance levels)

Technical:

R (‚â• 4.0) and RStudio installed
Completed Labs 1-8
Required packages:

```r
install.packages(c("remotes", "PoEdata")) 
remotes::install_github("ccolonescu/PoEdata")
library(PoEdata)
```

---

### Introduction: Why Hypothesis Testing Matters

In Lab 7, we estimated relationships. In Lab 8, we made predictions. But how do we know if our findings are statistically meaningful?
Consider our food expenditure model:

We estimated that food spending increases by $10.21 for every $100 increase in income
But is this effect real, or could it be due to random chance?
Could the true effect actually be zero (no relationship)?

Hypothesis testing provides a formal framework to answer these questions.

üí° Land Economics Application
When valuing agricultural land, we might find that proximity to roads increases land value by ¬£500 per meter. But before advising clients, we need to know: Is this effect statistically significant, or could it be sampling noise?


#### PART A - Hypothesis Tests for Individual Coefficients

We start with two competing claims:
$$
H_0: \beta_k = c, \qquad H_A: \beta_k \ne c.
$$

**Test statistic:**

Under the null hypothesis, the test statistic follows a t-distribution:
$$
t = \frac{b_k - c}{\operatorname{se}(b_k)},\quad t \sim t_{N-2}.\tag{6}
$$

where:

$b_k$‚Äã is our estimated coefficient

$c$ is the hypothesized value (zero in our case)

$\operatorname{se}(b_k)$ is the standard error of $b_k$

$N‚àíK$ is degrees of freedom (N observations, K parameters)

**Intuition:** This ratio measures how many standard errors our estimate is away from the hypothesized value.

**Example 1:** Two-Tailed Test (Is there any effect?) 
**Question:** Does income affect food expenditure? (i.e., is $Œ≤2‚â†0$?)


```{r}
# Set significance level
alpha <- 0.05

# Load data and estimate model
library(PoEdata); library(xtable); library(knitr)
data("food")
mod1 <- lm(food_exp ~ income, data=food)
smod1 <- summary(mod1)

# Regression output
table <- data.frame(xtable(mod1))
kable(table, caption="Regression output showing the coefficients")

# Extract coefficient and standard error
b2 <- coef(mod1)[["income"]]
seb2 <- sqrt(vcov(mod1)[2,2])
df  <- df.residual(mod1)

# Compute test statistic
t   <- b2/seb2

# Find critical value for two-tailed test
tcr <- qt(1-alpha/2, df)

t; tcr

# Display results
cat("Test statistic:", round(t, 3), "\n")
cat("Critical value (¬±):", round(tcr, 3), "\n")
cat("Decision:", ifelse(abs(t) > tcr, "Reject H0", "Fail to reject H0"), "\n")
```

**Interpretation:**

- Our test statistic is r round(t, 2), which is much larger than the critical value ¬±r round(tcr, 2)

- We reject the null hypothesis

- There is strong evidence that income affects food expenditure

- Right‚Äëtail and left‚Äëtail versions:

**Optional:** üìä Visualizing the Test:

```{r}
# Create visualization of t-distribution and test
curve(dt(x, df), from = -5, to = 5, 
      main = "Two-Tailed Hypothesis Test",
      xlab = "t-value", ylab = "Density",
      lwd = 2)

# Shade rejection regions
polygon(c(-4, seq(-4, -tcr, 0.01), -tcr),
        c(0, dt(seq(-4, -tcr, 0.01), df), 0),
        col = "red", border = NA, density = 20)
polygon(c(tcr, seq(tcr, 4, 0.01), 4),
        c(0, dt(seq(tcr, 4, 0.01), df), 0),
        col = "red", border = NA, density = 20)

# Mark critical values and test statistic
abline(v = c(-tcr, tcr), col = "red", lty = 2, lwd = 2)

abline(v = t, col = "blue", lwd = 2)

legend("topright", 
       legend = c("Rejection region (Œ±/2 each)", 
                  "Critical values", 
                  "Test statistic"),
       col = c("red", "red", "blue"),
       lty = c(1, 2, 1), lwd = 2, bty = "n")
```

**Example 2:** One-Tailed Tests
Sometimes we have a directional hypothesis based on economic theory.
**Right-Tailed Test:** Is the effect of income on food expenditure greater than $5.50?

$$
H_0: \beta_2 ‚â§ 5.5, \qquad H_A: \beta_2 > 5.5.
$$


```{r}
# Hypothesized value
c <- 5.5

# Compute test statistic
t_right <- (b2 - c)/seb2

# Critical value (right tail only)
tcr_right <- qt(1-alpha, df)

cat("Test statistic:", round(t_right, 3), "\n")
cat("Critical value:", round(tcr_right, 3), "\n")
cat("Decision:", ifelse(t_right > tcr_right, "Reject H0", "Fail to reject H0"), "\n")
```

**Left-Tailed Test:** Is the effect of income on food expenditure less than $15?

$$
H_0: \beta_2 ‚â• 15, \qquad H_A: \beta_2 < 15.
$$


```{r}
# Left-tail: H0: beta2 >= 15; HA: beta2 < 15
c <- 15
t_left <- (b2 - c)/seb2
tcr_left <- qt(alpha, df)

c(t_right=t_right, tcr_right=tcr_right, t_left=t_left, tcr_left=tcr_left)

cat("Test statistic:", round(t_left, 3), "\n")
cat("Critical value:", round(tcr_left, 3), "\n")
cat("Decision:", ifelse(t_left < tcr_left, "Reject H0", "Fail to reject H0"), "\n")
```

‚ö†Ô∏è **Common Mistake:** Students often confuse the direction of the inequality in $H_0$‚Äã and $H_A$. Remember: the alternative hypothesis represents what you're trying to find evidence *for*.

---


#### PART B - The p‚ÄëValue Approach

The p-value represents the probability of observing a t-statistic as extreme as, or more extreme than, the one obtained from the sample, assuming the null hypothesis is true. In other words, it measures how compatible the data are with the null hypothesis. We reject the null hypothesis when the p-value is smaller than the chosen significance level (e.g., 0.05).

For a right-tailed test, the p-value corresponds to the area to the right of the calculated t-statistic.
For a left-tailed test, it is the area to the left of the calculated t-statistic.
For a two-tailed test, the total p-value is divided equally between both tails‚Äîp/2 in the left tail and p/2 in the right tail.

In R, p-values are obtained using the function pt(t, df), where t is the calculated t-ratio and df is the model‚Äôs degrees of freedom. For two-tailed tests, the result should be adjusted to account for both tails, typically using 2*(1 - pt(abs(t), df)).

*Decision rule:* 
Reject $H_0$‚Äã if $p-value<Œ±p$
Fail to reject $H_0$‚Äã if $p-value‚â•Œ±p$

::: {.callout-important}
### Common Significance Levels
- **Œ± = 0.05** (most common): We're willing to accept a 5% chance of incorrectly rejecting H‚ÇÄ
- **Œ± = 0.01** (more conservative): Only 1% chance of Type I error
- **Œ± = 0.10** (more lenient): Accept 10% chance of error

**The smaller the p-value, the stronger the evidence against H‚ÇÄ**
:::

**Computing p-Values:**
$F_t$ stands for the cumulative distribution function (CDF) of the t-distribution.

Right‚Äëtail: $p = 1 - F_t(t)$ (probability in the upper tail)

Left‚Äëtail: $p = F_t(t)$ (probability in the lower tail) 

Two‚Äëtail: $p = 2 * (1-F_t(|t|)$ (probability in both tails)


**Example 1: Right-Tail Test**
**Question:** Is the marginal effect of income on food expenditure *greater than* $5.50?

$$H_0: \beta_2 \leq 5.5 \quad \text{vs} \quad H_A: \beta_2 > 5.5$$

```{r}
# Right-tail test (H0: Œ≤2 ‚â§ 5.5)
c <- 5.5
t <- (b2-c)/seb2
p_right <- 1-pt(t, df)

cat("=== RIGHT-TAIL TEST ===\n")
cat("H0: Œ≤2 ‚â§", c, "  (effect is at most $5.50)\n")
cat("HA: Œ≤2 >", c, "  (effect exceeds $5.50)\n\n")
cat("Test statistic:", round(t, 3), "\n")
cat("p-value:", round(p_right, 4), "\n")
cat("Decision at Œ±=0.05:", ifelse(p_right < 0.05, "‚úì REJECT H0", "‚úó FAIL TO REJECT H0"), "\n")
cat("\nInterpretation:", ifelse(p_right < 0.05, 
           "Strong evidence that Œ≤2 > 5.5 (the effect exceeds $5.50)", "Insufficient evidence that Œ≤2 > 5.5"), "\n\n")
```

**Example 2: Left-Tail Test**

**Question:** Is the marginal effect of income on food expenditure *less than* $15?

$$H_0: \beta_2 \geq 15 \quad \text{vs} \quad H_A: \beta_2 < 15$$

```{r}
# Left-tail test (H0: Œ≤2 ‚â• 15)
c <- 15
t <- (b2-c)/seb2
p_left <- pt(t, df)

cat("=== LEFT-TAIL TEST ===\n")
cat("H0: Œ≤2 ‚â•", c, "  (effect is at least $15)\n")
cat("HA: Œ≤2 <", c, "  (effect is below $15)\n\n")
cat("Test statistic:", round(t, 3), "\n")
cat("p-value:", round(p_left, 4), "\n")
cat("Decision at Œ±=0.05:", ifelse(p_left < 0.05, "‚úì REJECT H0", "‚úó FAIL TO REJECT H0"), "\n")
cat("\nInterpretation:", 
    ifelse(p_left < 0.05, 
           "Strong evidence that Œ≤2 < 15 (the effect is below $15)",
           "Insufficient evidence that Œ≤2 < 15"), "\n\n")
```

**Example 3: Two-Tail Test (Most Common)**

**Question:** Is there *any* relationship between income and food expenditure?

$$H_0: \beta_2 = 0 \quad \text{vs} \quad H_A: \beta_2 \neq 0$$

```{r}
# Two-tail test (H0: Œ≤2 = 0)
c <- 0  
t <- (b2-c)/seb2
p_two <- 2*(1-pt(abs(t), df))

cat("=== TWO-TAIL TEST ===\n")
cat("H0: Œ≤2 = 0  (no relationship)\n")
cat("HA: Œ≤2 ‚â† 0  (income affects food expenditure)\n\n")
cat("Test statistic:", round(t, 3), "\n")
cat("p-value:", format.pval(p_two, digits = 4), "\n")
cat("Decision at Œ±=0.05:", ifelse(p_two < 0.05, "‚úì REJECT H0", "‚úó FAIL TO REJECT H0"), "\n")
cat("\nInterpretation:", 
    ifelse(p_two < 0.05, 
           "Strong evidence of a relationship between income and food expenditure",
           "Insufficient evidence of a relationship"), "\n\n")
```

---

::: {.callout-tip}
### Summary of All Three Tests
```text
SUMMARY TABLE
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
Test Type        | H0          | p-value | Decision
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Right-tail (>)   | Œ≤2 ‚â§ 5.5    | %.4f    | if p_right < 0.05, "REJECT"
Left-tail (<)    | Œ≤2 ‚â• 15     | %.4f    | if p_left < 0.05, "REJECT"
Two-tail (‚â†)     | Œ≤2 = 0      | %.4f    | if p_two < 0.05, "REJECT"
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
```
:::

üí° Interpretation Guide:

$p < 0.01$: Very strong evidence against $H_0$

‚Äã
$0.01 ‚â§ p < 0.05$: Strong evidence against $H_0$

‚Äã
$0.05 ‚â§ p < 0.10$: Weak evidence against $H_0$

‚Äã
$p ‚â• 0.10$: Insufficient evidence against $H_0$


‚ö†Ô∏è **Common Student Mistakes to Avoid:**

1. **"p-value is the probability that H‚ÇÄ is true"** ‚Üí ‚ùå WRONG! 
   - Correct: p-value is the probability of getting our data *if* H‚ÇÄ were true

2. **"Fail to reject H‚ÇÄ means H‚ÇÄ is true"** ‚Üí ‚ùå WRONG!
   - Correct: We simply don't have enough evidence to reject it

3. **"Reject H‚ÇÄ means HA is proven true"** ‚Üí ‚ùå WRONG!
   - Correct: We have strong evidence *against* H‚ÇÄ, supporting HA

4. **Confusing the direction in one-tailed tests** ‚Üí Common error!
   - Right-tail: Testing if parameter is *greater than* c
   - Left-tail: Testing if parameter is *less than* c


#### PART C - Testing Linear Combinations of Parameters

We want to test hypotheses about combinations of parameters, not individual coefficients.**Example:** What is the expected food expenditure for a household earning $2,000/week (income = 20 in our units)?

$$
L = E(\text{food\_exp}\mid \text{income}=20) = \beta_1 + 20\,\beta_2
$$

Variance identities:

$$
\operatorname{var}(aX+bY) = a^2\,\operatorname{var}(X) + b^2\,\operatorname{var}(Y) + 2ab\,\operatorname{cov}(X,Y)
$$

$$
\operatorname{var}(b_1+20b_2) = \operatorname{var}(b_1) + 20^2\operatorname{var}(b_2) + 2\cdot20\operatorname{cov}(b_1,b_2)
$$


‚ö†Ô∏è Critical Point: Many students forget the covariance term! Omitting it leads to incorrect standard errors.

```{r}
# Significance level and x-value of interest
alpha <- 0.05
x <- 20

# the model
m1 <- lm(food_exp ~ income, data=food)


# Extract coefficients and variance-covariance matrix
b1 <- m1$coef[1]
b2 <- m1$coef[2]
varb1 <- vcov(m1)[1,1]
varb2 <- vcov(m1)[2,2]
covb1b2 <- vcov(m1)[1,2]

# Compute the linear combination and its variance
L <- b1 + b2*x
varL <- varb1 + x^2 * varb2 + 2*x*covb1b2
seL <- sqrt(varL)

# Confidence interval for L
df <- df.residual(m1)
tcr <- qt(1-alpha/2, df)
lowbL <- L - tcr*seL; upbL <- L + tcr*seL
c(L=L, seL=seL, low=lowbL, up=upbL)

cat("Expected food expenditure at income = $2000:\n")
cat("  Point estimate:", round(L, 2), "\n")
cat("  Standard error:", round(seL, 2), "\n")
cat("  95% CI: [", round(lowbL, 2), ",", round(upbL, 2), "]\n\n")
```

#### Hypothesis Test for Linear Combination

**Question:** Is expected food expenditure significantly different from $250 at this income level?

$$
H_0: L = 250, \qquad H_A: L \ne 250.
$$



```{r}
# Hypothesized value
c <- 250

# Test statistic
t <- (L - c)/seL

# p-value (two-tailed)
p_value <- 2*(1-pt(abs(t), df))
c(t=t, p_value=p_value)

cat("Hypothesis test: H0: L = 250\n")
cat("  Test statistic:", round(t, 3), "\n")
cat("  p-value:", round(p_value, 4), "\n")
cat("  Decision at Œ±=0.05:", ifelse(p_value < 0.05, "Reject H0", "Fail to reject H0"), "\n")
```

**Interpretation:** We fail to reject the null hypothesis. There is insufficient evidence that expected food expenditure differs from $250 for households earning $2,000/week.

#### PART D - MCQs
<div class="mcq-we">
<div class="mcq-we-q">
**1) In hypothesis testing, H‚ÇÄ represents:**  
`r webexercises::longmcq(c("The research hypothesis", "What we hope to prove", answer = "The status quo or claim being tested", "The alternative explanation"))`
<span class="mcq-key" data-correct="The status quo or claim being tested"></span>
<details class="hint"><summary>Hint</summary>Think of it as the "innocent until proven guilty" claim.</details>
</div>
<div class="mcq-we-q">
**2) The test statistic t = (b - c) / se(b) measures:**  
`r webexercises::longmcq(c("The absolute error", "The confidence level", answer = "How many standard errors b is from c", "The sample size"))`
<span class="mcq-key" data-correct="How many standard errors b is from c"></span>
<details class="hint"><summary>Hint</summary>It's a standardized distance.</details>
</div>
<div class="mcq-we-q">
**3) For a two-tailed test at Œ± = 0.05 with df = 38, the critical value is approximately:**  
`r webexercises::longmcq(c("1.645", "1.686", answer = "¬±2.024", "¬±1.96"))`
<span class="mcq-key" data-correct="¬±2.024"></span>
<details class="hint"><summary>Hint</summary>Use qt(0.975, 38) for the upper tail.</details>
</div>
<div class="mcq-we-q">
**4) A p-value of 0.03 means:**  
`r webexercises::longmcq(c("H‚ÇÄ has 3% chance of being true", "There's a 3% chance the result is wrong", answer = "If H‚ÇÄ were true, there's a 3% chance of results this extreme", "The effect size is 3%"))`
<span class="mcq-key" data-correct="If H‚ÇÄ were true, there's a 3% chance of results this extreme"></span>
</div>
<div class="mcq-we-q">
**5) We reject H‚ÇÄ when:**  
`r webexercises::longmcq(c(answer = "p-value < Œ±", "p-value > Œ±", "t-statistic < 0", "Confidence interval includes zero"))`
<span class="mcq-key" data-correct="p-value < Œ±"></span>
</div>
<div class="mcq-we-q">
**6) For a right-tailed test, the p-value is calculated as:**  
`r webexercises::longmcq(c("pt(t, df)", "2 * pt(abs(t), df)", answer = "1 - pt(t, df)", "pt(-t, df)"))`
<span class="mcq-key" data-correct="1 - pt(t, df)"></span>
<details class="hint"><summary>Hint</summary>We want the upper tail probability.</details>
</div>
<div class="mcq-we-q">
**7) A 95% confidence interval for Œ≤‚ÇÇ is [3, 8]. In a two-tailed test at Œ± = 0.05, we would:**  
`r webexercises::longmcq(c("Reject H‚ÇÄ: Œ≤‚ÇÇ = 5", answer = "Fail to reject H‚ÇÄ: Œ≤‚ÇÇ = 5", "Reject H‚ÇÄ: Œ≤‚ÇÇ = 0", "Need more information"))`
<span class="mcq-key" data-correct="Fail to reject H‚ÇÄ: Œ≤‚ÇÇ = 5"></span>
<details class="hint"><summary>Hint</summary>5 is inside the interval.</details>
</div>
<div class="mcq-we-q">
**8) The same confidence interval [3, 8] tells us we would:**  
`r webexercises::longmcq(c("Fail to reject H‚ÇÄ: Œ≤‚ÇÇ = 0", answer = "Reject H‚ÇÄ: Œ≤‚ÇÇ = 0", "Accept H‚ÇÄ: Œ≤‚ÇÇ = 10", "Reject H‚ÇÄ: Œ≤‚ÇÇ = 6"))`
<span class="mcq-key" data-correct="Reject H‚ÇÄ: Œ≤‚ÇÇ = 0"></span>
<details class="hint"><summary>Hint</summary>0 is outside the interval.</details>
</div>
<div class="mcq-we-q">
**9) To test L = Œ≤‚ÇÅ + 20Œ≤‚ÇÇ, we need:**  
`r webexercises::longmcq(c("Only var(Œ≤‚ÇÅ) and var(Œ≤‚ÇÇ)", "Only the covariance", answer = "Var(Œ≤‚ÇÅ), var(Œ≤‚ÇÇ), and cov(Œ≤‚ÇÅ, Œ≤‚ÇÇ)", "Just the point estimate"))`
<span class="mcq-key" data-correct="Var(Œ≤‚ÇÅ), var(Œ≤‚ÇÇ), and cov(Œ≤‚ÇÅ, Œ≤‚ÇÇ)"></span>
</div>
<div class="mcq-we-q">
**10) The variance of L = a¬∑X + b¬∑Y includes the term:**  
`r webexercises::longmcq(c("ab ¬∑ var(X)", "ab ¬∑ var(Y)", answer = "2ab ¬∑ cov(X,Y)", "(a+b)¬≤ ¬∑ var(X)"))`
<span class="mcq-key" data-correct="2ab ¬∑ cov(X,Y)"></span>
<details class="hint"><summary>Hint</summary>Don't forget the "2" and the product ab.</details>
</div>
<div class="mcq-we-q">
**11) vcov(model) returns:**  
`r webexercises::longmcq(c("Only variances", "Only covariances", answer = "A matrix with variances on diagonal and covariances off-diagonal", "Confidence intervals"))`
<span class="mcq-key" data-correct="A matrix with variances on diagonal and covariances off-diagonal"></span>
</div>
<div class="mcq-we-q">
**12) A Type I error occurs when:**  
`r webexercises::longmcq(c("We fail to reject a false H‚ÇÄ", answer = "We reject a true H‚ÇÄ", "Our sample size is too small", "The p-value is large"))`
<span class="mcq-key" data-correct="We reject a true H‚ÇÄ"></span>
<details class="hint"><summary>Hint</summary>It's controlled by Œ±.</details>
</div>
<div class="mcq-we-q">
**13) The significance level Œ± represents:**  
`r webexercises::longmcq(c("The p-value", "The probability H‚ÇÄ is true", answer = "The probability of Type I error", "The power of the test"))`
<span class="mcq-key" data-correct="The probability of Type I error"></span>
</div>
<div class="mcq-we-q">
**14) For H‚ÇÄ: Œ≤‚ÇÇ = 0, if the 95% CI is [2.5, 7.5], then:**  
`r webexercises::longmcq(c("p-value > 0.05", answer = "p-value < 0.05", "We cannot reject H‚ÇÄ", "The estimate is unbiased"))`
<span class="mcq-key" data-correct="p-value < 0.05"></span>
<details class="hint"><summary>Hint</summary>0 is not in the interval, so we reject.</details>
</div>
<div class="mcq-we-q">
**15) In R, to get the p-value for a right-tailed test with t = 2.5 and df = 30:**  
`r webexercises::longmcq(c("pt(2.5, 30)", "2*pt(2.5, 30)", answer = "1 - pt(2.5, 30)", "pt(2.5, 30, lower.tail=TRUE)"))`
<span class="mcq-key" data-correct="1 - pt(2.5, 30)"></span>
</div>
<div class="mcq-we-q">
**16) A larger sample size generally:**  
`r webexercises::longmcq(c("Increases standard errors", answer = "Decreases standard errors", "Doesn't affect hypothesis tests", "Increases the critical value"))`
<span class="mcq-key" data-correct="Decreases standard errors"></span>
<details class="hint"><summary>Hint</summary>More data = more precision.</details>
</div>
<div class="mcq-we-q">
**17) The degrees of freedom for a simple regression (N=40) is:**  
`r webexercises::longmcq(c("40", "39", answer = "38", "37"))`
<span class="mcq-key" data-correct="38"></span>
<details class="hint"><summary>Hint</summary>df = N - K where K=2 (intercept + slope).</details>
</div>
<div class="mcq-we-q">
**18) If |t| < t_critical, we:**  
`r webexercises::longmcq(c("Reject H‚ÇÄ", answer = "Fail to reject H‚ÇÄ", "Accept H‚ÇÅ", "Recalculate the test"))`
<span class="mcq-key" data-correct="Fail to reject H‚ÇÄ"></span>
</div>
<div class="mcq-we-q">
**19) A one-tailed test is appropriate when:**  
`r webexercises::longmcq(c("We don't know the direction", "We want more power", answer = "Theory predicts a specific direction", "The sample size is small"))`
<span class="mcq-key" data-correct="Theory predicts a specific direction"></span>
</div>
<div class="mcq-we-q">
**20) The t-distribution approaches the normal distribution as:**  
`r webexercises::longmcq(c("Œ± decreases", "Œ± increases", "n decreases", answer = "n increases"))`
<span class="mcq-key" data-correct="n increases"></span>
<details class="hint"><summary>Hint</summary>With large df, t ‚âà z.</details>
</div>
</div>

### Summary

- Hypothesis testing provides a formal framework for evaluating claims about parameters
- Test statistics measure how far estimates are from hypothesized values in standard error units
- p-values quantify the strength of evidence against H‚ÇÄ
- Confidence intervals and hypothesis tests are mathematically equivalent
- Linear combinations require accounting for covariances in variance calculations

---

üèÅ  *End of lab 9*
üõë Remember to save your script üíæ

---

---

### References

Acemoglu, D., Laibson, D., & List, J. A. (2015). Microeconomics (2nd ed.). Pearson. pp. 512‚Äì530.
(For conceptual understanding of regression interpretation and causal inference basics.)

Colonescu, C. (2022). Principles of Econometrics with R (Version 2.0). Open Textbook Library. Chapters 7‚Äì9. (Used directly in labs, matches the PoEdata package and R examples.)

Gujarati, D. N., Porter, D. C., & Gunasekar, S. (2017). Basic Econometrics (5th ed.). McGraw-Hill Education. pp. 126‚Äì150, 175‚Äì195. (For theory and assumptions behind simple and multiple linear regression.)

Stock, J. H., & Watson, M. W. (2020). Introduction to Econometrics (4th ed.). Pearson. pp. 97‚Äì121, 145‚Äì165. (For hypothesis testing, p-values, and model diagnostics.)

---

### Formative Test (This test does NOT carry any marks)

![Click to start the MCQs test!](/pics/formative_assessment_1.PNG)
*or link: [MCQs test ](https://forms.microsoft.com/Pages/ResponsePage.aspx?id=RQSlSfq9eUut41R7TzmG6e2v58mrCWNIjQV8bfnU-6tUMDEyV0lGR01JV1BIMEFSTVc1VEZWWkJFRS4u)*