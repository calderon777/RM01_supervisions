---
title: "Supervision 2"
date-modified: last-modified
date-format: "D MMM YYYY, HH:mm"   # e.g., 07 Oct 2025, 14:32

---

## üß™ Lab 7 ‚Äî The Simple Linear Regression Model

### üéØ Learning outcomes
By the end of this lab you will be able to:

1. Understand the mathematical formulation of the simple linear regression model
2. Estimate regression coefficients using `lm()` in R
3. Interpret the intercept and slope parameters in an applied context
4. Extract and examine regression output using `summary()` and related functions
5. Visualize the fitted regression line on a scatter plot
6. Assess the quality of model fit using residuals and R-squared

---

### üß∞ Prerequisites
- R (‚â• 4.0) and RStudio installed
- Completed Labs 1-6
- Packages (install once):
```r
install.packages(c("remotes", "PoEdata"))
remotes::install_github("ccolonescu/PoEdata")
```

---

**Summary.** In simple linear regression, we use **one independent variable** to predict a **dependent variable**. The relationship is represented by a straight line with an intercept and a slope.

**Introduction.** We explore the fundamentals of the Simple Linear Regression Model, a method to understand the relationship between two variables. This model is foundational for more advanced analyses.

**Key terms.** The **dependent variable** $(y)$ is what we aim to predict; the **independent variable**, $x$, explains $y$.

### The General Model

Assume a linear relationship between the conditional expectation of \(y\) and \(x\):

$$
y_i = \beta_1 + \beta_2 x_i + e_i \tag{1}
$$

- $\beta_1$: intercept 
- $\beta_2$: slope  
- $e_i$: error term with variance $\sigma^2$  
- $i=1,\dots,N$: observation index

The predicted (estimated) value of $y$ given $x$ is:

$$
\hat{y} = b_1 + b_2 x \tag{2}
$$

Assumptions: 

(i) non-random \(x\); 
(ii) constant error variance (homoskedasticity); 
(iii) errors uncorrelated across observations; 
(iv) $E[e_i \mid x_i]=0$.

#### PART A. Example: Food Expenditure vs Income

```r
# Recall to install the package PoEdata once if needed:
install.packages("remotes")
remotes::install_github("ccolonescu/PoEdata")
```

```{r}
library(PoEdata)
data("cps_small")
plot(cps_small$educ, cps_small$wage, xlab="Education", ylab="Wage")

# Food data
data("food")
head(food)
plot(food$income, food$food_exp,
     ylim=c(0, max(food$food_exp)),
     xlim=c(0, max(food$income)),
     xlab="weekly income in $100", ylab="weekly food expenditure in $",
     type="p")
```

#### PART B. Estimating a Linear Regression

For the food data the model is

$$
\text{food\_exp}_i = \beta_1 + \beta_2\,\text{income}_i + e_i.\tag{3}
$$

```{r}
library(PoEdata)
mod1 <- lm(food_exp ~ income, data = food)
b1 <- coef(mod1)[[1]]
b2 <- coef(mod1)[[2]]
smod1 <- summary(mod1)
smod1
```

Add the regression line to the scatter plot:

```{r}
plot(food$income, food$food_exp,
     ylim=c(0, max(food$food_exp)),
     xlim=c(0, max(food$income)),
     xlab="weekly income in $100", ylab="weekly food expenditure in $",
     type = "p")
abline(b1, b2)
```

Retrieve common results:

```{r}
names(mod1)
names(smod1)

mod1$coefficients
smod1$coefficients
```


#### PART C. MCQs

<!-- FEEDBACK SCRIPT: sets .is-correct / .is-incorrect on the clicked label -->
<script>
(function () {
  const norm = s => (s || "").replace(/\s+/g, " ").trim();
  document.addEventListener("change", function (e) {
    const input = e.target.closest('.mcq-we-q input[type="radio"]');
    if (!input) return;

    // find the corresponding label (works for both nested and for= cases)
    const label = input.closest('label') || document.querySelector(`label[for="${input.id}"]`);
    if (!label) return;

    const q = label.closest(".mcq-we-q") || input.closest(".mcq-we-q");
    if (!q) return;

    const keyEl = q.querySelector(".mcq-key");
    if (!keyEl) return;

    const key = norm(keyEl.getAttribute("data-correct"));
    const choice = norm(label.textContent);

    // reset previous feedback in this question
    q.querySelectorAll("label").forEach(l => l.classList.remove("is-correct","is-incorrect"));

    // add feedback to the chosen option
    label.classList.add(choice === key ? "is-correct" : "is-incorrect");
  });
})();
</script>




<div class="mcq-we">

<div class="mcq-we-q">
**1) What relationship does simple linear regression assume between y and x?**  
`r webexercises::longmcq(c("Quadratic", "Logarithmic", answer = "Linear", "Exponential"))`
<span class="mcq-key" data-correct="Linear"></span>
<details class="hint"><summary>Hint</summary> Think straight line: one slope + one intercept.</details>Œ≤
</div>

<div class="mcq-we-q">
**2) In the model y = Œ≤‚ÇÄ + Œ≤‚ÇÅ x + Œµ, what does Œ≤‚ÇÅ represent?**  
`r webexercises::longmcq(c("The average of y", "The error variance", answer = "Expected change in y for a one-unit increase in x", "The intercept"))`
<span class="mcq-key" data-correct="Expected change in y for a one-unit increase in x"></span>
<details class="hint"><summary>Hint</summary> Œîy for +1 in x.</details>
</div>

<div class="mcq-we-q">
**3) In the model y = Œ≤‚ÇÄ + Œ≤‚ÇÅ x + Œµ, what does Œ≤‚ÇÄ represent?**  
`r webexercises::longmcq(c("The slope of x", answer = "The expected value of y when x = 0", "The variance of Œµ", "Correlation between x and y"))`
<span class="mcq-key" data-correct="The expected value of y when x = 0"></span>
<details class="hint"><summary>Hint</summary> Value of y at x = 0.</details>
</div>

<div class="mcq-we-q">
**4) A residual is defined as:**  
`r webexercises::longmcq(c("Predicted y minus observed y", "Observed x minus predicted x", answer = "Observed y minus predicted y", "The fitted value"))`
<span class="mcq-key" data-correct="Observed y minus predicted y"></span>
<details class="hint"><summary>Hint</summary> Actual ‚àí fitted.</details>
</div>

<div class="mcq-we-q">
**5) Ordinary Least Squares (OLS) chooses coefficients to:**  
`r webexercises::longmcq(c("Maximize R¬≤", "Minimize the sum of absolute residuals", answer = "Minimize the sum of squared residuals", "Maximize the likelihood under normal errors only"))`
<span class="mcq-key" data-correct="Minimize the sum of squared residuals"></span>
<details class="hint"><summary>Hint</summary> Squares penalize big errors more.</details>
</div>

<div class="mcq-we-q">
**6) With an intercept in the model, OLS residuals:**  
`r webexercises::longmcq(c("Sum to a positive number", "Sum to a negative number", answer = "Sum to zero", "Sum to the sample mean of y"))`
<span class="mcq-key" data-correct="Sum to zero"></span>
<details class="hint"><summary>Hint</summary> ‚àëe·µ¢ = 0 and X·µÄe = 0 when an intercept is included.</details>
</div>

<div class="mcq-we-q">
**7) In simple OLS with an intercept, the fitted line passes through...**  
`r webexercises::longmcq(c("The origin (0,0)", answer = "The point (xÃÑ, »≥)", "The median of x and y", "The maximum of y"))`
<span class="mcq-key" data-correct="The point (xÃÑ, »≥)"></span>
<details class="hint"><summary>Hint</summary> It goes through the means.</details>
</div>

<div class="mcq-we-q">
**8) Which R function fits a simple linear regression model?**  
`r webexercises::longmcq(c("plot()", "cor()", answer = "lm()", "predict()"))`
<span class="mcq-key" data-correct="lm()"></span>
</div>

<div class="mcq-we-q">
**9) In `summary(mod)`, the p-value for the slope tests:**  
`r webexercises::longmcq(c("H‚ÇÄ: Œ≤‚ÇÅ ‚â† 0", answer = "H‚ÇÄ: Œ≤‚ÇÅ = 0", "H‚ÇÄ: Œ≤‚ÇÄ = 0", "H‚ÇÄ: errors are normal"))`
<span class="mcq-key" data-correct="H‚ÇÄ: Œ≤‚ÇÅ = 0"></span>
</div>

<div class="mcq-we-q">
**10) The p-value is best described as:**  
`r webexercises::longmcq(c("The probability H‚ÇÅ is true", "The probability the estimate is correct", answer = "The probability (under H‚ÇÄ) of a result at least as extreme as observed", "The Type II error rate"))`
<span class="mcq-key" data-correct="The probability (under H‚ÇÄ) of a result at least as extreme as observed"></span>
</div>

<div class="mcq-we-q">
**11) A 95% confidence interval for Œ≤‚ÇÅ is generally:**  
`r webexercises::longmcq(c("Estimate ¬± 1.96 √ó œÉ of y", answer = "Estimate ¬± critical t √ó SE(Œ≤‚ÇÅ)", "Estimate ¬± 2 √ó residual SD", "SE(Œ≤‚ÇÅ) ¬± estimate"))`
<span class="mcq-key" data-correct="Estimate ¬± critical t √ó SE(Œ≤‚ÇÅ)"></span>
</div>

<div class="mcq-we-q">
**12) R¬≤ measures:**  
`r webexercises::longmcq(c("The correlation between x and y", answer = "Proportion of variance in y explained by x", "The average residual size", "The probability the model is correct"))`
<span class="mcq-key" data-correct="Proportion of variance in y explained by x"></span>
</div>

<div class="mcq-we-q">
**13) Adjusted R¬≤ differs from R¬≤ because it:**  
`r webexercises::longmcq(c("Ignores sample size", "Is always larger than R¬≤", answer = "Penalizes adding predictors relative to sample size", "Equals 1 ‚àí R¬≤"))`
<span class="mcq-key" data-correct="Penalizes adding predictors relative to sample size"></span>
</div>

<div class="mcq-we-q">
**14) To make a prediction at x = x‚ÇÄ, you use:**  
`r webexercises::longmcq(c("Œ≤‚ÇÅ √ó x‚ÇÄ", "Œ≤‚ÇÄ √∑ x‚ÇÄ", answer = "≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó x‚ÇÄ", "SE(Œ≤‚ÇÅ) √ó x‚ÇÄ"))`
<span class="mcq-key" data-correct="≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó x‚ÇÄ"></span>
<details class="hint"><summary>Hint</summary> Plug x‚ÇÄ into the fitted line.</details>
</div>

<div class="mcq-we-q">
**15) For OLS to be unbiased, a key assumption is:**  
`r webexercises::longmcq(c("x is normally distributed", "Errors have zero variance", answer = "E[Œµ | x] = 0 (errors have zero mean conditional on x)", "y is standardized"))`
<span class="mcq-key" data-correct="E[Œµ | x] = 0 (errors have zero mean conditional on x)"></span>
</div>

<div class="mcq-we-q">
**16) Homoskedasticity means:**  
`r webexercises::longmcq(c("Errors are perfectly correlated", answer = "Variance of errors is constant across x", "Mean of errors is zero only at xÃÑ", "x has constant variance"))`
<span class="mcq-key" data-correct="Variance of errors is constant across x"></span>
</div>

<div class="mcq-we-q">
**17) In `summary(mod)`, the t value for Œ≤‚ÇÅ equals:**  
`r webexercises::longmcq(c("Œ≤‚ÇÅ", "SE(Œ≤‚ÇÅ)", answer = "Estimate(Œ≤‚ÇÅ) / SE(Œ≤‚ÇÅ)", "1 ‚àí p-value"))`
<span class="mcq-key" data-correct="Estimate(Œ≤‚ÇÅ) / SE(Œ≤‚ÇÅ)"></span>
<details class="hint"><summary>Hint</summary> Signal √∑ noise for the slope estimate.</details>
</div>

<div class="mcq-we-q">
**18) The units of Œ≤‚ÇÅ are best described as:**  
`r webexercises::longmcq(c("Units of y", answer = "Units of y per unit of x", "Unitless", "Units of x per unit of y"))`
<span class="mcq-key" data-correct="Units of y per unit of x"></span>
</div>

<div class="mcq-we-q">
**19) Rescaling x from dollars to hundreds of dollars will:**  
`r webexercises::longmcq(c("Leave Œ≤‚ÇÅ unchanged", answer = "Rescale Œ≤‚ÇÅ numerically but leave R¬≤ and the t-test for Œ≤‚ÇÅ unchanged", "Change the data but not the model", "Invalidate OLS"))`
<span class="mcq-key" data-correct="Rescale Œ≤‚ÇÅ numerically but leave R¬≤ and the t-test for Œ≤‚ÇÅ unchanged"></span>
</div>

<div class="mcq-we-q">
**20) After fitting `mod <- lm(y ~ x, d)`, the fitted value at x‚ÇÄ is:**  
`r webexercises::longmcq(c("residuals(mod)[x==x‚ÇÄ]", answer = "predict(mod, newdata = data.frame(x = x‚ÇÄ))", "coef(mod)['x']", "fitted(mod)[1] always"))`
<span class="mcq-key" data-correct="predict(mod, newdata = data.frame(x = x‚ÇÄ))"></span>
<details class="hint"><summary>Hint</summary> Use <code>predict()</code> with a small data frame for x‚ÇÄ.</details>
</div>

</div>


---

üèÅ  *End of lab 7*
üõë Remember to save your script üíæ

---

## üß™ Lab 8 ‚Äî Prediction with the Linear Regression Model

### üéØ Learning outcomes
By the end of this lab you will be able to:

1. Generate point predictions and quantify their uncertainty using R's prediction framework
2. Distinguish between confidence intervals for average outcomes and prediction intervals for individual cases, and select the appropriate interval for applied research contexts
3. Assess the reliability of coefficient estimates through their variance-covariance structure
4. Recognize when predictions involve extrapolation beyond observed data and understand the associated risks
5. Handle non-linear relationships through quadratic and log-linear specifications

---

### üß∞ Prerequisites
**Knowledge:**
- Understanding of simple linear regression (Lab 7)
- Familiarity with residuals, fitted values, and R¬≤
- Basic probability concepts (sampling distributions, standard errors)

**Technical:**
- R (‚â• 4.0) and RStudio installed
- Completed Labs 1-7
- Required packages (install if needed):

```r
install.packages(c("remotes", "PoEdata")) 
remotes::install_github("ccolonescu/PoEdata")
library(PoEdata)
```

**Datasets:**

food - household food expenditure and income (40 observations)
br - Baton Rouge house prices and characteristics (1,080 observations)
---

::: {.callout-note}
Notation reminder: We use Greek letters (Œ≤1,Œ≤2‚Äã) for true population parameters and Latin letters (b1,b2‚Äã) for their sample estimates.

≈∑: predicted y value

$\hat{\beta}$ or $\hat{b}$ are sometimes used interchangeably with b for estimates
:::

In Lab 7, we learned how to estimate relationships, But estimation is only half the story. In applied research, we often need to make predictions. In applied land economics research, prediction is crucial‚Äîwhether forecasting property values, estimating rental yields, or projecting land use changes. This lab equips you with the tools to make robust predictions and quantify their uncertainty. Let's begin with the basic prediction workflow.

Once we have estimated our regression coefficients $b_1$ (intercept) and $b_2$ (slope), we can use them to predict food expenditure for any given income level using the fitted regression equation (Eq. 2).

$$
\hat{y} = b_1 + b_2 x \tag{2}
$$

‚ö†Ô∏è**Unit conversion:** In the following R script "income = \$2000" is **"income = 20"** (the data is in hundreds of dollars).

```{r}
#### Step 1: Fit the model (recap from Lab 7)

library(PoEdata)
data("food")

# Estimate the food expenditure model
mod1 <- lm(food_exp ~ income, data = food)

# Review estimates
summary(mod1)

#### Step 2: Create a data frame with target income values

# Scenario: Predict food expenditure for three household types
# Note: income is in $100s, so divide actual income by 100
newx <- data.frame(income = c(20, 25, 27))

#### Step 3: Generate predictions

# The predict() function takes two arguments:
#   1. A fitted model object (mod1)
#   2. A data frame with new x-values (must have same column names as original data)
yhat <- predict(mod1, newx)

# Give friendly names to each prediction so the output is   easy to read
names(yhat) <- c("Low income = $2000", "Median income = $2500", "High income = $2700")

# Show the predictions
yhat

#### Step 4: Interpret the results

# A household earning $2,000/week is predicted to spend $287.60 on food per week.
# This represents about 14.4% of their income (287.6/2000).

# For the median household ($2,500/week), predicted food expenditure is $338.70,
# or about 13.5% of income.

# Observation: The proportion of income spent on food decreases as income rises‚Äî
# this is consistent with Engel's Law from economics.
```

**What's Missing? Uncertainty!**

These are **point predictions**‚Äîour single best guess at each income level. But how confident are we?

Consider the household earning $2,000/week:
- We predict they'll spend $287.60 on food
- But there's sampling uncertainty (our $b_1$ and $b_2$ are estimates)
- And individual variation (not all $2,000/week households spend exactly $287.60)

**Two types of intervals address these concerns:**

1. **Confidence intervals** answer: "What's the average food expenditure for *all* households at this income level?"
2. **Prediction intervals** answer: "What might *one specific* household at this income level spend?"

---

### Understanding Prediction Uncertainty Through Sampling Variability

Our predictions depend entirely on our estimated coefficients ($b_1, b_2$). But these are *sample estimates*‚Äîif we collected a different dataset, we'd get different values and therefore different predictions.

Before making predictions, it's important to understand that our coefficient estimates vary across samples. This matters for land economics applications: if we're advising on property valuations, we need to know how sensitive our predictions are to sampling variation.

::: {.callout-tip}
### What is "sampling with replacement"?
Imagine our 40 observations are numbered balls in an urn. We draw a ball, record its data, **put it back**, then draw again. Some observations may appear multiple times in a bootstrap sample; others not at all. This mimics the randomness of drawing a new sample from the population.
:::

The following bootstrap exercise (simulating sampling variability) demonstrates this variability by repeatedly resampling our data and re-estimating the model:

```{r}
N <- nrow(food)   # observations
C <- 50           # repeats
S <- 38           # subsample size

sumb2 <- 0        # sum of slopes
for (i in 1:C){
  set.seed(3*i)   # reproducible
  # Draw a bootstrap sample (with replacement)
  subsample <- food[sample(1:N, S, replace = TRUE), ]  # bootstrap draw
  # Fit the model on this bootstrap sample
  mod2 <- lm(food_exp ~ income, data = subsample)      
  sumb2 <- sumb2 + coef(mod2)[2]                       # store slope Œ≤2
}

print(sumb2 / C, digits = 3)   # average slope
```

Compare this bootsrap average (repeated samples) witht the original OLS regression output (from `mod1`) for $b_2$. They should be similar, confirming that OLS is unbiased.


### Estimated Variances and Covariance of Coefficients

The variance-covariance matrix tells us two crucial things:
1. **Variances (diagonal):** How much each coefficient estimate varies
2. **Covariances (off-diagonal):** How the estimates move together

This matters for prediction because uncertainty in $\hat{y}$ depends on uncertainty in *both* coefficients *and* their correlation. For land valuation models, ignoring this covariance can lead to overconfident predictions.

::: {.callout-warning}
**Common Error in Applied Work**
Many analysts incorrectly calculate prediction variance as just $\text{Var}(b_1) + x_0^2 \cdot \text{Var}(b_2)$, omitting the covariance term. This can make predictions appear less precise than they actually are.
:::

The following R script extracts estimated variances and covariances from the object `mod 1`.

```{r}
# coef variance‚Äìcovariance matrix.
# This is a 2√ó2 symmetric matrix:
# - Top-left (1884.44): Var(b‚ÇÅ), variance of intercept
# - Bottom-right (4.38): Var(b‚ÇÇ), variance of slope  
# - Off-diagonals (-87.78): Cov(b‚ÇÅ,b‚ÇÇ), covariance between coefficients
vcov(mod1)                       

#### Extract individual components
(varb1   <- vcov(mod1)[1, 1])        # Var(B1), intercept variance
(varb2   <- vcov(mod1)[2, 2])        # Var(B2), slope variance
(covb1b2 <- vcov(mod1)[1, 2])        # Cov(B1, B2), covariance
```

Having established how to assess coefficient stability, we now turn to cases where linear relationships are insufficient to capture real-world patterns in land markets.

### Non-Linear Relationships - When Straight Lines Don't Fit

In practice, many land economic relationships are non-linear:
- Property prices don't increase linearly with size (diminishing returns to scale)
- Land values may have threshold effects near transport nodes
- Agricultural productivity often follows diminishing returns

We'll explore two common approaches **Quadratic Models** and **Log-Linear Models**. 

#### PART A. Quadratic model - for U-shaped or inverted-U relationships

$$
y_i = \beta_1 + \beta_2 x_i^2 + e_i.\tag{5}
$$

```{r}
library(PoEdata)            # load package
data(br)                    # dataset

mod3 <- lm(price ~ I(sqft^2), data = br)  # fit model
b1 <- coef(mod3)[1]        # intercept
b2 <- coef(mod3)[2]        # coeff on sqft^2

sqftx <- c(2000, 4000, 6000)              # evaluation points
pricex <- b1 + b2 * sqftx^2               # predicted price
DpriceDsqft <- 2 * b2 * sqftx             # marginal effect d(price)/d(sqft)
elasticity <- (DpriceDsqft * sqftx) / pricex  # elasticity

b1; b2; DpriceDsqft; elasticity           # output
```

Plotting two alternatives for the quadratic fit:

```{r}
mod31 <- lm(price ~ I(sqft^2), data = br)                # fit
plot(br$sqft, br$price, col = "grey",
     xlab = "Total square feet", ylab = "Sale price, $") # scatter

b <- coef(mod31)                                         # [intercept], sqft^2
curve(b[1] + b[2]*x^2,
      from = min(br$sqft), to = max(br$sqft),            # range
      add = TRUE, lwd = 2)                               # fitted curve

```

```{r}
ordat <- br[order(br$sqft), ]                             # sort by sqft
mod31 <- lm(price ~ I(sqft^2), data = ordat)              # fit model on sorted data

plot(br$sqft, br$price, col = "grey",                     # scatter
     main = "Dataset ordered by 'sqft'",
     xlab = "Total square feet", ylab = "Sale price, $")

lines(fitted(mod31) ~ ordat$sqft)                         # add fitted curve
```

#### PART B. Log-Linear Models - for proportional (percentage) relationships

$$
\log(y_i) = \beta_1 + \beta_2 x_i + e_i.\tag{6}
$$

```{r}
hist(br$price)                # price distribution
hist(log(br$price))           # log-price distribution

mod4 <- lm(log(price) ~ sqft, data = br)  # log-linear fit
b1 <- coef(mod4)[1]          # intercept
b2 <- coef(mod4)[2]          # slope

# Back-transform fitted curve to price scale
ordat <- br[order(br$sqft), ]                     # sort by sqft
mod4  <- lm(log(price) ~ sqft, data = ordat)      # refit (ordered)
plot(br$sqft, br$price, col = "grey",
     xlab = "Total square feet", ylab = "Sale price, $")  # scatter
lines(exp(fitted(mod4)) ~ ordat$sqft, lwd = 2)    # exp of fitted log-price

# ‚ö†Ô∏è Important: Simply exponentiating log predictions (exp(fitted)) gives the median, not the mean. For mean predictions, apply a smearing correction
```

Elasticity and marginal effect at the median price:

```{r}
pricex <- median(br$price)                                   # target price (median)
sqftx  <- (log(pricex) - coef(mod4)[1]) / coef(mod4)[2]      # back out sqft from log model
(DyDx <- pricex * coef(mod4)[2])                              # marginal effect d(price)/d(sqft) = b2 * price
(elasticity <- sqftx * coef(mod4)[2])                         # elasticity = (dP/dx)*(x/P) = b2 * sqft

```

Multiple points:

```{r}
b1 <- coef(mod4)[1]                     # intercept of log(price) ~ sqft
b2 <- coef(mod4)[2]                     # slope wrt sqft

sqftx  <- c(2000, 3000, 4000)           # sqft points
pricex <- c(100000, exp(b1 + b2*sqftx)) # prices: first fixed at 100k; others from model

sqftx  <- (log(pricex) - b1) / b2       # implied sqft from each price (now length = length(pricex))
(elasticities <- b2 * sqftx)            # elasticity = (dP/dx)*(x/P) = b2 * sqft

```

üí° **Land Economics Application**
In rental valuation, we rarely observe rental income for vacant plots. Prediction intervals help quantify the uncertainty in forecasted rents, which is crucial for development feasibility analysis.

‚ö†Ô∏è **Common Mistake**
Students often confuse confidence intervals (for the average) with prediction intervals (for individuals). In property valuation, this distinction matters: are you estimating the average price for houses of this type, or the likely sale price of one specific house?


#### PART C. MCQs

<div class="mcq-we">

<div class="mcq-we-q">
**1) Using B‚ÇÄ and B‚ÇÅ primarily helps to:**  
`r webexercises::longmcq(c("Test normality of residuals", "Estimate œÉ¬≤", answer = "Predict E[y|x]", "Standardize x and y"))`
<span class="mcq-key" data-correct="Predict E[y|x]"></span>
<details class="hint"><summary>Hint</summary> Plug x into the fitted line.</details>
</div>

<div class="mcq-we-q">
**2) The function lm() is used to:**  
`r webexercises::longmcq(c("Simulate data", "Draw histograms", answer = "Estimate a linear model", "Compute correlation only"))`
<span class="mcq-key" data-correct="Estimate a linear model"></span>
<details class="hint"><summary>Hint</summary> It returns coefficients and a model object.</details>
</div>

<div class="mcq-we-q">
**3) The function predict() mainly:**  
`r webexercises::longmcq(c("Fits a model", "Computes residuals only", answer = "Estimates y-hat for new data", "Sorts a data frame"))`
<span class="mcq-key" data-correct="Estimates y-hat for new data"></span>
<details class="hint"><summary>Hint</summary> You pass newdata.</details>
</div>

<div class="mcq-we-q">
**4) Coefficients are random because they:**  
`r webexercises::longmcq(c("Depend only on fixed formulas", "Are set by the user", answer = "Depend on the sample", "Don‚Äôt change across samples"))`
<span class="mcq-key" data-correct="Depend on the sample"></span>
</div>

<div class="mcq-we-q">
**5) Random subsamples help to:**  
`r webexercises::longmcq(c("Reduce file size", "Guarantee higher R¬≤", answer = "Evaluate stability/variability", "Eliminate outliers always"))`
<span class="mcq-key" data-correct="Evaluate stability/variability"></span>
</div>

<div class="mcq-we-q">
**6) vcov(model) returns:**  
`r webexercises::longmcq(c("Residuals", "Fitted values", answer = "Variances and covariances of coefficients", "Confidence intervals for y-hat"))`
<span class="mcq-key" data-correct="Variances and covariances of coefficients"></span>
</div>

<div class="mcq-we-q">
**7) Using data.frame() with predict() is to:**  
`r webexercises::longmcq(c(answer = "Provide new x values with correct column names", "Shuffle the rows", "Change variable types to character", "Compute R¬≤"))`
<span class="mcq-key" data-correct="Provide new x values with correct column names"></span>
</div>

<div class="mcq-we-q">
**8) To request standard errors from predict.lm you set:**  
`r webexercises::longmcq(c("stderr = TRUE", "se = TRUE", answer = "se.fit = TRUE", "ci = TRUE"))`
<span class="mcq-key" data-correct="se.fit = TRUE"></span>
<details class="hint"><summary>Hint</summary> It returns fit and se.fit.</details>
</div>

<div class="mcq-we-q">
**9) For a confidence interval for the mean response at x‚ÇÄ, use:**  
`r webexercises::longmcq(c("interval = 'pi'", "bands = 'mean'", answer = "interval = 'confidence'", "type = 'mean'"))`
<span class="mcq-key" data-correct="interval = 'confidence'"></span>
<details class="hint"><summary>Hint</summary> CI for E[y|x‚ÇÄ].</details>
</div>

<div class="mcq-we-q">
**10) For an interval predicting an individual future y at x‚ÇÄ, use:**  
`r webexercises::longmcq(c("interval = 'confidence'", answer = "interval = 'prediction'", "level = 'future'", "type = 'response'"))`
<span class="mcq-key" data-correct="interval = 'prediction'"></span>
<details class="hint"><summary>Hint</summary> Includes error variance.</details>
</div>

<div class="mcq-we-q">
**11) Prediction intervals are typically:**  
`r webexercises::longmcq(c("Narrower than confidence intervals", answer = "Wider than confidence intervals", "Identical to confidence intervals", "Unrelated to residual variance"))`
<span class="mcq-key" data-correct="Wider than confidence intervals"></span>
</div>

<div class="mcq-we-q">
**12) confint(model) returns:**  
`r webexercises::longmcq(c("Intervals for y-hat", answer = "Confidence intervals for coefficients", "Prediction intervals for new y", "Variance of residuals"))`
<span class="mcq-key" data-correct="Confidence intervals for coefficients"></span>
</div>

<div class="mcq-we-q">
**13) For models with multiple predictors, newdata must:**  
`r webexercises::longmcq(c(answer = "Contain columns matching model terms and types", "Be a vector in any order", "Include only the response", "Be sorted by the response"))`
<span class="mcq-key" data-correct="Contain columns matching model terms and types"></span>
<details class="hint"><summary>Hint</summary> Names must match the formula‚Äôs variables.</details>
</div>

<div class="mcq-we-q">
**14) In a log-linear model log(y) ~ x, to back-transform the mean prediction you should:**  
`r webexercises::longmcq(c("Use only exp(eta)", answer = "Multiply exp(eta) by a smearing factor", "Square the fitted values", "Add residual SD then exponentiate"))`
<span class="mcq-key" data-correct="Multiply exp(eta) by a smearing factor"></span>
<details class="hint"><summary>Hint</summary> Duan‚Äôs correction: mean(exp(ŒµÃÇ)).</details>
</div>

<div class="mcq-we-q">
**15) A bootstrap (or resampling with replacement) in R uses:**  
`r webexercises::longmcq(c("sample(..., replace = FALSE)", answer = "sample(..., replace = TRUE)", "sort(...)", "order(...)"))`
<span class="mcq-key" data-correct="sample(..., replace = TRUE)"></span>
</div>

<div class="mcq-we-q">
**16) set.seed(123) is used to:**  
`r webexercises::longmcq(c("Speed up lm()", "Normalize variables", answer = "Make random subsamples reproducible", "Center predictors"))`
<span class="mcq-key" data-correct="Make random subsamples reproducible"></span>
</div>

<div class="mcq-we-q">
**17) For out-of-sample performance, a good metric is:**  
`r webexercises::longmcq(c("Training R¬≤", answer = "Test RMSE", "Number of predictors", "Intercept size"))`
<span class="mcq-key" data-correct="Test RMSE"></span>
<details class="hint"><summary>Hint</summary> Evaluate on held-out data.</details>
</div>

<div class="mcq-we-q">
**18) Extrapolation risk means:**  
`r webexercises::longmcq(c("Predictions are always unbiased", answer = "x‚ÇÄ lies outside the observed x range so predictions can be unreliable", "R¬≤ increases automatically", "CI and PI become identical"))`
<span class="mcq-key" data-correct="x‚ÇÄ lies outside the observed x range so predictions can be unreliable"></span>
</div>

<div class="mcq-we-q">
**19) Variance of the predicted mean at x‚ÇÄ depends on:**  
`r webexercises::longmcq(c("Only œÉ¬≤", "Only the intercept", answer = "x‚ÇÄ·µÄ Var(BÃÇ) x‚ÇÄ", "Only sample size"))`
<span class="mcq-key" data-correct="x‚ÇÄ·µÄ Var(BÃÇ) x‚ÇÄ"></span>
<details class="hint"><summary>Hint</summary> Use the variance‚Äìcovariance of coefficients.</details>
</div>

<div class="mcq-we-q">
**20) When predicting with factors, newdata must:**  
`r webexercises::longmcq(c(answer = "Use levels seen in the training data", "Introduce new unseen levels freely", "Coerce factors to numeric", "Omit the factor columns"))`
<span class="mcq-key" data-correct="Use levels seen in the training data"></span>
<details class="hint"><summary>Hint</summary> Unseen levels cause errors in predict().</details>
</div>

</div>

---

üèÅ  *End of lab 8*
üõë Remember to save your script üíæ

---

## üß™ Lab 9 ‚Äî Hypothesis Test, p‚ÄëValue & Testing Linear Combinations

### Hypothesis Tests

Null vs alternative for a coefficient \(\beta_k\):

$$
H_0: \beta_k = c, \qquad H_A: \beta_k \ne c.
$$

Test statistic:
$$
t = \frac{b_k - c}{\operatorname{se}(b_k)},\quad t \sim t_{N-2}.\tag{6}
$$

Example for \(b_2\) in the food model:

```{r}
alpha <- 0.05
library(PoEdata); library(xtable); library(knitr)
data("food")
mod1 <- lm(food_exp ~ income, data=food); smod1 <- summary(mod1)

table <- data.frame(xtable(mod1))
kable(table, caption="Regression output showing the coefficients")

b2 <- coef(mod1)[["income"]]
seb2 <- sqrt(vcov(mod1)[2,2])
df  <- df.residual(mod1)

t   <- b2/seb2
tcr <- qt(1-alpha/2, df)
t; tcr
```

Right‚Äëtail and left‚Äëtail versions:

```{r}
# Right-tail: H0: beta2 <= 5.5; HA: beta2 > 5.5
c <- 5.5
t_right <- (b2 - c)/seb2
tcr_right <- qt(1-alpha, df)

# Left-tail: H0: beta2 >= 15; HA: beta2 < 15
c <- 15
t_left <- (b2 - c)/seb2
tcr_left <- qt(alpha, df)

c(t_right=t_right, tcr_right=tcr_right, t_left=t_left, tcr_left=tcr_left)
```

### The p‚ÄëValue

Right‚Äëtail: \(p = 1 - F_t(t)\). Left‚Äëtail: \(p = F_t(t)\). Two‚Äëtail: \(p = 2\big(1-F_t(|t|)\big)\).

```{r}
# Right tail
c <- 5.5; t <- (b2-c)/seb2; p_right <- 1-pt(t, df)

# Left tail
c <- 15; t <- (b2-c)/seb2; p_left <- pt(t, df)

# Two tail
c <- 0;  t <- (b2-c)/seb2; p_two <- 2*(1-pt(abs(t), df))

c(p_right=p_right, p_left=p_left, p_two=p_two)
```

### Testing Linear Combinations of Parameters

Expected food expenditure at income = \$2000 (i.e. \(x=20\)):

$$
L = E(\text{food\_exp}\mid \text{income}=20) = \beta_1 + 20\,\beta_2.\tag{3.7}
$$

Variance identities:

$$
\operatorname{var}(aX+bY) = a^2\,\operatorname{var}(X) + b^2\,\operatorname{var}(Y) + 2ab\,\operatorname{cov}(X,Y).\tag{3.8}
$$

$$
\operatorname{var}(b_1+20b_2) = \operatorname{var}(b_1) + 20^2\operatorname{var}(b_2) + 2\cdot20\operatorname{cov}(b_1,b_2).\tag{3.9}
$$

```{r}
alpha <- 0.05; x <- 20
m1 <- lm(food_exp ~ income, data=food)
df <- df.residual(m1)

b1 <- m1$coef[1]; b2 <- m1$coef[2]
varb1 <- vcov(m1)[1,1]; varb2 <- vcov(m1)[2,2]; covb1b2 <- vcov(m1)[1,2]

L <- b1 + b2*x
varL <- varb1 + x^2 * varb2 + 2*x*covb1b2
seL <- sqrt(varL)

tcr <- qt(1-alpha/2, df)
lowbL <- L - tcr*seL; upbL <- L + tcr*seL
c(L=L, seL=seL, low=lowbL, up=upbL)
```

Two‚Äëtail, left‚Äëtail, right‚Äëtail tests for \(L\):

```{r}
c <- 250
t <- (L - c)/seL
p_value <- 2*(1-pt(abs(t), df))
c(t=t, p_value=p_value)
```


---

üèÅ  *End of lab 9*
üõë Remember to save your script üíæ

---



## üß™ Lab 10 ‚Äî The p‚ÄëValue (Recap)

- **Right‚Äëtail:** \(p=1-pt(t,\,df)\)  
- **Left‚Äëtail:** \(p=pt(t,\,df)\)  
- **Two‚Äëtail:** \(p=2(1-pt(|t|,\,df))\)

```{r}
# Examples computed earlier are reproduced here:
c(p_right=p_right, p_left=p_left, p_two=p_two)
```

---

### References

Adkins (2014); Allaire et al. (2016); Colonescu (2016); Croissant & Millo (2015); Dahl (2016); Fox & Weisberg (2016); Fox et al. (2016); Ghalanos (2015); Graves (2014); Grolemund & Wickham (2016); Henningsen & Hamann (2015); Hill, Griffiths & Lim (2011); Hlavac (2015); Hothorn et al. (2015); Hyndman (2016); Kleiber & Zeileis (2015); Komashko (2016); Lander (2013); Lumley & Zeileis (2015); Pfaff (2013); R Core Team (2008); Reinhart (2015); Robinson (2016); RStudio Team (2015); Spada et al. (2012); Trapletti & Hornik (2016); Wickham & Chang (2016); Xie (2014, 2016a, 2016b); Zeileis (2016).

---

üèÅ  *End of lab 10*
üõë Remember to save your script üíæ

---
